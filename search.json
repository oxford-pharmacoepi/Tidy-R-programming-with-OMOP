[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#is-this-book-for-me",
    "href": "index.html#is-this-book-for-me",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "Is this book for me?",
    "text": "Is this book for me?\nWeâ€™ve written this book for anyone interested in working with Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) instances using a tidyverse style approach. That is, human centered, consistent, composable, and inclusive (see Tidy design principles for more details on these principles).\nNew to R? We recommend you take a look at R for data science before reading this book. We assume that you have R installed together with an adequate Integrated Development Environment (IDE) such as RStudio or Positron. See this tutorial if you need guidance on how to get started. The book uses multiple packages that you will need to install. See the list in the R packages section.\nNew to databases? We recommend you take a look at some web tutorials on SQL, such as SQLBolt or SQLZoo to have a basic understanding of how databases work.\nNew to the OMOP CDM? Weâ€™d recommend you pair this book with The Book of OHDSI.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-is-the-book-organised",
    "href": "index.html#how-is-the-book-organised",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "How is the book organised?",
    "text": "How is the book organised?\nThe book is divided into two parts. The first half of the book is focused on the general principles for working with databases from R. In these chapters you will see how you can use familiar tidyverse-style code to build up analytic pipelines that start with data held in a database and end with your analytic results. The second half of the book is focused on working with data in the OMOP CDM format, a widely used data format for health care data. In these chapters you will see how to work with this data format using the general principles from the first half of the book along with a set of R packages that have been built for the OMOP CDM.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "Contributors",
    "text": "Contributors\nAuthors: Edward Burn    , MartÃ­ CatalÃ     \nReviewers: Adam Black, Berta RaventÃ³s, Cecilia Campanile, Daniel Prieto-Alhambra, Elin Rowlands, NÃºria MercadÃ©-Besora, Markus Haug, Marta Alcalde, Mike Du, Kim LÃ³pez-GÃ¼ell, Yuchen Guo",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "Citation",
    "text": "Citation\nIf you found this book useful, please help us by citing it:\n\nBurn E, CatalÃ  M. Tidy R programming with the OMOP Common Data Model. GitHub; \n2025. https://github.com/oxford-pharmacoepi/Tidy-R-programming-with-OMOP",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "License",
    "text": "License\n This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "Code",
    "text": "Code\nThe source code for the book can be found at this GitHub repository, please star it if you found it useful.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Tidy R programming with the OMOP Common Data Model",
    "section": "R Packages",
    "text": "R Packages\nThis book is rendered automatically through GitHub Actions using the following version of packages:\n\n\n\n\n\n\n\n\nPackage\nVersion\nLink\n\n\n\n\nCDMConnector\n2.2.0\nðŸ”—\n\n\nCodelistGenerator\n3.5.0\nðŸ”—\n\n\nCohortCharacteristics\n1.0.2\nðŸ”—\n\n\nCohortConstructor\n0.5.0\nðŸ”—\n\n\nDBI\n1.2.3\nðŸ”—\n\n\nDatabaseConnector\n7.0.0\nðŸ”—\n\n\nLahman\n13.0-0\nðŸ”—\n\n\nOmopSketch\n0.5.1\nðŸ”—\n\n\nPatientProfiles\n1.4.3\nðŸ”—\n\n\nRPostgres\n1.4.8\nðŸ”—\n\n\nbit64\n4.6.0-1\nðŸ”—\n\n\ncli\n3.6.5\nðŸ”—\n\n\nclock\n0.7.3\nðŸ”—\n\n\ndbplyr\n2.5.1\nðŸ”—\n\n\ndm\n1.0.12\nðŸ”—\n\n\ndplyr\n1.1.4\nðŸ”—\n\n\nduckdb\n1.4.1\nðŸ”—\n\n\nggplot2\n4.0.0\nðŸ”—\n\n\nnycflights13\n1.0.2\nðŸ”—\n\n\nomock\n0.5.0\nðŸ”—\n\n\nomopgenerics\n1.3.2\nðŸ”—\n\n\npalmerpenguins\n0.1.1\nðŸ”—\n\n\npurrr\n1.1.0\nðŸ”—\n\n\nsloop\n1.0.1\nðŸ”—\n\n\nstringr\n1.5.2\nðŸ”—\n\n\ntidyr\n1.3.1\nðŸ”—\n\n\n\n\n\n\n\nNote: we only included the packages called explicitly in the book.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "tidy_databases/index.html",
    "href": "tidy_databases/index.html",
    "title": "Getting started with databases from R",
    "section": "",
    "text": "In this first half of the book, we will explore how to work with databases from R. In the following chapters, youâ€™ll see that when working with data held in a relational database, we can leverage various open-source R packages to perform tidyverse-style data analyses.\n\nIn 1Â  A first analysis using data in a database we will perform a simple data analysis from start to finish using a table in a database.\nIn 2Â  Core verbs for analytic pipelines utilising a database we will see in more detail how familiar dplyr functions can be used to combine data spread across different tables in a database into an analytic dataset for further analysis in R.\nIn 3Â  Supported expressions for database queries we will see how we can perform more complex data manipulation via translation of R code into SQL specific to the database management system being used.\nIn 4Â  Building analytic pipelines for a data model we will see how we can build data pipelines by creating a data model in R to represent the relational database weâ€™re working with and creating functions and methods to work with it.",
    "crumbs": [
      "Getting started with databases from R"
    ]
  },
  {
    "objectID": "tidy_databases/working_with_databases.html",
    "href": "tidy_databases/working_with_databases.html",
    "title": "1Â  A first analysis using data in a database",
    "section": "",
    "text": "1.1 Getting set up\nArtwork by @allison_horst\nBefore we start working with healthcare data spread across a database using the OMOP Common Data Model, letâ€™s first do a simpler analysis. In this case, we will do a quick data analysis with R using a simple dataset held in a database to understand the general approach. For this weâ€™ll use data from the palmerpenguins package, which contains data on penguins collected from the Palmer Station in Antarctica.\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(ggplot2)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(palmerpenguins)",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/working_with_databases.html#taking-a-peek-at-the-data",
    "href": "tidy_databases/working_with_databases.html#taking-a-peek-at-the-data",
    "title": "1Â  A first analysis using data in a database",
    "section": "1.2 Taking a peek at the data",
    "text": "1.2 Taking a peek at the data\nThe package palmerpenguins contains two datasets, one of them called penguins, which we will use in this chapter. We can get an overview of the data using the glimpse() command.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelâ€¦\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerseâ€¦\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, â€¦\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, â€¦\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186â€¦\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, â€¦\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, maleâ€¦\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007â€¦\n\n\nOr we could take a look at the first rows of the data using head():\n\nhead(penguins, 5)\n\n# A tibble: 5 Ã— 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# â„¹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/working_with_databases.html#inserting-data-into-a-database",
    "href": "tidy_databases/working_with_databases.html#inserting-data-into-a-database",
    "title": "1Â  A first analysis using data in a database",
    "section": "1.3 Inserting data into a database",
    "text": "1.3 Inserting data into a database\nBy default, the data provided by the package is local (stored in memory on your computer), so letâ€™s first put it into a DuckDB database. We need to first create the database.\n\ncon &lt;- dbConnect(drv = duckdb())\n\nSee that now we have created an empty DuckDB database. We can easily add the penguins data to it.\n\ndbWriteTable(conn = con, name = \"penguins\", value = penguins)\n\nWith the function dbListTables() we can list the tables of a database. In our case, we can see it now has one table:\n\ndbListTables(conn = con)\n\n[1] \"penguins\"\n\n\nAnd now that the data is in a database we could use SQL directly to get the first rows that we saw before.\n\ndbGetQuery(conn = con, statement = \"SELECT * FROM penguins LIMIT 5\")\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n\n\nAs you can see we have the same data that we had locally but now itâ€™s located inside the DuckDB database we created.\n\n\n\n\n\n\nTipConnecting to databases from R\n\n\n\n\n\nDatabase connections from R can be made using the DBI package where the back-end for DBI is facilitated by database-specific driver packages (such as RPostgres for Postgres and Amazon Redshift, and bigrquery for Google BigQuery), or the odbc R package can be used with ODBC drivers. Or instead of using ODBC drivers, JDBC drivers can be used via the DatabaseConnector R package.\nIn the code snippets above, we created a new, empty, in-process DuckDB via database to which we then added our dataset. But we could have instead connected to an existing duckdb database. This could, for example, look like:\n\ncon &lt;- dbConnect(drv = duckdb(dbdir = \"my_duckdb_database.ducdkb\"))\n\nNote that if you point to a non-existing DuckDB file, this will be created with an empty database.\nIn this book, for simplicity, we will mostly be working with in-process DuckDB databases with synthetic data. However, when analysing real patient data we will be more often working with client-server databases, where we are connecting from our computer to a central server with the database or working with data held in the cloud. The approaches shown throughout this book will work in the same way for these other types of database management systems, but the way to connect to the database will be different (although still using DBI). In general, creating connections is supported by associated back-end packages. For example a connection to a Postgres database would use the RPostgres R package and look something like this:\n\nlibrary(DBI)\nlibrary(RPostgres)\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"my_database\",\n                 host = \"my_server\",\n                 user = \"user\",\n                 password = \"password\")\n\nOr if using the DatabaseConnector R package creating a connection would look something like:\n\nlibrary(DatabaseConnector)\ndownloadJdbcDrivers(\"postgresql\")\ncon &lt;- connect(dbms = \"postgresql\",\n               connectionString = \"jdbc:postgresql://my_server\",\n               user = \"user\",\n               password = \"password\")",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/working_with_databases.html#translation-from-r-to-sql",
    "href": "tidy_databases/working_with_databases.html#translation-from-r-to-sql",
    "title": "1Â  A first analysis using data in a database",
    "section": "1.4 Translation from R to SQL",
    "text": "1.4 Translation from R to SQL\nInstead of using SQL to query our database, we might instead want to use the same R code as before. However, instead of working with the local dataset, now we will need it to query the data held in the database. To do this, first we can create a reference to the table in the database as such:\n\npenguins_db &lt;- tbl(src = con, \"penguins\")\npenguins_db\n\n# Source:   table&lt;penguins&gt; [?? x 8]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# â„¹ more rows\n# â„¹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nOnce we have this reference, we can then use it with familiar looking R code.\n\nhead(penguins_db, 5)\n\n# Source:   SQL [?? x 8]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n# â„¹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe magic here is provided by the dbplyr package, which takes the R code and converts it into SQL. In this case the query looks like the SQL we wrote directly before.\n\nhead(penguins_db, 5) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT penguins.*\nFROM penguins\nLIMIT 5",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/working_with_databases.html#example-analysis",
    "href": "tidy_databases/working_with_databases.html#example-analysis",
    "title": "1Â  A first analysis using data in a database",
    "section": "1.5 Example analysis",
    "text": "1.5 Example analysis\nMore complicated SQL can also be generated by using familiar dplyr code. For example, we could get a summary of bill length by species like so:\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(\n    n = n(),\n    min_bill_length_mm = min(bill_length_mm, na.rm = TRUE),\n    mean_bill_length_mm = mean(bill_length_mm, na.rm = TRUE),\n    max_bill_length_mm = max(bill_length_mm, na.rm = TRUE)\n  ) |&gt;\n  mutate(min_max_bill_length_mm = paste0(\n    min_bill_length_mm, \" to \", max_bill_length_mm\n  )) |&gt;\n  select(\"species\", \"mean_bill_length_mm\", \"min_max_bill_length_mm\")\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  species   mean_bill_length_mm min_max_bill_length_mm\n  &lt;fct&gt;                   &lt;dbl&gt; &lt;chr&gt;                 \n1 Adelie                   38.8 32.1 to 46.0          \n2 Chinstrap                48.8 40.9 to 58.0          \n3 Gentoo                   47.5 40.9 to 59.6          \n\n\nThe benefit of using dbplyr now becomes quite clear if we take a look at the corresponding SQL that is generated for us:\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(\n    n = n(),\n    min_bill_length_mm = min(bill_length_mm, na.rm = TRUE),\n    mean_bill_length_mm = mean(bill_length_mm, na.rm = TRUE),\n    max_bill_length_mm = max(bill_length_mm, na.rm = TRUE)\n  ) |&gt;\n  mutate(min_max_bill_length_mm = paste0(\n    min_bill_length_mm, \" to \", max_bill_length_mm\n  )) |&gt;\n  select(\"species\", \"mean_bill_length_mm\", \"min_max_bill_length_mm\") |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  species,\n  mean_bill_length_mm,\n  CONCAT_WS('', min_bill_length_mm, ' to ', max_bill_length_mm) AS min_max_bill_length_mm\nFROM (\n  SELECT\n    species,\n    COUNT(*) AS n,\n    MIN(bill_length_mm) AS min_bill_length_mm,\n    AVG(bill_length_mm) AS mean_bill_length_mm,\n    MAX(bill_length_mm) AS max_bill_length_mm\n  FROM penguins\n  GROUP BY species\n) q01\n\n\nInstead of having to write this somewhat complex SQL specific to DuckDB, we can use the friendlier dplyr syntax that will be more familiar if youâ€™re coming from an R programming background.\n\n\n\n\n\n\nNoteTranslation to different SQL dialects\n\n\n\n\n\nNote this same R code will also work for other SQL dialects such as Postgres, SQL server, Snowflake and Spark. Here you can see the different generated translations:\n\nPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `mean_bill_length_mm`,\n  CONCAT_WS('', `min_bill_length_mm`, ' to ', `max_bill_length_mm`) AS `min_max_bill_length_mm`\nFROM (\n  SELECT\n    `species`,\n    COUNT(*) AS `n`,\n    MIN(`bill_length_mm`) AS `min_bill_length_mm`,\n    AVG(`bill_length_mm`) AS `mean_bill_length_mm`,\n    MAX(`bill_length_mm`) AS `max_bill_length_mm`\n  FROM `df`\n  GROUP BY `species`\n) AS `q01`\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `mean_bill_length_mm`,\n  `min_bill_length_mm` + ' to ' + `max_bill_length_mm` AS `min_max_bill_length_mm`\nFROM (\n  SELECT\n    `species`,\n    COUNT_BIG(*) AS `n`,\n    MIN(`bill_length_mm`) AS `min_bill_length_mm`,\n    AVG(`bill_length_mm`) AS `mean_bill_length_mm`,\n    MAX(`bill_length_mm`) AS `max_bill_length_mm`\n  FROM `df`\n  GROUP BY `species`\n) AS `q01`\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `mean_bill_length_mm`,\n  `min_bill_length_mm` || ' to ' || `max_bill_length_mm` AS `min_max_bill_length_mm`\nFROM (\n  SELECT\n    `species`,\n    COUNT(*) AS `n`,\n    MIN(`bill_length_mm`) AS `min_bill_length_mm`,\n    AVG(`bill_length_mm`) AS `mean_bill_length_mm`,\n    MAX(`bill_length_mm`) AS `max_bill_length_mm`\n  FROM `df`\n  GROUP BY `species`\n) AS `q01`\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `mean_bill_length_mm`,\n  ARRAY_TO_STRING(ARRAY_CONSTRUCT_COMPACT(`min_bill_length_mm`, ' to ', `max_bill_length_mm`), '') AS `min_max_bill_length_mm`\nFROM (\n  SELECT\n    `species`,\n    COUNT(*) AS `n`,\n    MIN(`bill_length_mm`) AS `min_bill_length_mm`,\n    AVG(`bill_length_mm`) AS `mean_bill_length_mm`,\n    MAX(`bill_length_mm`) AS `max_bill_length_mm`\n  FROM `df`\n  GROUP BY `species`\n) AS `q01`\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT\n  `species`,\n  `mean_bill_length_mm`,\n  CONCAT_WS('', `min_bill_length_mm`, ' to ', `max_bill_length_mm`) AS `min_max_bill_length_mm`\nFROM (\n  SELECT\n    `species`,\n    COUNT(*) AS `n`,\n    MIN(`bill_length_mm`) AS `min_bill_length_mm`,\n    AVG(`bill_length_mm`) AS `mean_bill_length_mm`,\n    MAX(`bill_length_mm`) AS `max_bill_length_mm`\n  FROM `df`\n  GROUP BY `species`\n) AS `q01`\n\n\n\n\n\nNote that even though the different SQL statements look similar, each SQL dialect has its own particularities. Using the dbplyr approach allows us to support multiple different SQL dialects and back-ends by just writing R code.\n\n\n\nNot having to worry about the SQL translation behind our queries allows us to query the database in a simple way even for more complex questions. For instance, suppose now that we are particularly interested in the body mass variable. We can first notice that there are a couple of missing records for this.\n\npenguins_db |&gt;\n  mutate(missing_body_mass_g = if_else(is.na(body_mass_g), 1, 0)) |&gt;\n  group_by(species, missing_body_mass_g) |&gt;\n  tally()\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  species   missing_body_mass_g     n\n  &lt;fct&gt;                   &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie                      0   151\n2 Gentoo                      0   123\n3 Adelie                      1     1\n4 Gentoo                      1     1\n5 Chinstrap                   0    68\n\n\nWe can get the mean for each of the species, first dropping those two missing records:\n\npenguins_db |&gt;\n  group_by(species) |&gt;\n  summarise(mean_body_mass_g = round(mean(body_mass_g, na.rm = TRUE)))\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  species   mean_body_mass_g\n  &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie                3701\n2 Chinstrap             3733\n3 Gentoo                5076\n\n\nWe could also make a histogram of values for each of the species using the ggplot2 package. Here we would bring our data back into R before creating our plot with the collect() function.\n\npenguins_db |&gt;\n  select(\"species\", \"body_mass_g\") |&gt; \n  collect() |&gt;\n  ggplot(aes(group = species, fill = species)) +\n  facet_grid(species ~ .) +\n  geom_histogram(aes(body_mass_g), colour = \"black\", binwidth = 100) +\n  xlab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow letâ€™s look at the relationship between body mass and bill depth.\n\npenguins_db |&gt;\n  select(\"species\", \"body_mass_g\", \"bill_depth_mm\") |&gt; \n  collect() |&gt;\n  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Bill depth (mm)\") +\n  ylab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWe see a negative correlation between body mass and bill depth, which seems rather unexpected. But what about if we stratify this query by species?\n\npenguins_db |&gt;\n  select(\"species\", \"body_mass_g\", \"bill_depth_mm\") |&gt;\n  collect() |&gt;\n  ggplot(aes(x = bill_depth_mm, y = body_mass_g)) +\n  facet_grid(species ~ .) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Bill depth (mm)\") +\n  ylab(\"Body mass (g)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAs well as having an example of working with data in database from R, you also have an example of Simpsonâ€™s paradox!",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/working_with_databases.html#disconnecting-from-the-database",
    "href": "tidy_databases/working_with_databases.html#disconnecting-from-the-database",
    "title": "1Â  A first analysis using data in a database",
    "section": "1.6 Disconnecting from the database",
    "text": "1.6 Disconnecting from the database\nNow that weâ€™ve reached the end of this example, we can close our connection to the database.\n\ndbDisconnect(conn = con)",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/working_with_databases.html#further-reading",
    "href": "tidy_databases/working_with_databases.html#further-reading",
    "title": "1Â  A first analysis using data in a database",
    "section": "1.7 Further reading",
    "text": "1.7 Further reading\n\nR for Data Science (Chapter 13: Relational data)\nWriting SQL with dbplyr\nData Carpentry: SQL databases and R",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>A first analysis using data in a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_verbs.html",
    "href": "tidy_databases/tidyverse_verbs.html",
    "title": "2Â  Core verbs for analytic pipelines utilising a database",
    "section": "",
    "text": "2.1 Tidyverse functions\nWe saw in the previous chapter that we can use familiar dplyr verbs with data held in a database. There, we were working with just a single table which we loaded into the database. When working with databases, we will typically be working with multiple tables (as weâ€™ll see later when working with data in the OMOP CDM format). For this chapter, we will see more tidyverse functionality that can be used with data in a database, this time using the nycflights13 data. As we can see, we now have a set of related tables with data on flights departing from New York City airports in 2013.\nLetâ€™s load the required libraries, add our data to a DuckDB database, and then create references to each of these tables.\nFor almost all analyses, we want to go from having our starting data spread out across multiple tables in the database to a single tidy table containing all the data we need for the specific analysis. We can often get to our tidy analytic dataset using the tidyverse functions below (most of which come from dplyr, but a couple also from the tidyr package). These functions all work with data in a database by generating SQL that will have the same purpose as if these functions were being run against data in R.",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_verbs.html#tidyverse-functions",
    "href": "tidy_databases/tidyverse_verbs.html#tidyverse-functions",
    "title": "2Â  Core verbs for analytic pipelines utilising a database",
    "section": "",
    "text": "Important\n\n\n\nUntil we use compute() or collect() (or printing the first few rows of the result), all weâ€™re doing is translating R code into SQL. This means no code is being executed on the database side.\n\ncompute() will execute the query and store it in a new table in the database.\ncollect() will execute the query and bring the result back to R.\nprinting (e.g.Â glimpse() or print()) will execute the query, limiting the result to the first set of rows, which leads to shorter computation time on the database side.\n\n\n\n\n\n\nPurpose\nFunctions\nDescription\n\n\n\n\nSelecting rows\nfilter, distinct\nTo select rows in a table.\n\n\nOrdering rows\narrange\nTo order rows in a table.\n\n\nColumn Transformation\nmutate, select, relocate, rename\nTo create new columns or change existing ones.\n\n\nGrouping and ungrouping\ngroup_by, rowwise, ungroup\nTo group data by one or more variables and to remove grouping.\n\n\nAggregation\ncount, tally, summarise\nThese functions are used for summarising data.\n\n\nData merging and joining\ninner_join, left_join, right_join, full_join, anti_join, semi_join, cross_join\nThese functions are used to combine data from different tables based on common columns.\n\n\nData reshaping\npivot_wider, pivot_longer\nThese functions are used to reshape data between wide and long formats.\n\n\nData union\nunion_all, union\nThis function combines two tables.\n\n\nRandomly selects rows\nslice_sample\nWe can use this to take a random subset a table.\n\n\n\n\n\n\n\n\n\nTipBehind the scenes\n\n\n\n\n\nBy using the above functions we can use the same code regardless of whether the data is held in the database or locally in R. This is because the functions used above are generic functions which behave differently depending on the type of input they are given. Letâ€™s take inner_join() for example. We can see that this function is a S3 generic function (with S3 being the most common object-oriented system used in R).\n\nlibrary(sloop)\nftype(inner_join)\n\n[1] \"S3\"      \"generic\"\n\n\nAmong others, the references we create to tables in a database have tbl_lazy as a class attribute. Meanwhile, we can see that when collected into R, the object changes to have different attributes, one of which is data.frame:\n\nclass(flights_db)\n\n[1] \"tbl_duckdb_connection\" \"tbl_dbi\"               \"tbl_sql\"              \n[4] \"tbl_lazy\"              \"tbl\"                  \n\nclass(flights_db |&gt; head(1) |&gt; collect())\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe can see that inner_join() has different methods for tbl_lazy and data.frame.\n\ns3_methods_generic(\"inner_join\")\n\n# A tibble: 2 Ã— 4\n  generic    class      visible source             \n  &lt;chr&gt;      &lt;chr&gt;      &lt;lgl&gt;   &lt;chr&gt;              \n1 inner_join data.frame FALSE   registered S3method\n2 inner_join tbl_lazy   FALSE   registered S3method\n\n\nWhen working with references to tables in the database the tbl_lazy method will be used.\n\ns3_dispatch(flights_db |&gt; \n              inner_join(planes_db))\n\n   inner_join.tbl_duckdb_connection\n   inner_join.tbl_dbi\n   inner_join.tbl_sql\n=&gt; inner_join.tbl_lazy\n   inner_join.tbl\n   inner_join.default\n\n\nBut once we bring data into R, the data.frame method will be used.\n\ns3_dispatch(flights_db |&gt; head(1) |&gt; collect() |&gt; \n              inner_join(planes_db |&gt; head(1) |&gt; collect()))\n\n   inner_join.tbl_df\n   inner_join.tbl\n=&gt; inner_join.data.frame\n   inner_join.default",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_verbs.html#getting-to-an-analytic-dataset",
    "href": "tidy_databases/tidyverse_verbs.html#getting-to-an-analytic-dataset",
    "title": "2Â  Core verbs for analytic pipelines utilising a database",
    "section": "2.2 Getting to an analytic dataset",
    "text": "2.2 Getting to an analytic dataset\nTo see a little more on how we can use the above functions, letâ€™s say we want to do an analysis of late flights from JFK airport. We want to see whether there is some relationship between plane characteristics and the risk of delay.\nFor this, weâ€™ll first use the filter() and select() dplyr verbs to get the data from the flights table. Note, weâ€™ll rename arr_delay to just delay.\n\ndelayed_flights_db &lt;- flights_db |&gt; \n  filter(!is.na(arr_delay) & origin == \"JFK\") |&gt; \n  select(\"dest\", \"distance\", \"carrier\", \"tailnum\", \"delay\" = \"arr_delay\")\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\nSee the resultant DuckDB query:\n\n\n&lt;SQL&gt;\nSELECT dest, distance, carrier, tailnum, arr_delay AS delay\nFROM flights\nWHERE (NOT((arr_delay IS NULL)) AND origin = 'JFK')\n\n\n\n\n\nWhen executed, our results will look like the following:\n\ndelayed_flights_db\n\n# Source:   SQL [?? x 5]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   dest  distance carrier tailnum delay\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 MIA       1089 AA      N619AA     33\n 2 BQN       1576 B6      N804JB    -18\n 3 MCO        944 B6      N593JB     -8\n 4 PBI       1028 B6      N793JB     -2\n 5 TPA       1005 B6      N657JB     -3\n 6 LAX       2475 UA      N29129      7\n 7 BOS        187 B6      N708JB     -4\n 8 ATL        760 DL      N3739P     -8\n 9 SFO       2586 UA      N532UA     14\n10 RSW       1074 B6      N635JB      4\n# â„¹ more rows\n\n\nNow weâ€™ll add plane characteristics from the planes table. We will use an inner_join so that only records for which we have the plane characteristics are kept.\n\ndelayed_flights_db &lt;- delayed_flights_db |&gt; \n  inner_join(\n    planes_db |&gt; \n      select(\"tailnum\", \"seats\"),\n    by = \"tailnum\"\n  )\n\nNote that our first query was not executed, as we didnâ€™t use either compute() or collect(), so weâ€™ll now have added our join to the original query.\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\nSee that now the SQL code combines both queries:\n\n\n&lt;SQL&gt;\nSELECT LHS.*, seats\nFROM (\n  SELECT dest, distance, carrier, tailnum, arr_delay AS delay\n  FROM flights\n  WHERE (NOT((arr_delay IS NULL)) AND origin = 'JFK')\n) LHS\nINNER JOIN planes\n  ON (LHS.tailnum = planes.tailnum)\n\n\n\n\n\nAnd when executed, our results will look like the following:\n\ndelayed_flights_db\n\n# Source:   SQL [?? x 6]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   dest  distance carrier tailnum delay seats\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;\n 1 MIA       1089 AA      N619AA     33   178\n 2 BQN       1576 B6      N804JB    -18   200\n 3 MCO        944 B6      N593JB     -8   200\n 4 PBI       1028 B6      N793JB     -2   200\n 5 TPA       1005 B6      N657JB     -3   200\n 6 LAX       2475 UA      N29129      7   178\n 7 BOS        187 B6      N708JB     -4   200\n 8 ATL        760 DL      N3739P     -8   189\n 9 RSW       1074 B6      N635JB      4   200\n10 SJU       1598 B6      N794JB    -21   200\n# â„¹ more rows\n\n\nThis tidy dataset has been created in the database via R code translated to SQL. With this, we can now collect our analytic dataset into R and proceed from there (for example, to perform statistical analyses locally that might not be possible to run in a database, such as plots, density distributions, regression, or anything beyond data manipulation).\n\ndelayed_flights &lt;- delayed_flights_db |&gt; \n  collect() \n\nglimpse(delayed_flights)\n\nRows: 93,298\nColumns: 6\n$ dest     &lt;chr&gt; \"LAX\", \"CLT\", \"MCO\", \"SFO\", \"ATL\", \"FLL\", \"BUF\", \"RSW\", \"LAS\"â€¦\n$ distance &lt;dbl&gt; 2475, 541, 944, 2586, 760, 1069, 301, 1074, 2248, 1182, 2153,â€¦\n$ carrier  &lt;chr&gt; \"UA\", \"US\", \"B6\", \"UA\", \"DL\", \"B6\", \"B6\", \"B6\", \"B6\", \"B6\", \"â€¦\n$ tailnum  &lt;chr&gt; \"N34137\", \"N117UW\", \"N632JB\", \"N502UA\", \"N681DA\", \"N568JB\", \"â€¦\n$ delay    &lt;dbl&gt; -10, -34, -2, 7, -12, -3, 2, 2, 0, -19, -35, -8, 7, -12, 11, â€¦\n$ seats    &lt;int&gt; 178, 182, 200, 178, 178, 200, 20, 200, 200, 20, 379, 20, 200,â€¦",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_verbs.html#disconnecting-from-the-database",
    "href": "tidy_databases/tidyverse_verbs.html#disconnecting-from-the-database",
    "title": "2Â  Core verbs for analytic pipelines utilising a database",
    "section": "2.3 Disconnecting from the database",
    "text": "2.3 Disconnecting from the database\nNow that weâ€™ve reached the end of this example, we can close our connection to the database.\n\ndbDisconnect(conn = con)",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_verbs.html#further-reading",
    "href": "tidy_databases/tidyverse_verbs.html#further-reading",
    "title": "2Â  Core verbs for analytic pipelines utilising a database",
    "section": "2.4 Further reading",
    "text": "2.4 Further reading\n\nWickham H, FranÃ§ois R, Henry L, MÃ¼ller K, Vaughan D (2025). dplyr: A Grammar of Data Manipulation. R package version 1.1.4, https://dplyr.tidyverse.org\nWickham H, Vaughan D, Girlich M (2025). tidyr: Tidy Messy Data. R package version 1.3.1, https://tidyr.tidyverse.org.",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Core verbs for analytic pipelines utilising a database</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html",
    "href": "tidy_databases/tidyverse_expressions.html",
    "title": "3Â  Supported expressions for database queries",
    "section": "",
    "text": "3.1 Data types\nIn the previous chapter, Chapter 2, we saw that there are a core set of tidyverse functions that can be used with databases to extract data for analysis. The SQL code used in the previous chapter is consistent across database management systems, since it only involves basic operations such as joins and variable selection.\nFor more complex data pipelines, we will, however, often need to incorporate additional expressions within these functions. Because of differences across database management systems, the translated SQL can vary. Moreover, some expressions are only supported for some databases.\nWhen writing code that should work across different database management systems, we need to keep in mind which expressions are supported where. To help with this, the sections below show the available SQL translations for common expressions we might want to use.\nLetâ€™s first load the packages which these expressions come from. In addition to base R types, bit64 adds support for integer64. The stringr package provides functions for working with strings, while clock has various functions for working with dates. Many other useful expressions will come from dplyr itself.\nCommonly used data types are consistently supported across database backends. We can use the base as.numeric(), as.integer(), as.character(), as.Date(), and as.POSIXct(). We can also use as.integer64() from the bit64 package to coerce to integer64, and the as_date() and as_datetime() from the clock package instead of as.Date() and as.POSIXct(), respectively.",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#data-types",
    "href": "tidy_databases/tidyverse_expressions.html#data-types",
    "title": "3Â  Supported expressions for database queries",
    "section": "",
    "text": "TipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\ncon &lt;- simulate_duckdb()\ntranslate_sql(as.numeric(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS NUMERIC)\n\ntranslate_sql(as.integer(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ncon &lt;- simulate_postgres()\ntranslate_sql(as.numeric(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS NUMERIC)\n\ntranslate_sql(as.integer(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ncon &lt;- simulate_mssql()\ntranslate_sql(as.numeric(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(`var` AS FLOAT)\n\ntranslate_sql(as.integer(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(TRY_CAST(`var` AS NUMERIC) AS INT)\n\ntranslate_sql(as.integer64(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(TRY_CAST(`var` AS NUMERIC(38, 0)) AS BIGINT)\n\ntranslate_sql(as.character(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(`var` AS VARCHAR(MAX))\n\ntranslate_sql(as.Date(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATETIME2)\n\ntranslate_sql(as_datetime(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(`var` AS DATETIME2)\n\ntranslate_sql(as.logical(var), con = con)\n\n&lt;SQL&gt; TRY_CAST(`var` AS BIT)\n\n\n\n\n\ncon &lt;- simulate_redshift()\ntranslate_sql(as.numeric(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS FLOAT)\n\ntranslate_sql(as.integer(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS INTEGER)\n\ntranslate_sql(as.integer64(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TEXT)\n\ntranslate_sql(as.Date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ncon &lt;- simulate_snowflake()\ntranslate_sql(as.numeric(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DOUBLE)\n\ntranslate_sql(as.integer(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS INT)\n\ntranslate_sql(as.integer64(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS STRING)\n\ntranslate_sql(as.Date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)\n\n\n\n\n\ncon &lt;- simulate_spark_sql()\ntranslate_sql(as.numeric(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DOUBLE)\n\ntranslate_sql(as.integer(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS INT)\n\ntranslate_sql(as.integer64(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BIGINT)\n\ntranslate_sql(as.character(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS STRING)\n\ntranslate_sql(as.Date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as_date(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS DATE)\n\ntranslate_sql(as.POSIXct(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as_datetime(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS TIMESTAMP)\n\ntranslate_sql(as.logical(var), con = con)\n\n&lt;SQL&gt; CAST(`var` AS BOOLEAN)",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#comparison-and-logical-operators",
    "href": "tidy_databases/tidyverse_expressions.html#comparison-and-logical-operators",
    "title": "3Â  Supported expressions for database queries",
    "section": "3.2 Comparison and logical operators",
    "text": "3.2 Comparison and logical operators\nBase R comparison operators, such as &lt;, &lt;=, ==, &gt;=, &gt;, are also well supported in all database backends. Logical operators, such as & and |, can also be used as if the data were in R.\n\n\n\n\n\n\nTipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\ncon &lt;- simulate_duckdb()\ntranslate_sql(var_1 == var_2, con = con)\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, con = con)\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), con = con)\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), con = con)\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ncon &lt;- simulate_postgres()\ntranslate_sql(var_1 == var_2, con = con)\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, con = con)\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), con = con)\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), con = con)\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ncon &lt;- simulate_mssql()\ntranslate_sql(var_1 == var_2, con = con)\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, con = con)\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), con = con)\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), con = con)\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ncon &lt;- simulate_redshift()\ntranslate_sql(var_1 == var_2, con = con)\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, con = con)\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), con = con)\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), con = con)\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ncon &lt;- simulate_snowflake()\ntranslate_sql(var_1 == var_2, con = con)\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, con = con)\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), con = con)\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), con = con)\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0\n\n\n\n\n\ncon &lt;- simulate_spark_sql()\ntranslate_sql(var_1 == var_2, con = con)\n\n&lt;SQL&gt; `var_1` = `var_2`\n\ntranslate_sql(var_1 &gt;= var_2, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= `var_2`\n\ntranslate_sql(var_1 &lt; 100, con = con)\n\n&lt;SQL&gt; `var_1` &lt; 100.0\n\ntranslate_sql(var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; `var_1` IN ('a', 'b', 'c')\n\ntranslate_sql(!var_1 %in% c(\"a\", \"b\", \"c\"), con = con)\n\n&lt;SQL&gt; NOT(`var_1` IN ('a', 'b', 'c'))\n\ntranslate_sql(is.na(var_1), con = con)\n\n&lt;SQL&gt; (`var_1` IS NULL)\n\ntranslate_sql(!is.na(var_1), con = con)\n\n&lt;SQL&gt; NOT((`var_1` IS NULL))\n\ntranslate_sql(var_1 &gt;= 100 & var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 AND `var_1` &lt; 200.0\n\ntranslate_sql(var_1 &gt;= 100 | var_1 &lt; 200, con = con)\n\n&lt;SQL&gt; `var_1` &gt;= 100.0 OR `var_1` &lt; 200.0",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#conditional-statements",
    "href": "tidy_databases/tidyverse_expressions.html#conditional-statements",
    "title": "3Â  Supported expressions for database queries",
    "section": "3.3 Conditional statements",
    "text": "3.3 Conditional statements\nThe base ifelse function, along with if_else and case_when from dplyr are translated for each database backend. As can be seen in the translations, case_when maps to the SQL CASE WHEN statement.\n\n\n\n\n\n\nTipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\ncon &lt;- simulate_duckdb()\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = NULL), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = \"something else\"), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ncon &lt;- simulate_postgres()\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = NULL), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = \"something else\"), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ncon &lt;- simulate_mssql()\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; IIF(`var` = 'a', 1, 2)\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; IIF(`var` = 'a', 1, 2)\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = NULL), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = \"something else\"), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ncon &lt;- simulate_redshift()\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = NULL), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = \"something else\"), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ncon &lt;- simulate_snowflake()\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = NULL), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = \"something else\"), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND\n\n\n\n\n\ncon &lt;- simulate_spark_sql()\ntranslate_sql(ifelse(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(if_else(var == \"a\", 1L, 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 WHEN NOT (`var` = 'a') THEN 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, .default = 2L), con = con)\n\n&lt;SQL&gt; CASE WHEN (`var` = 'a') THEN 1 ELSE 2 END\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = NULL), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nEND\n\ntranslate_sql(case_when(var == \"a\" ~ 1L, var == \"b\" ~ 2L, var == \"c\" ~ 3L, .default = \"something else\"), \n              con = con)\n\n&lt;SQL&gt; CASE\nWHEN (`var` = 'a') THEN 1\nWHEN (`var` = 'b') THEN 2\nWHEN (`var` = 'c') THEN 3\nELSE 'something else'\nEND",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#working-with-strings",
    "href": "tidy_databases/tidyverse_expressions.html#working-with-strings",
    "title": "3Â  Supported expressions for database queries",
    "section": "3.4 Working with strings",
    "text": "3.4 Working with strings\nCompared to the previous sections, there is much more variation in support of functions to work with strings across database management systems. In particular, although various useful stringr functions do have translations ubiquitously, it can be seen below that more translations are available for some databases compared to others.\n\n\n\n\n\n\nTipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\ncon &lt;- simulate_duckdb()\ntranslate_sql(nchar(var), con = con)\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), con = con)\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), con = con)\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), con = con)\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), con = con)\n\n&lt;SQL&gt; TRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g'))\n\ntranslate_sql(str_detect(var, \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, 'b')\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), con = con)\n\n&lt;SQL&gt; (NOT(REGEXP_MATCHES(`var`, 'b')))\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), con = con)\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, '[aeiou]')\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 'g')\n\ntranslate_sql(str_remove(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), con = con)\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, '^(?:' || 'a' || ')')\n\ntranslate_sql(str_ends(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_MATCHES(`var`, '(?:' || 'a' || ')$')\n\n\n\n\n\ncon &lt;- simulate_postgres()\ntranslate_sql(nchar(var), con = con)\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), con = con)\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), con = con)\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), con = con)\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g')))\n\ntranslate_sql(str_detect(var, \"b\"), con = con)\n\n&lt;SQL&gt; `var` ~ 'b'\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), con = con)\n\n&lt;SQL&gt; !(`var` ~ 'b')\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), con = con)\n\n&lt;SQL&gt; `var` ~ '[aeiou]'\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 'g')\n\ntranslate_sql(str_remove(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), con = con)\n\n&lt;SQL&gt; `var` ILIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), con = con)\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"), con = con)\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ncon &lt;- simulate_mssql()\ntranslate_sql(nchar(var), con = con)\n\n&lt;SQL&gt; LEN(`var`)\n\ntranslate_sql(nzchar(var), con = con)\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), con = con)\n\n&lt;SQL&gt; SUBSTRING(`var`, 1, 2)\n\ntranslate_sql(trimws(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), con = con)\n\nError in `str_to_title()`:\n! `str_to_title()` is not available in this SQL variant.\n\ntranslate_sql(str_trim(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), con = con)\n\nError in `str_squish()`:\n! `str_squish()` is not available in this SQL variant.\n\ntranslate_sql(str_detect(var, \"b\"), con = con)\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), con = con)\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), con = con)\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), con = con)\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), con = con)\n\nError in `str_replace_all()`:\n! `str_replace_all()` is not available in this SQL variant.\n\ntranslate_sql(str_remove(var, \"a\"), con = con)\n\nError in `str_remove()`:\n! `str_remove()` is not available in this SQL variant.\n\ntranslate_sql(str_remove_all(var, \"a\"), con = con)\n\nError in `str_remove_all()`:\n! `str_remove_all()` is not available in this SQL variant.\n\ntranslate_sql(str_like(var, \"a\"), con = con)\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), con = con)\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"), con = con)\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ncon &lt;- simulate_redshift()\ntranslate_sql(nchar(var), con = con)\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), con = con)\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), con = con)\n\n&lt;SQL&gt; SUBSTRING(`var`, 1, 2)\n\ntranslate_sql(trimws(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), con = con)\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(REGEXP_REPLACE(`var`, '\\s+', ' ', 'g')))\n\ntranslate_sql(str_detect(var, \"b\"), con = con)\n\n&lt;SQL&gt; `var` ~ 'b'\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), con = con)\n\n&lt;SQL&gt; !(`var` ~ 'b')\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), con = con)\n\n&lt;SQL&gt; `var` ~ '[aeiou]'\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), con = con)\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_remove(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '')\n\ntranslate_sql(str_remove_all(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 'g')\n\ntranslate_sql(str_like(var, \"a\"), con = con)\n\n&lt;SQL&gt; `var` ILIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), con = con)\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"), con = con)\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.\n\n\n\n\n\ncon &lt;- simulate_snowflake()\ntranslate_sql(nchar(var), con = con)\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), con = con)\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), con = con)\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), con = con)\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), con = con)\n\n&lt;SQL&gt; TRIM(`var`)\n\ntranslate_sql(str_squish(var), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(TRIM(`var`), '\\\\s+', ' ')\n\ntranslate_sql(str_detect(var, \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_INSTR(`var`, 'b') != 0\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), con = con)\n\n&lt;SQL&gt; REGEXP_INSTR(`var`, 'b') = 0\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), con = con)\n\n&lt;SQL&gt; REGEXP_INSTR(`var`, '[aeiou]') != 0\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b', 1.0, 1.0)\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', 'b')\n\ntranslate_sql(str_remove(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a', '', 1.0, 1.0)\n\ntranslate_sql(str_remove_all(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_REPLACE(`var`, 'a')\n\ntranslate_sql(str_like(var, \"a\"), con = con)\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_INSTR(`var`, 'a') = 1\n\ntranslate_sql(str_ends(var, \"a\"), con = con)\n\n&lt;SQL&gt; REGEXP_INSTR(`var`, 'a', 1, 1, 1) = (LENGTH(`var`) + 1)\n\n\n\n\n\ncon &lt;- simulate_spark_sql()\ntranslate_sql(nchar(var), con = con)\n\n&lt;SQL&gt; LENGTH(`var`)\n\ntranslate_sql(nzchar(var), con = con)\n\n&lt;SQL&gt; ((`var` IS NULL) OR `var` != '')\n\ntranslate_sql(substr(var, 1, 2), con = con)\n\n&lt;SQL&gt; SUBSTR(`var`, 1, 2)\n\ntranslate_sql(trimws(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(tolower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(str_to_lower(var), con = con)\n\n&lt;SQL&gt; LOWER(`var`)\n\ntranslate_sql(toupper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_upper(var), con = con)\n\n&lt;SQL&gt; UPPER(`var`)\n\ntranslate_sql(str_to_title(var), con = con)\n\n&lt;SQL&gt; INITCAP(`var`)\n\ntranslate_sql(str_trim(var), con = con)\n\n&lt;SQL&gt; LTRIM(RTRIM(`var`))\n\ntranslate_sql(str_squish(var), con = con)\n\nError in `str_squish()`:\n! `str_squish()` is not available in this SQL variant.\n\ntranslate_sql(str_detect(var, \"b\"), con = con)\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"b\", negate = TRUE), con = con)\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_detect(var, \"[aeiou]\"), con = con)\n\nError in `str_detect()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_replace(var, \"a\", \"b\"), con = con)\n\nError in `str_replace()`:\n! `str_replace()` is not available in this SQL variant.\n\ntranslate_sql(str_replace_all(var, \"a\", \"b\"), con = con)\n\nError in `str_replace_all()`:\n! `str_replace_all()` is not available in this SQL variant.\n\ntranslate_sql(str_remove(var, \"a\"), con = con)\n\nError in `str_remove()`:\n! `str_remove()` is not available in this SQL variant.\n\ntranslate_sql(str_remove_all(var, \"a\"), con = con)\n\nError in `str_remove_all()`:\n! `str_remove_all()` is not available in this SQL variant.\n\ntranslate_sql(str_like(var, \"a\"), con = con)\n\n&lt;SQL&gt; `var` LIKE 'a'\n\ntranslate_sql(str_starts(var, \"a\"), con = con)\n\nError in `str_starts()`:\n! Only fixed patterns are supported on database backends.\n\ntranslate_sql(str_ends(var, \"a\"), con = con)\n\nError in `str_ends()`:\n! Only fixed patterns are supported on database backends.",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#working-with-dates",
    "href": "tidy_databases/tidyverse_expressions.html#working-with-dates",
    "title": "3Â  Supported expressions for database queries",
    "section": "3.5 Working with dates",
    "text": "3.5 Working with dates\nLike with strings, support for working with dates is somewhat mixed. In general, we would use functions from the clock package such as get_day(), get_month(), get_year() to extract parts from a date, add_days() to add or subtract days to a date, and date_count_between() to get the number of days between two date variables.\n\n\n\n\n\n\nTipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\ncon &lt;- simulate_duckdb()\ntranslate_sql(get_day(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), con = con)\n\n&lt;SQL&gt; DATE_ADD(`date_1`, INTERVAL (1.0) day)\n\ntranslate_sql(add_years(date_1, 1), con = con)\n\n&lt;SQL&gt; DATE_ADD(`date_1`, INTERVAL (1.0) year)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), con = con)\n\n&lt;SQL&gt; DATEDIFF('day', `date_1`, `date_2`)\n\n\n\n\n\ncon &lt;- simulate_postgres()\ntranslate_sql(get_day(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), con = con)\n\n&lt;SQL&gt; (`date_1` + 1.0*INTERVAL'1 day')\n\ntranslate_sql(add_years(date_1, 1), con = con)\n\n&lt;SQL&gt; (`date_1` + 1.0*INTERVAL'1 year')\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), con = con)\n\n&lt;SQL&gt; `date_2` - `date_1`\n\n\n\n\n\ncon &lt;- simulate_mssql()\ntranslate_sql(get_day(date_1), con = con)\n\n&lt;SQL&gt; DATEPART(DAY, `date_1`)\n\ntranslate_sql(get_month(date_1), con = con)\n\n&lt;SQL&gt; DATEPART(MONTH, `date_1`)\n\ntranslate_sql(get_year(date_1), con = con)\n\n&lt;SQL&gt; DATEPART(YEAR, `date_1`)\n\ntranslate_sql(add_days(date_1, 1), con = con)\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), con = con)\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), con = con)\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\n\n\n\n\ncon &lt;- simulate_redshift()\ntranslate_sql(get_day(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('day', `date_1`)\n\ntranslate_sql(get_month(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('month', `date_1`)\n\ntranslate_sql(get_year(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('year', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), con = con)\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), con = con)\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), con = con)\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\n\n\n\n\ncon &lt;- simulate_snowflake()\ntranslate_sql(get_day(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART(DAY, `date_1`)\n\ntranslate_sql(get_month(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART(MONTH, `date_1`)\n\ntranslate_sql(get_year(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART(YEAR, `date_1`)\n\ntranslate_sql(add_days(date_1, 1), con = con)\n\n&lt;SQL&gt; DATEADD(DAY, 1.0, `date_1`)\n\ntranslate_sql(add_years(date_1, 1), con = con)\n\n&lt;SQL&gt; DATEADD(YEAR, 1.0, `date_1`)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), con = con)\n\n&lt;SQL&gt; DATEDIFF(DAY, `date_1`, `date_2`)\n\n\n\n\n\ncon &lt;- simulate_spark_sql()\ntranslate_sql(get_day(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('DAY', `date_1`)\n\ntranslate_sql(get_month(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('MONTH', `date_1`)\n\ntranslate_sql(get_year(date_1), con = con)\n\n&lt;SQL&gt; DATE_PART('YEAR', `date_1`)\n\ntranslate_sql(add_days(date_1, 1), con = con)\n\n&lt;SQL&gt; DATE_ADD(`date_1`, 1.0)\n\ntranslate_sql(add_years(date_1, 1), con = con)\n\n&lt;SQL&gt; ADD_MONTHS(`date_1`, 1.0 * 12.0)\n\ntranslate_sql(date_count_between(date_1, date_2, \"day\"), con = con)\n\n&lt;SQL&gt; DATEDIFF(`date_2`, `date_1`)",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#data-aggregation",
    "href": "tidy_databases/tidyverse_expressions.html#data-aggregation",
    "title": "3Â  Supported expressions for database queries",
    "section": "3.6 Data aggregation",
    "text": "3.6 Data aggregation\nWithin the context of using summarise(), we can get aggregated results across entire columns using functions such as n(), n_distinct(), sum(), min(), max(), mean(), and sd(). As can be seen below, the SQL for these calculations is similar across different database management systems.\n\n\n\n\n\n\nTipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\nlazy_frame(x = c(1, 2), con = simulate_duckdb()) |&gt;\n  summarise(\n    n = n(),\n    n_unique = n_distinct(x),\n    sum = sum(x, na.rm = TRUE),\n    sum_is_1 = sum(x == 1, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT row(`x`)) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1, 2), con = simulate_postgres()) |&gt;\n  summarise(\n    n = n(),\n    n_unique = n_distinct(x),\n    sum = sum(x, na.rm = TRUE),\n    sum_is_1 = sum(x == 1, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1, 2), con = simulate_mssql()) |&gt;\n  summarise(\n    n = n(),\n    n_unique = n_distinct(x),\n    sum = sum(x, na.rm = TRUE),\n    sum_is_1 = sum(x == 1, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT_BIG(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(CAST(IIF(`x` = 1.0, 1, 0) AS BIT)) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDEV(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1, 2), con = simulate_redshift()) |&gt;\n  summarise(\n    n = n(),\n    n_unique = n_distinct(x),\n    sum = sum(x, na.rm = TRUE),\n    sum_is_1 = sum(x == 1, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1, 2), con = simulate_snowflake()) |&gt;\n  summarise(\n    n = n(),\n    n_unique = n_distinct(x),\n    sum = sum(x, na.rm = TRUE),\n    sum_is_1 = sum(x == 1, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV(`x`) AS `sd`\nFROM `df`\n\n\n\n\n\nlazy_frame(x = c(1, 2), con = simulate_spark_sql()) |&gt;\n  summarise(\n    n = n(),\n    n_unique = n_distinct(x),\n    sum = sum(x, na.rm = TRUE),\n    sum_is_1 = sum(x == 1, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    mean = mean(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  COUNT(*) AS `n`,\n  COUNT(DISTINCT `x`) AS `n_unique`,\n  SUM(`x`) AS `sum`,\n  SUM(`x` = 1.0) AS `sum_is_1`,\n  MIN(`x`) AS `min`,\n  AVG(`x`) AS `mean`,\n  MAX(`x`) AS `max`,\n  STDDEV_SAMP(`x`) AS `sd`\nFROM `df`",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#window-functions",
    "href": "tidy_databases/tidyverse_expressions.html#window-functions",
    "title": "3Â  Supported expressions for database queries",
    "section": "3.7 Window functions",
    "text": "3.7 Window functions\nWindow functions differ from data aggregation functions in that they perform calculations across rows that are related to the current row, rather than collapsing multiple rows into a single result. For these operations, we use mutate() instead of summarise().\nFor instance, we can use window functions like cumsum() and cummean() to calculate running totals and averages, or lag() and lead() to help compare rows to their preceding or following rows.\nGiven that window functions compare rows to rows before or after them, we will often use arrange() or window_order() to specify the order of rows. This will translate into an ORDER BY clause in the SQL. In addition, we may well also want to apply window functions within some specific groupings in our data. Using group_by() would result in a PARTITION BY clause in the translated SQL so that the window function operates on each group independently.\n\n\n\n\n\n\nTipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\ncon &lt;- simulate_duckdb()\nlazy_frame(x = c(10, 20, 30), z = c(1, 2, 3), con = con) |&gt; \n  window_order(z) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20), y = c(\"a\", \"b\"), z = c(1, 2), con = con) |&gt;\n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\ncon &lt;- simulate_postgres()\nlazy_frame(x = c(10, 20, 30), z = c(1, 2, 3), con = con) |&gt; \n  window_order(z) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (ORDER BY `z`)\n\nlazy_frame(x = c(10, 20), y = c(\"a\", \"b\"), z = c(1, 2), con = con) |&gt;\n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (PARTITION BY `y` ORDER BY `z`)\n\n\n\n\n\ncon &lt;- simulate_mssql()\nlazy_frame(x = c(10, 20, 30), z = c(1, 2, 3), con = con) |&gt; \n  window_order(z) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20), y = c(\"a\", \"b\"), z = c(1, 2), con = con) |&gt;\n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\ncon &lt;- simulate_redshift()\nlazy_frame(x = c(10, 20, 30), z = c(1, 2, 3), con = con) |&gt; \n  window_order(z) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20), y = c(\"a\", \"b\"), z = c(1, 2), con = con) |&gt;\n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\ncon &lt;- simulate_snowflake()\nlazy_frame(x = c(10, 20, 30), z = c(1, 2, 3), con = con) |&gt; \n  window_order(z) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (ORDER BY `z`) AS `lead_x`\nFROM `df`\n\nlazy_frame(x = c(10, 20), y = c(\"a\", \"b\"), z = c(1, 2), con = con) |&gt;\n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `sum_x`,\n  AVG(`x`) OVER (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING) AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER (PARTITION BY `y` ORDER BY `z`) AS `lead_x`\nFROM `df`\n\n\n\n\n\ncon &lt;- simulate_spark_sql()\nlazy_frame(x = c(10, 20, 30), z = c(1, 2, 3), con = con) |&gt; \n  window_order(z) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (ORDER BY `z`)\n\nlazy_frame(x = c(10, 20), y = c(\"a\", \"b\"), z = c(1, 2), con = con) |&gt;\n  window_order(z) |&gt; \n  group_by(y) |&gt; \n  mutate(\n    sum_x = cumsum(x),\n    mean_x = cummean(x),\n    lag_x = lag(x), \n    lead_x = lead(x)\n  ) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  SUM(`x`) OVER `win1` AS `sum_x`,\n  AVG(`x`) OVER `win1` AS `mean_x`,\n  LAG(`x`, 1, NULL) OVER `win2` AS `lag_x`,\n  LEAD(`x`, 1, NULL) OVER `win2` AS `lead_x`\nFROM `df`\nWINDOW\n  `win1` AS (PARTITION BY `y` ORDER BY `z` ROWS UNBOUNDED PRECEDING),\n  `win2` AS (PARTITION BY `y` ORDER BY `z`)\n\n\n\n\n\n\n\n\nTODO add note arrange vs window_order\n\n\n\n\n\n\nNotearrange() vs window_order()\n\n\n\n\n\nAlthough arrange() and window_order() both involve ordering, they serve different purposes.\n\narrange(): changes the order of the final output of a table, by ordering it as the user specified.\nwindow_order(): defines the order within window functions. It controlls how functions (e.g., lag(), lead(), rank(), cumsum()) are applied across rows.\n\nIn conclusion, for all dialects, arrange() changes the output row order, while window_order() changes how window functions calculate values.",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/tidyverse_expressions.html#calculating-quantiles-including-the-median",
    "href": "tidy_databases/tidyverse_expressions.html#calculating-quantiles-including-the-median",
    "title": "3Â  Supported expressions for database queries",
    "section": "3.8 Calculating quantiles, including the median",
    "text": "3.8 Calculating quantiles, including the median\nSo far weâ€™ve seen that we can perform various data manipulations and calculate summary statistics for different database management systems using the same R code. Although the translated SQL has been different, the databases all supported similar approaches to perform these queries.\nA case where this is not true is when we are interested in summarising distributions of the data and estimating quantiles. For example, letâ€™s take estimating the median as an example. Some databases only support calculating the median as an aggregation function similar to how min, mean, and max were calculated above. However, some others only support it as a window function like lead and lag above. Unfortunately, this means that for some databases, quantiles can only be calculated using the summarise aggregation approach, while in others only the mutate window approach can be used.\n\n\n\n\n\n\nTipShow SQL\n\n\n\n\n\n\nDuckDBPostgresSQL ServerRedshiftSnowflakeSpark\n\n\n\ncon &lt;- simulate_duckdb()\nlazy_frame(x = c(1,2), con = con) |&gt;\n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT MEDIAN(`x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = con) |&gt;\n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `df`.*, MEDIAN(`x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\ncon &lt;- simulate_postgres()\nlazy_frame(x = c(1,2), con = con) |&gt;\n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = con) |&gt;\n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `mutate()` is not supported for\n  PostgreSQL.\nâ„¹ Use a combination of `summarise()` and `left_join()` instead:\n  `df %&gt;% left_join(summarise(&lt;col&gt; = median(x, na.rm = TRUE)))`.\n\n\n\n\n\ncon &lt;- simulate_mssql()\nlazy_frame(x = c(1,2), con = con) |&gt;\n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `summarise()` is not supported for SQL\n  Server.\nâ„¹ Use a combination of `distinct()` and `mutate()` for the same result:\n  `mutate(&lt;col&gt; = median(x, na.rm = TRUE)) %&gt;% distinct(&lt;col&gt;)`\n\nlazy_frame(x = c(1,2), con = con) |&gt;\n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\ncon &lt;- simulate_redshift()\nlazy_frame(x = c(1,2), con = con) |&gt;\n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = con) |&gt;\n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\nError in `median()`:\n! Translation of `median()` in `mutate()` is not supported for\n  PostgreSQL.\nâ„¹ Use a combination of `summarise()` and `left_join()` instead:\n  `df %&gt;% left_join(summarise(&lt;col&gt; = median(x, na.rm = TRUE)))`.\n\n\n\n\n\ncon &lt;- simulate_snowflake()\nlazy_frame(x = c(1,2), con = con) |&gt;\n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = con) |&gt;\n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  `df`.*,\n  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `x`) OVER () AS `median`\nFROM `df`\n\n\n\n\n\ncon &lt;- simulate_spark_sql()\nlazy_frame(x = c(1,2), con = con) |&gt;\n  summarise(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT MEDIAN(`x`) AS `median`\nFROM `df`\n\nlazy_frame(x = c(1,2), con = con) |&gt;\n  mutate(median = median(x, na.rm = TRUE)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT `df`.*, MEDIAN(`x`) OVER () AS `median`\nFROM `df`",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Supported expressions for database queries</span>"
    ]
  },
  {
    "objectID": "tidy_databases/data_model.html",
    "href": "tidy_databases/data_model.html",
    "title": "4Â  Building analytic pipelines for a data model",
    "section": "",
    "text": "4.1 Defining a data model\nIn the previous chapters, weâ€™ve seen that after connecting to a database, we can create references to the various tables weâ€™re interested in and write custom analytic code to query them. However, if we are working with the same database over and over again, we might want to build some tooling for tasks we often perform.\nTo see how we can develop a data model with associated methods and functions, we will use the Lahman baseball data. The data is stored across various related tables.\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(cli)\nlibrary(dbplyr)\nlibrary(Lahman)\n\ncon &lt;- dbConnect(drv = duckdb())\ncopy_lahman(con = con)\nInstead of manually creating references for each one of the tables (so we can access them easily), we will write a function to create a single reference to the Lahman data.\nlahmanFromCon &lt;- function(con) {\n  lahmanRef &lt;- set_names(c(\n    \"AllstarFull\", \"Appearances\", \"AwardsManagers\", \"AwardsPlayers\", \"AwardsManagers\",\n    \"AwardsShareManagers\", \"Batting\", \"BattingPost\", \"CollegePlaying\", \"Fielding\",\n    \"FieldingOF\", \"FieldingOFsplit\", \"FieldingPost\", \"HallOfFame\", \"HomeGames\",\n    \"LahmanData\", \"Managers\", \"ManagersHalf\", \"Parks\", \"People\", \"Pitching\",\n    \"PitchingPost\", \"Salaries\", \"Schools\", \"SeriesPost\", \"Teams\", \"TeamsFranchises\",\n    \"TeamsHalf\"\n  ))\n  \n  lahmanRef &lt;- map(lahmanRef, \\(x) tbl(src = con, from = x))\n  \n  class(lahmanRef) &lt;- c(\"lahman_ref\", class(lahmanRef))\n  return(lahmanRef)\n}\nWith this function we can now easily get references to all our Lahman tables in one go using our lahmanFromCon() function.\nlahman &lt;- lahmanFromCon(con = con)\n\nlahman$People |&gt;\n  glimpse()\n\nRows: ??\nColumns: 26\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n$ playerID     &lt;chr&gt; \"aardsda01\", \"aaronha01\", \"aaronto01\", \"aasedo01\", \"abadaâ€¦\n$ birthYear    &lt;int&gt; 1981, 1934, 1939, 1954, 1972, 1985, 1850, 1877, 1869, 186â€¦\n$ birthMonth   &lt;int&gt; 12, 2, 8, 9, 8, 12, 11, 4, 11, 10, 6, 9, 3, 10, 2, 8, 9, â€¦\n$ birthDay     &lt;int&gt; 27, 5, 5, 8, 25, 17, 4, 15, 11, 14, 1, 20, 16, 22, 16, 17â€¦\n$ birthCity    &lt;chr&gt; \"Denver\", \"Mobile\", \"Mobile\", \"Orange\", \"Palm Beach\", \"Laâ€¦\n$ birthCountry &lt;chr&gt; \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"D.R.\", \"USA\", \"USA\", â€¦\n$ birthState   &lt;chr&gt; \"CO\", \"AL\", \"AL\", \"CA\", \"FL\", \"La Romana\", \"PA\", \"PA\", \"Vâ€¦\n$ deathYear    &lt;int&gt; NA, 2021, 1984, NA, NA, NA, 1905, 1957, 1962, 1926, NA, Nâ€¦\n$ deathMonth   &lt;int&gt; NA, 1, 8, NA, NA, NA, 5, 1, 6, 4, NA, NA, 2, 6, NA, NA, Nâ€¦\n$ deathDay     &lt;int&gt; NA, 22, 16, NA, NA, NA, 17, 6, 11, 27, NA, NA, 13, 11, NAâ€¦\n$ deathCountry &lt;chr&gt; NA, \"USA\", \"USA\", NA, NA, NA, \"USA\", \"USA\", \"USA\", \"USA\",â€¦\n$ deathState   &lt;chr&gt; NA, \"GA\", \"GA\", NA, NA, NA, \"NJ\", \"FL\", \"VT\", \"CA\", NA, Nâ€¦\n$ deathCity    &lt;chr&gt; NA, \"Atlanta\", \"Atlanta\", NA, NA, NA, \"Pemberton\", \"Fort â€¦\n$ nameFirst    &lt;chr&gt; \"David\", \"Hank\", \"Tommie\", \"Don\", \"Andy\", \"Fernando\", \"Joâ€¦\n$ nameLast     &lt;chr&gt; \"Aardsma\", \"Aaron\", \"Aaron\", \"Aase\", \"Abad\", \"Abad\", \"Abaâ€¦\n$ nameGiven    &lt;chr&gt; \"David Allan\", \"Henry Louis\", \"Tommie Lee\", \"Donald Williâ€¦\n$ weight       &lt;int&gt; 215, 180, 190, 190, 184, 235, 192, 170, 175, 169, 192, 22â€¦\n$ height       &lt;int&gt; 75, 72, 75, 75, 73, 74, 72, 71, 71, 68, 72, 74, 71, 70, 7â€¦\n$ bats         &lt;fct&gt; R, R, R, R, L, L, R, R, R, L, L, R, R, R, R, R, L, R, L, â€¦\n$ throws       &lt;fct&gt; R, R, R, R, L, L, R, R, R, L, L, R, R, R, R, L, L, R, L, â€¦\n$ debut        &lt;chr&gt; \"2004-04-06\", \"1954-04-13\", \"1962-04-10\", \"1977-07-26\", \"â€¦\n$ bbrefID      &lt;chr&gt; \"aardsda01\", \"aaronha01\", \"aaronto01\", \"aasedo01\", \"abadaâ€¦\n$ finalGame    &lt;chr&gt; \"2015-08-23\", \"1976-10-03\", \"1971-09-26\", \"1990-10-03\", \"â€¦\n$ retroID      &lt;chr&gt; \"aardd001\", \"aaroh101\", \"aarot101\", \"aased001\", \"abada001â€¦\n$ deathDate    &lt;date&gt; NA, 2021-01-22, 1984-08-16, NA, NA, NA, 1905-05-17, 1957â€¦\n$ birthDate    &lt;date&gt; 1981-12-27, 1934-02-05, 1939-08-05, 1954-09-08, 1972-08-â€¦",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "tidy_databases/data_model.html#defining-a-data-model",
    "href": "tidy_databases/data_model.html#defining-a-data-model",
    "title": "4Â  Building analytic pipelines for a data model",
    "section": "",
    "text": "Notecopy_lahman\n\n\n\n\n\nThe copy_lahman() function inserts all the different tables in the connection. It works in the same way as we have done before with the for loop and the dbWriteTable() function.\nSee that there are 28 new tables inserted in our DuckDB database:\n\ndbListTables(conn = con)\n\n [1] \"AllstarFull\"         \"Appearances\"         \"AwardsManagers\"     \n [4] \"AwardsPlayers\"       \"AwardsShareManagers\" \"AwardsSharePlayers\" \n [7] \"Batting\"             \"BattingPost\"         \"CollegePlaying\"     \n[10] \"Fielding\"            \"FieldingOF\"          \"FieldingOFsplit\"    \n[13] \"FieldingPost\"        \"HallOfFame\"          \"HomeGames\"          \n[16] \"LahmanData\"          \"Managers\"            \"ManagersHalf\"       \n[19] \"Parks\"               \"People\"              \"Pitching\"           \n[22] \"PitchingPost\"        \"Salaries\"            \"Schools\"            \n[25] \"SeriesPost\"          \"Teams\"               \"TeamsFranchises\"    \n[28] \"TeamsHalf\"          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteThe dm package\n\n\n\n\n\nIn this chapter we will be creating a bespoke data model for our database. This approach can be further extended using the dm package, which also provides various helpful functions for creating a data model and working with it.\nSimilar to above, we can use dm() to create a single object to access our database tables.\n\nlibrary(dm)\nlahman_dm &lt;- dm(batting = tbl(con, \"Batting\"), people = tbl(con, \"People\"))\nlahman_dm\n\nâ”€â”€ Table source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsrc:  DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\nâ”€â”€ Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTables: `batting`, `people`\nColumns: 48\nPrimary keys: 0\nForeign keys: 0\n\n\nUsing this approach, we can make use of various utility functions. For example here we specify primary and foreign keys and then check that the key constraints are satisfied.\n\nlahman_dm &lt;- lahman_dm |&gt;\n  dm_add_pk(table = \"people\", columns = \"playerID\") |&gt;\n  dm_add_fk(table = \"batting\", columns = \"playerID\", ref_table = \"people\") \n\nlahman_dm\n\nâ”€â”€ Table source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsrc:  DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\nâ”€â”€ Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTables: `batting`, `people`\nColumns: 48\nPrimary keys: 1\nForeign keys: 1\n\ndm_examine_constraints(.dm = lahman_dm)\n\nâ„¹ All constraints satisfied.\n\n\nFor more information on the dm package see https://dm.cynkra.com/index.html",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "tidy_databases/data_model.html#creating-functions-for-the-data-model",
    "href": "tidy_databases/data_model.html#creating-functions-for-the-data-model",
    "title": "4Â  Building analytic pipelines for a data model",
    "section": "4.2 Creating functions for the data model",
    "text": "4.2 Creating functions for the data model\nGiven that we know the structure of the data, we can build a set of functions tailored to the Lahman data model to simplify data analyses.\nLetâ€™s start by creating a simple function that returns the teams each player has played for. We can see that the code we use follows on from the last couple of chapters.\n\ngetTeams &lt;- function(lahman, name = \"Barry Bonds\") {\n  lahman$Batting |&gt;\n    inner_join(\n      lahman$People |&gt;\n        mutate(full_name = paste0(nameFirst, \" \", nameLast)) |&gt;\n        filter(full_name %in% name) |&gt;\n        select(\"playerID\"),\n      by = \"playerID\"\n    ) |&gt;\n    distinct(teamID, yearID) |&gt;\n    left_join(\n      lahman$Teams, \n      by = c(\"teamID\", \"yearID\")) |&gt;\n    distinct(name)\n}\n\nNow we can easily get which teams a player has represented. We can see how changing the player name changes the SQL that is run behind the scenes.\n\ngetTeams(lahman = lahman, name = \"Babe Ruth\")\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  name            \n  &lt;chr&gt;           \n1 Boston Red Sox  \n2 New York Yankees\n3 Boston Braves   \n\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT DISTINCT q01.*\nFROM (\n  SELECT \"name\"\n  FROM (\n    SELECT DISTINCT q01.*\n    FROM (\n      SELECT teamID, yearID\n      FROM Batting\n      INNER JOIN (\n        SELECT playerID\n        FROM (\n          SELECT People.*, CONCAT_WS('', nameFirst, ' ', nameLast) AS full_name\n          FROM People\n        ) q01\n        WHERE (full_name IN ('Babe Ruth'))\n      ) RHS\n        ON (Batting.playerID = RHS.playerID)\n    ) q01\n  ) LHS\n  LEFT JOIN Teams\n    ON (LHS.teamID = Teams.teamID AND LHS.yearID = Teams.yearID)\n) q01\n\n\n\n\n\n\ngetTeams(lahman = lahman, name = \"Barry Bonds\")\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  name                \n  &lt;chr&gt;               \n1 Pittsburgh Pirates  \n2 San Francisco Giants\n\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT DISTINCT q01.*\nFROM (\n  SELECT \"name\"\n  FROM (\n    SELECT DISTINCT q01.*\n    FROM (\n      SELECT teamID, yearID\n      FROM Batting\n      INNER JOIN (\n        SELECT playerID\n        FROM (\n          SELECT People.*, CONCAT_WS('', nameFirst, ' ', nameLast) AS full_name\n          FROM People\n        ) q01\n        WHERE (full_name IN ('Barry Bonds'))\n      ) RHS\n        ON (Batting.playerID = RHS.playerID)\n    ) q01\n  ) LHS\n  LEFT JOIN Teams\n    ON (LHS.teamID = Teams.teamID AND LHS.yearID = Teams.yearID)\n) q01\n\n\n\n\n\n\n\n\n\n\n\nTipChoosing the right time to collect data into R\n\n\n\n\n\nThe function collect() brings data out of the database and into R. When working with large datasets, as is often the case when interacting with a database, we typically want to keep as much computation as possible on the database side. In the case of our getTeams() function, for example, everything is done on the database side. Collecting the result will bring the result of the teams the person played for out of the database. In this case, we could also use pull() to get our result out as a vector rather than a data frame.\n\ngetTeams(lahman = lahman, name = \"Barry Bonds\") |&gt;\n  collect()\n\n# A tibble: 2 Ã— 1\n  name                \n  &lt;chr&gt;               \n1 Pittsburgh Pirates  \n2 San Francisco Giants\n\ngetTeams(lahman = lahman, name = \"Barry Bonds\") |&gt;\n  pull()\n\n[1] \"Pittsburgh Pirates\"   \"San Francisco Giants\"\n\n\nHowever, in other cases we may need to collect the data to perform analyses that can not be done in SQL. This might be the case for plotting or for other analytic steps(i.e., fitting statistical models). In such cases, it is important to only bring out the data that we need (as our local computer will typically have far less memory available than the database system).\n\n\n\nSimilarly, we can make a function to add a playerâ€™s year of birth to another Lahman table.\n\naddBirthCountry &lt;- function(x){\n  x |&gt; \n    left_join(\n      lahman$People |&gt; \n        select(\"playerID\", \"birthCountry\"),\n      by = \"playerID\"\n    )\n}\n\n\nlahman$Batting |&gt;\n  addBirthCountry()\n\n# Source:   SQL [?? x 23]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   playerID  yearID stint teamID lgID      G    AB     R     H   X2B   X3B    HR\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL       11     0     0     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL       45     2     0     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL       25     0     0     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL       47     1     0     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL       73     0     0     0     0     0     0\n 6 aardsda01   2010     1 SEA    AL       53     0     0     0     0     0     0\n 7 aardsda01   2012     1 NYA    AL        1     0     0     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL       43     0     0     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL       33     1     0     0     0     0     0\n10 aaronha01   1954     1 ML1    NL      122   468    58   131    27     6    13\n# â„¹ more rows\n# â„¹ 11 more variables: RBI &lt;int&gt;, SB &lt;int&gt;, CS &lt;int&gt;, BB &lt;int&gt;, SO &lt;int&gt;,\n#   IBB &lt;int&gt;, HBP &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;, birthCountry &lt;chr&gt;\n\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT Batting.*, birthCountry\nFROM Batting\nLEFT JOIN People\n  ON (Batting.playerID = People.playerID)\n\n\n\n\n\n\nlahman$Pitching |&gt;\n  addBirthCountry()\n\n# Source:   SQL [?? x 31]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   playerID  yearID stint teamID lgID      W     L     G    GS    CG   SHO    SV\n   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 aardsda01   2004     1 SFN    NL        1     0    11     0     0     0     0\n 2 aardsda01   2006     1 CHN    NL        3     0    45     0     0     0     0\n 3 aardsda01   2007     1 CHA    AL        2     1    25     0     0     0     0\n 4 aardsda01   2008     1 BOS    AL        4     2    47     0     0     0     0\n 5 aardsda01   2009     1 SEA    AL        3     6    73     0     0     0    38\n 6 aardsda01   2010     1 SEA    AL        0     6    53     0     0     0    31\n 7 aardsda01   2012     1 NYA    AL        0     0     1     0     0     0     0\n 8 aardsda01   2013     1 NYN    NL        2     2    43     0     0     0     0\n 9 aardsda01   2015     1 ATL    NL        1     1    33     0     0     0     0\n10 aasedo01    1977     1 BOS    AL        6     2    13    13     4     2     0\n# â„¹ more rows\n# â„¹ 19 more variables: IPouts &lt;int&gt;, H &lt;int&gt;, ER &lt;int&gt;, HR &lt;int&gt;, BB &lt;int&gt;,\n#   SO &lt;int&gt;, BAOpp &lt;dbl&gt;, ERA &lt;dbl&gt;, IBB &lt;int&gt;, WP &lt;int&gt;, HBP &lt;int&gt;, BK &lt;int&gt;,\n#   BFP &lt;int&gt;, GF &lt;int&gt;, R &lt;int&gt;, SH &lt;int&gt;, SF &lt;int&gt;, GIDP &lt;int&gt;,\n#   birthCountry &lt;chr&gt;\n\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT Pitching.*, birthCountry\nFROM Pitching\nLEFT JOIN People\n  ON (Pitching.playerID = People.playerID)\n\n\n\n\n\nWe can then use our addBirthCountry() function as part of a larger query to summarise and plot the proportion of players from each country over time (based on their presence in the batting table).\n\nplot_data &lt;- lahman$Batting |&gt;\n  select(\"playerID\", \"yearID\") |&gt; \n  addBirthCountry() |&gt;\n  filter(yearID &gt; 1960) |&gt; \n  mutate(birthCountry = case_when(\n    birthCountry == \"USA\" ~ \"USA\",\n    birthCountry == \"D.R.\" ~ \"Dominican Republic\",\n    birthCountry == \"Venezuela\" ~ \"Venezuela\",\n    birthCountry == \"P.R.\" ~ \"Puerto Rico \",\n    birthCountry == \"Cuba\" ~ \"Cuba\",\n    birthCountry == \"CAN\" ~ \"Canada\",\n    birthCountry == \"Mexico\" ~ \"Mexico\",\n    .default = \"Other\"\n  )) |&gt; \n  group_by(yearID, birthCountry) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt; \n  group_by(yearID) |&gt;\n  mutate(percentage = n / sum(n) * 100) |&gt; \n  ungroup() |&gt; \n  collect()\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT q01.*, (n / SUM(n) OVER (PARTITION BY yearID)) * 100.0 AS percentage\nFROM (\n  SELECT yearID, birthCountry, COUNT(*) AS n\n  FROM (\n    SELECT\n      playerID,\n      yearID,\n      CASE\nWHEN (birthCountry = 'USA') THEN 'USA'\nWHEN (birthCountry = 'D.R.') THEN 'Dominican Republic'\nWHEN (birthCountry = 'Venezuela') THEN 'Venezuela'\nWHEN (birthCountry = 'P.R.') THEN 'Puerto Rico '\nWHEN (birthCountry = 'Cuba') THEN 'Cuba'\nWHEN (birthCountry = 'CAN') THEN 'Canada'\nWHEN (birthCountry = 'Mexico') THEN 'Mexico'\nELSE 'Other'\nEND AS birthCountry\n    FROM (\n      SELECT Batting.playerID AS playerID, yearID, birthCountry\n      FROM Batting\n      LEFT JOIN People\n        ON (Batting.playerID = People.playerID)\n    ) q01\n    WHERE (yearID &gt; 1960.0)\n  ) q01\n  GROUP BY yearID, birthCountry\n) q01\n\n\n\n\n\n\nlibrary(ggplot2)\nplot_data |&gt; \n  ggplot() +\n  geom_col(\n    mapping = aes(yearID, percentage, fill = birthCountry), \n    width = 1\n  ) + \n  theme_minimal() + \n  theme(\n    legend.title = element_blank(), \n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDefining methods for the data model\n\n\n\n\n\nAs part of our lahmanFromCon() function, our data model object has the class â€œlahman_refâ€. Therefore, apart from creating user-friendly functions to work with our Lahman data model, we can also define methods for this object.\n\nclass(lahman)\n\n[1] \"lahman_ref\" \"list\"      \n\n\nWith this we can make some specific methods for a â€œlahman_refâ€ object. For example, we can define a print method like so:\n\nprint.lahman_ref &lt;- function(x, ...) {\n  len &lt;- length(names(x))\n  cli_h1(\"# Lahman reference - {len} tables\")\n  cli_li(paste(\"{.strong tables:}\", paste(names(x), collapse = \", \")))\n  invisible(x)\n}\n\nNow we can see a summary of our Lahman data model when we print the object.\n\nlahman\n\n\n\n\nâ”€â”€ # Lahman reference - 28 tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nâ€¢ tables: AllstarFull, Appearances, AwardsManagers, AwardsPlayers,\nAwardsManagers, AwardsShareManagers, Batting, BattingPost, CollegePlaying,\nFielding, FieldingOF, FieldingOFsplit, FieldingPost, HallOfFame, HomeGames,\nLahmanData, Managers, ManagersHalf, Parks, People, Pitching, PitchingPost,\nSalaries, Schools, SeriesPost, Teams, TeamsFranchises, TeamsHalf\n\n\nAnd we can see that this print is being done by the method we defined.\n\nlibrary(sloop)\ns3_dispatch(print(lahman))\n\n=&gt; print.lahman_ref\n   print.list\n * print.default",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "tidy_databases/data_model.html#building-efficient-analytic-pipelines",
    "href": "tidy_databases/data_model.html#building-efficient-analytic-pipelines",
    "title": "4Â  Building analytic pipelines for a data model",
    "section": "4.3 Building efficient analytic pipelines",
    "text": "4.3 Building efficient analytic pipelines\n\n4.3.1 The risk of â€œcleanâ€ R code\nFollowing on from the above approach, we might think it is a good idea to make another function addBirthYear(). We can then use it along with our addBirthCountry() to get a summarised average salary by birth country and birth year.\n\naddBirthYear &lt;- function(lahmanTbl){\n  lahmanTbl |&gt; \n    left_join(\n      lahman$People |&gt; \n        select(\"playerID\", \"birthYear\"),\n      by = \"playerID\"\n    )\n}\n\nlahman$Salaries |&gt; \n  addBirthCountry() |&gt; \n  addBirthYear() |&gt; \n  group_by(birthCountry, birthYear) |&gt;\n  summarise(average_salary = mean(salary), .groups = \"drop\")\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   birthCountry birthYear average_salary\n   &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;\n 1 USA               1966       1761151.\n 2 Venezuela         1974       4269365.\n 3 D.R.              1984       2924854.\n 4 Mexico            1982       1174912.\n 5 Panama            1981        555833.\n 6 USA               1978       3133596.\n 7 P.R.              1959        297786.\n 8 USA               1961        811250.\n 9 USA               1990        728740.\n10 USA               1950        625076.\n# â„¹ more rows\n\n\nAlthough the R code looks fine, when we look at the SQL we can see that our query has two joins to the People table. One join gets information on the birth country and the other on the birth year.\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT birthCountry, birthYear, AVG(salary) AS average_salary\nFROM (\n  SELECT\n    Salaries.*,\n    \"People...2\".birthCountry AS birthCountry,\n    \"People...3\".birthYear AS birthYear\n  FROM Salaries\n  LEFT JOIN People \"People...2\"\n    ON (Salaries.playerID = \"People...2\".playerID)\n  LEFT JOIN People \"People...3\"\n    ON (Salaries.playerID = \"People...3\".playerID)\n) q01\nGROUP BY birthCountry, birthYear\n\n\n\n\n\nTo improve the performance of the code, we can build a single function to get simultaneously the birth country and birth year, so only one join is done.\n\naddCharacteristics &lt;- function(lahmanTbl){\n  lahmanTbl |&gt; \n    left_join(\n      lahman$People |&gt; \n        select(\"playerID\", \"birthYear\", \"birthCountry\"),\n      by = \"playerID\"\n    )\n}\n\nlahman$Salaries |&gt; \n  addCharacteristics() |&gt; \n  group_by(birthCountry, birthYear) |&gt;\n  summarise(average_salary = mean(salary), .groups = \"drop\")\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   birthCountry birthYear average_salary\n   &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;\n 1 USA               1954        860729.\n 2 USA               1967       1833526.\n 3 D.R.              1977       2724606.\n 4 Mexico            1970        783395.\n 5 USA               1958        960490.\n 6 D.R.              1991        582794.\n 7 USA               1971       1547025.\n 8 USA               1963       1585150.\n 9 USA               1962       1303970.\n10 P.R.              1982       4261730.\n# â„¹ more rows\n\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT birthCountry, birthYear, AVG(salary) AS average_salary\nFROM (\n  SELECT Salaries.*, birthYear, birthCountry\n  FROM Salaries\n  LEFT JOIN People\n    ON (Salaries.playerID = People.playerID)\n) q01\nGROUP BY birthCountry, birthYear\n\n\n\n\n\nThis query produces the same result but is simpler than the previous one, thus reducing the computational cost of the analysis. This shows the importance of being aware of the SQL code being executed when working in R with databases.\n\n\n4.3.2 Piping and SQL\nPiping functions has little impact on performance when using R with data in memory. However, when working with a database, the SQL generated will differ when using multiple function calls (with a separate operation specified in each) instead of multiple operations within a single function call.\nFor example, a single mutate function creating two new variables would generate the below SQL.\n\nlahman$People |&gt; \n  mutate(\n    birthDatePlus1 = add_years(x = birthDate, n = 1L),\n    birthDatePlus10 = add_years(x = birthDate, n = 10L)\n  ) |&gt; \n  select(\"playerID\", \"birthDatePlus1\", \"birthDatePlus10\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  playerID,\n  DATE_ADD(birthDate, INTERVAL (1) year) AS birthDatePlus1,\n  DATE_ADD(birthDate, INTERVAL (10) year) AS birthDatePlus10\nFROM People\n\n\nWhereas the SQL will be different if these were created using multiple mutate calls (with now one being created in a sub-query).\n\nlahman$People |&gt; \n  mutate(birthDatePlus1 = add_years(x = birthDate, n = 1L)) |&gt; \n  mutate(birthDatePlus10 = add_years(x = birthDate, n = 10L)) |&gt; \n  select(\"playerID\", \"birthDatePlus1\", \"birthDatePlus10\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  playerID,\n  birthDatePlus1,\n  DATE_ADD(birthDate, INTERVAL (10) year) AS birthDatePlus10\nFROM (\n  SELECT People.*, DATE_ADD(birthDate, INTERVAL (1) year) AS birthDatePlus1\n  FROM People\n) q01\n\n\n\n\n4.3.3 Computing intermediate queries\nLetâ€™s now summarise home runs (Batting table) and strike outs (Pitching table) by college player and their birth year. We can do this like so:\n\nplayers_with_college &lt;- lahman$People |&gt; \n  select(\"playerID\", \"birthYear\") |&gt; \n  inner_join(\n    lahman$CollegePlaying |&gt; \n      filter(!is.na(schoolID)) |&gt; \n      distinct(playerID, schoolID),\n    by = \"playerID\"\n  )\n\nlahman$Batting |&gt; \n  left_join(\n    players_with_college,\n    by = \"playerID\"\n  ) |&gt; \n  group_by(schoolID, birthYear) |&gt;\n  summarise(home_runs = sum(H, na.rm = TRUE), .groups = \"drop\") |&gt; \n  collect()\n\n# A tibble: 6,205 Ã— 3\n   schoolID   birthYear home_runs\n   &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1 gamiddl         1972         2\n 2 illinoisst      1981         1\n 3 lehigh          1901         1\n 4 tamukvill       1978         0\n 5 chicago         1874         2\n 6 capalom         1963        15\n 7 hawaiipac       1971       299\n 8 bostonuniv      1929       135\n 9 byu             1961        28\n10 kentucky        1985         1\n# â„¹ 6,195 more rows\n\nlahman$Pitching |&gt; \n  left_join(\n    players_with_college, \n    by = \"playerID\"\n  ) |&gt; \n  group_by(schoolID, birthYear) |&gt;\n  summarise(strike_outs = sum(SO, na.rm = TRUE), .groups = \"drop\")|&gt; \n  collect()\n\n# A tibble: 3,663 Ã— 3\n   schoolID   birthYear strike_outs\n   &lt;chr&gt;          &lt;int&gt;       &lt;dbl&gt;\n 1 pennst          1981         340\n 2 cacerri         1971         327\n 3 usc             1947         275\n 4 pepperdine      1969           4\n 5 lsu             1978         162\n 6 miamidade       1982          56\n 7 upperiowa       1918          11\n 8 jamesmad        1966           4\n 9 flinternat      1971         133\n10 ucla            1984         323\n# â„¹ 3,653 more rows\n\n\nIf we look at the SQL code we will realise that there is code duplication, because as part of each full query, we have run our players_with_college query.\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(H) AS home_runs\nFROM (\n  SELECT Batting.*, birthYear, schoolID\n  FROM Batting\n  LEFT JOIN (\n    SELECT People.playerID AS playerID, birthYear, schoolID\n    FROM People\n    INNER JOIN (\n      SELECT DISTINCT playerID, schoolID\n      FROM CollegePlaying\n      WHERE (NOT((schoolID IS NULL)))\n    ) RHS\n      ON (People.playerID = RHS.playerID)\n  ) RHS\n    ON (Batting.playerID = RHS.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(SO) AS strike_outs\nFROM (\n  SELECT Pitching.*, birthYear, schoolID\n  FROM Pitching\n  LEFT JOIN (\n    SELECT People.playerID AS playerID, birthYear, schoolID\n    FROM People\n    INNER JOIN (\n      SELECT DISTINCT playerID, schoolID\n      FROM CollegePlaying\n      WHERE (NOT((schoolID IS NULL)))\n    ) RHS\n      ON (People.playerID = RHS.playerID)\n  ) RHS\n    ON (Pitching.playerID = RHS.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n\n\n\nTo avoid this, we can make use of the compute() function to force the computation of the players_with_college query to a temporary table in the database.\n\nplayers_with_college &lt;- players_with_college |&gt; \n  compute()\n\nNow we have a temporary table with the result of our players_with_college query, and we can use this in both of our aggregation queries.\n\nplayers_with_college |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT *\nFROM dbplyr_q4u2R5KrRY\n\n\n\nlahman$Batting |&gt; \n  left_join(players_with_college, by = \"playerID\") |&gt; \n  group_by(schoolID, birthYear) |&gt;\n  summarise(home_runs = sum(H, na.rm = TRUE), .groups = \"drop\") |&gt; \n  collect()\n\n# A tibble: 6,205 Ã— 3\n   schoolID   birthYear home_runs\n   &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1 vermont         1869        38\n 2 michigan        1967         2\n 3 nmstate         1968         0\n 4 cacerri         1971         3\n 5 chicago         1874         2\n 6 byu             1961        28\n 7 pepperdine      1969         1\n 8 lsu             1978         2\n 9 miamidade       1982         0\n10 stanford        1961      1611\n# â„¹ 6,195 more rows\n\nlahman$Pitching |&gt; \n  left_join(players_with_college, by = \"playerID\") |&gt; \n  group_by(schoolID, birthYear) |&gt;\n  summarise(strike_outs = sum(SO, na.rm = TRUE), .groups = \"drop\") |&gt; \n  collect()\n\n# A tibble: 3,663 Ã— 3\n   schoolID  birthYear strike_outs\n   &lt;chr&gt;         &lt;int&gt;       &lt;dbl&gt;\n 1 longbeach      1968         273\n 2 elon           1921          13\n 3 lehigh         1901           1\n 4 usc            1947         275\n 5 tamukvill      1978         409\n 6 stanford       1972         218\n 7 upenn          1964          14\n 8 arkansas       1962         537\n 9 kentucky       1985          91\n10 txsjjcn        1983         571\n# â„¹ 3,653 more rows\n\n\n\n\n\n\n\n\nNoteShow query\n\n\n\n\n\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(H) AS home_runs\nFROM (\n  SELECT Batting.*, birthYear, schoolID\n  FROM Batting\n  LEFT JOIN dbplyr_q4u2R5KrRY\n    ON (Batting.playerID = dbplyr_q4u2R5KrRY.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n&lt;SQL&gt;\nSELECT schoolID, birthYear, SUM(SO) AS strike_outs\nFROM (\n  SELECT Pitching.*, birthYear, schoolID\n  FROM Pitching\n  LEFT JOIN dbplyr_q4u2R5KrRY\n    ON (Pitching.playerID = dbplyr_q4u2R5KrRY.playerID)\n) q01\nGROUP BY schoolID, birthYear\n\n\n\n\n\nIn this example, the SQL code of the intermediate table, players_with_college, was quite simple. However, in some cases, the SQL associated code can become very complicated and unmanageable, resulting with inefficient code. Therefore, although we do not want to overuse computation of intermediate queries, it is often useful when creating our analytic pipelines.\n\n\n\n\n\n\nNoteIndexes\n\n\n\n\n\nSome SQL dialects use indexes for more efficient â€˜joinsâ€™ performance. Briefly speaking, indexes store the location of the different values of a column. Every time that you create a new table with compute(), the indexes will not be carried over. Hence, if you want your new table to keep some indexes, you will have to add them manually. That is why sometimes it will not be more efficient to add a compute() in between, because the new table generated will not have the indexes that make your query to be executed faster.",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "tidy_databases/data_model.html#disconnecting-from-the-database",
    "href": "tidy_databases/data_model.html#disconnecting-from-the-database",
    "title": "4Â  Building analytic pipelines for a data model",
    "section": "4.4 Disconnecting from the database",
    "text": "4.4 Disconnecting from the database\nNow that we have reached the end of this example, we can close our connection to the database.\n\ndbDisconnect(conn = con)",
    "crumbs": [
      "Getting started with databases from R",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Building analytic pipelines for a data model</span>"
    ]
  },
  {
    "objectID": "omop/index.html",
    "href": "omop/index.html",
    "title": "Working with the OMOP CDM from R",
    "section": "",
    "text": "In this second half of the book, we will focus on how we can work with data in the OMOP CDM format from R.\n\nIn 5Â  Creating a CDM reference we will see how to create a cdm_reference in R, a data model that contains references to the OMOP CDM tables and provides the foundation for analysis.\nThe OMOP CDM is a person-centric model, and the person and observation period tables are two key tables for any analysis. In 6Â  Exploring the OMOP CDM we will see more on how these tables can be used as the starting point for identifying your study participants.\nIn 7Â  Identifying patient characteristics we will see how to add demographics information to different tables of interest and summarise it using dplyr code. Finally, we will also see how to use tidyverse verbs to add some custom features.\nIn 8Â  Adding cohorts to the CDM we will have a look at the cohort object, how it is defined and what are their attributes. We will also see how to create some simple base cohorts and apply some inclusion criteria to them.\nFinally, in 9Â  Working with cohorts we will learn how to intersect cohorts with one another, extracting counts, presence indicators, specific dates, or time differences to obtain the information of interest for our study population.",
    "crumbs": [
      "Working with the OMOP CDM from R"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html",
    "href": "omop/cdm_reference.html",
    "title": "5Â  Creating a CDM reference",
    "section": "",
    "text": "5.1 The OMOP CDM layout\nThe OMOP CDM standardises the structure of healthcare data. Data is stored across a system of tables with established relationships between them. In other words, the OMOP CDM provides a relational database structure, with version 5.4 of the OMOP CDM shown below.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#the-omop-cdm-layout",
    "href": "omop/cdm_reference.html#the-omop-cdm-layout",
    "title": "5Â  Creating a CDM reference",
    "section": "",
    "text": "FigureÂ 5.1: OMOP CDM v5.4 entity-relationship diagram retrieved from https://ohdsi.github.io/CommonDataModel/cdm54erd.html",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#creating-a-reference-to-the-omop-cdm",
    "href": "omop/cdm_reference.html#creating-a-reference-to-the-omop-cdm",
    "title": "5Â  Creating a CDM reference",
    "section": "5.2 Creating a reference to the OMOP CDM",
    "text": "5.2 Creating a reference to the OMOP CDM\nAs we saw in Chapter 4, creating a data model in R to represent the OMOP CDM can provide a basis for analytic pipelines using the data. Luckily for us, we wonâ€™t have to create functions and methods for this ourselves. Instead, we will use the omopgenerics package which defines a data model for OMOP CDM data and the CDMConnector package which provides functions for connecting to OMOP CDM data held in a database.\nTo see how this works, we will use the omock package to create example data in the format of the OMOP CDM, which we will then copy to a DuckDB database created by the duckdb package.\n\nlibrary(duckdb)\nlibrary(dplyr)\nlibrary(omock)\nlibrary(CDMConnector)\nlibrary(palmerpenguins)\n\ncdm_local &lt;- mockCdmReference() |&gt;\n  mockPerson(nPerson = 100) |&gt;\n  mockObservationPeriod() |&gt;\n  mockConditionOccurrence() |&gt;\n  mockDrugExposure() |&gt;\n  mockObservation() |&gt;\n  mockMeasurement() |&gt;\n  mockVisitOccurrence() |&gt;\n  mockProcedureOccurrence()\n\ncon &lt;- dbConnect(drv = duckdb())\nsrc &lt;- dbSource(con = con, writeSchema = \"main\")\n\ncdm &lt;- insertCdmTo(cdm = cdm_local, to = src)\n\nNote that insertCdmTo() output is already a &lt;cdm_reference&gt; object. But how would we create this cdm reference from the connection? We can use the function cdmFromCon() from CDMConnector to create our cdm reference. Note that as well as specifying the schema containing our OMOP CDM tables, we will also specify a write schema where any database tables we create during our analysis will be stored. Often, our OMOP CDM tables will be in a schema that we only have read-access to, and weâ€™ll have another schema where we can have write-access and where intermediate tables can be created for a given study.\n\ncdm &lt;- cdmFromCon(con = con, \n                  cdmSchema = \"main\", \n                  writeSchema = \"main\",\n                  cdmName = \"example_data\")\n\n\ncdm\n\n\n\n\nâ”€â”€ # OMOP CDM reference (duckdb) of example_data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nâ€¢ omop tables: cdm_source, concept, concept_ancestor, concept_relationship,\nconcept_synonym, condition_occurrence, drug_exposure, drug_strength,\nmeasurement, observation, observation_period, person, procedure_occurrence,\nvisit_occurrence, vocabulary\n\n\nâ€¢ cohort tables: -\n\n\nâ€¢ achilles tables: -\n\n\nâ€¢ other tables: -\n\n\n\n\n\n\n\n\nTipSetting a write prefix\n\n\n\n\n\nWe can also specify a write prefix and this will be used whenever permanent tables are created in the write schema. This can be useful when weâ€™re sharing our write schema with others and want to avoid table name conflicts and easily drop tables created as part of a particular study.\n\ncdm &lt;- cdmFromCon(con = con,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  writePrefix = \"my_study_\",\n                  cdmName = \"example_data\")\n\nNote you only have to specify this writePrefix once at the connection stage, and then the cdm_reference object will store that and use it every time that you create a new table.\n\n\n\nWe can see that we now have an object that contains references to all the OMOP CDM tables. We can reference specific tables using the â€œ$â€ or â€œ[[ â€¦ ]]â€ operators.\n\ncdm$person\n\n# Source:   table&lt;person&gt; [?? x 18]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n       &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n 1         1              8532          1974              7            1\n 2         2              8507          1954              2           25\n 3         3              8532          1998              5            2\n 4         4              8532          1958              5           23\n 5         5              8507          1964              8           14\n 6         6              8507          1972              9           24\n 7         7              8532          1964              6           22\n 8         8              8507          1992              8           10\n 9         9              8532          1967              4           21\n10        10              8532          1957              1           10\n# â„¹ more rows\n# â„¹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\ncdm[[\"observation_period\"]]\n\n# Source:   table&lt;observation_period&gt; [?? x 5]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   observation_period_id person_id observation_period_sâ€¦Â¹ observation_period_eâ€¦Â²\n                   &lt;int&gt;     &lt;int&gt; &lt;date&gt;                 &lt;date&gt;                \n 1                     1         1 1989-04-03             1995-10-15            \n 2                     2         2 1957-04-14             2017-03-02            \n 3                     3         3 2019-10-11             2019-11-14            \n 4                     4         4 2011-04-21             2016-12-07            \n 5                     5         5 2009-10-11             2015-07-12            \n 6                     6         6 1985-10-29             1994-06-28            \n 7                     7         7 1995-09-14             2017-03-31            \n 8                     8         8 2019-01-13             2019-02-17            \n 9                     9         9 1973-12-16             1980-02-12            \n10                    10        10 1977-11-25             2016-02-29            \n# â„¹ more rows\n# â„¹ abbreviated names: Â¹â€‹observation_period_start_date,\n#   Â²â€‹observation_period_end_date\n# â„¹ 1 more variable: period_type_concept_id &lt;int&gt;\n\n\nNote that here we have first created a local version of the cdm with all the tables of interest with omock (cdm_local), then copied it to a DuckDB database, and finally created a reference to it with CDMConnector, so that we can work with the final cdm object as we normally would for one created with our own healthcare data. In that case, we would directly use cdmFromCon() with our own database information. Throughout this chapter, however, we will keep working with the mock dataset.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#cdm-attributes",
    "href": "omop/cdm_reference.html#cdm-attributes",
    "title": "5Â  Creating a CDM reference",
    "section": "5.3 CDM attributes",
    "text": "5.3 CDM attributes\n\n5.3.1 CDM name\nOur cdm reference will be associated with a name. By default, this name will be taken from the cdm_source_name field from the cdm_source table. We will use the function cdmName() from omopgenerics to get it.\n\ncdm &lt;- cdmFromCon(con = con,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\")\ncdm$cdm_source\n\n# Source:   table&lt;cdm_source&gt; [?? x 10]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  cdm_source_name cdm_source_abbreviation cdm_holder source_description\n  &lt;chr&gt;           &lt;chr&gt;                   &lt;chr&gt;      &lt;chr&gt;             \n1 mock            &lt;NA&gt;                    &lt;NA&gt;       &lt;NA&gt;              \n# â„¹ 6 more variables: source_documentation_reference &lt;chr&gt;,\n#   cdm_etl_reference &lt;chr&gt;, source_release_date &lt;date&gt;,\n#   cdm_release_date &lt;date&gt;, cdm_version &lt;chr&gt;, vocabulary_version &lt;chr&gt;\n\ncdmName(cdm)\n\n[1] \"mock\"\n\n\nHowever, we can instead set this name to whatever else we want when creating our cdm reference.\n\ncdm &lt;- cdmFromCon(con = con,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  cdmName = \"my_cdm\")\ncdmName(cdm)\n\n[1] \"my_cdm\"\n\n\nNote that we can also get our cdm name from any of the tables in our cdm reference.\n\ncdmName(cdm$person)\n\n[1] \"my_cdm\"\n\n\n\n\n\n\n\n\nTipBehind the scenes\n\n\n\n\n\nThe class of the cdm reference itself is &lt;cdm_reference&gt;.\n\nclass(cdm)\n\n[1] \"cdm_reference\"\n\n\nEach of the tables has class &lt;cdm_table&gt;. If the table is one of the standard OMOP CDM tables, it will also have class &lt;omop_table&gt;. This latter class is defined so that we can allow different behavior for these core tables (person, condition_occurrence, observation_period, etc.) compared to other tables that are added to the cdm reference during the course of running a study.\n\nclass(cdm$person)\n\n[1] \"omop_table\"            \"cdm_table\"             \"tbl_duckdb_connection\"\n[4] \"tbl_dbi\"               \"tbl_sql\"               \"tbl_lazy\"             \n[7] \"tbl\"                  \n\n\nWe can see that cdmName() is a generic function, which works for both the cdm reference as a whole and individual tables.\n\nlibrary(sloop)\ns3_dispatch(cdmName(cdm))\n\n=&gt; cdmName.cdm_reference\n * cdmName.default\n\ns3_dispatch(cdmName(cdm$person))\n\n   cdmName.omop_table\n=&gt; cdmName.cdm_table\n   cdmName.tbl_duckdb_connection\n   cdmName.tbl_dbi\n   cdmName.tbl_sql\n   cdmName.tbl_lazy\n   cdmName.tbl\n * cdmName.default\n\n\n\n\n\n\n\n5.3.2 CDM version\nWe can also easily check the OMOP CDM version that is being used with the function cdmVersion() from omopgenerics like so:\n\ncdmVersion(cdm)\n\n[1] \"5.3\"\n\n\n\n\n\n\n\n\nNotecdmVersion\n\n\n\n\n\nNote, the cdmVersion() function also works for &lt;cdm_table&gt; objects:\n\ncdmVersion(cdm$person)\n\n[1] \"5.3\"\n\n\n\n\n\n\n\n\n\n\n\nNoteMethods functions\n\n\n\n\n\nAlthough as stated, the cdmName() and cdmVersion() functions are defined by the omopgenerics packages, these functions are re-exported in other packages and you wonâ€™t need to load omopgenerics explicitly.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#including-cohort-tables-in-the-cdm-reference",
    "href": "omop/cdm_reference.html#including-cohort-tables-in-the-cdm-reference",
    "title": "5Â  Creating a CDM reference",
    "section": "5.4 Including cohort tables in the cdm reference",
    "text": "5.4 Including cohort tables in the cdm reference\nA cohort is a fundamental piece in epidemiological studies. Later, weâ€™ll see how to create cohorts in more detail in Chapter 8. For the moment, letâ€™s just outline how we can include the reference to an existing cohort in our cdm reference. For this, weâ€™ll use omock to add a cohort to our local cdm and upload that to a DuckDB database again.\n\ncdm_local &lt;- cdm_local |&gt; \n  mockCohort(name = \"my_study_cohort\")\ncon &lt;- dbConnect(drv = duckdb())\nsrc &lt;- dbSource(con = con, writeSchema = \"main\")\ncdm &lt;- insertCdmTo(cdm = cdm_local, to = src)\n\nNow we can specify we want to include this existing cohort table to our cdm object when creating our cdm reference.\n\ncdm &lt;- cdmFromCon(con = con, \n                  cdmSchema = \"main\", \n                  writeSchema = \"main\",\n                  cohortTables = \"my_study_cohort\",\n                  cdmName = \"example_data\")\ncdm\n\n\ncdm$my_study_cohort |&gt; \n  glimpse()\n\nRows: ??\nColumns: 4\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ subject_id           &lt;int&gt; 1, 1, 1, 3, 3, 3, 4, 5, 5, 5, 6, 8, 9, 10, 12, 12â€¦\n$ cohort_start_date    &lt;date&gt; 1989-09-20, 1991-08-04, 1995-08-27, 2019-11-04, â€¦\n$ cohort_end_date      &lt;date&gt; 1989-12-18, 1993-08-15, 1995-10-07, 2019-11-08, â€¦\n\n\n\n\n\n\n\n\nNoteTables included in the cdm reference\n\n\n\n\n\nNote that by default the cohort table wonâ€™t be included in the cdm_reference object.\n\ncdm &lt;- cdmFromCon(con = con, \n                  cdmSchema = \"main\", \n                  writeSchema = \"main\",\n                  cdmName = \"example_data\")\ncdm\n\n\n\n\nâ”€â”€ # OMOP CDM reference (duckdb) of example_data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nâ€¢ omop tables: cdm_source, concept, concept_ancestor, concept_relationship,\nconcept_synonym, condition_occurrence, drug_exposure, drug_strength,\nmeasurement, observation, observation_period, person, procedure_occurrence,\nvisit_occurrence, vocabulary\n\n\nâ€¢ cohort tables: -\n\n\nâ€¢ achilles tables: -\n\n\nâ€¢ other tables: -\n\n\nEven if the cohort exists in the database:\n\ndbListTables(conn = con)\n\n [1] \"cdm_source\"                \"concept\"                  \n [3] \"concept_ancestor\"          \"concept_relationship\"     \n [5] \"concept_synonym\"           \"condition_occurrence\"     \n [7] \"drug_exposure\"             \"drug_strength\"            \n [9] \"measurement\"               \"my_study_cohort\"          \n[11] \"my_study_cohort_attrition\" \"my_study_cohort_codelist\" \n[13] \"my_study_cohort_set\"       \"observation\"              \n[15] \"observation_period\"        \"person\"                   \n[17] \"procedure_occurrence\"      \"visit_occurrence\"         \n[19] \"vocabulary\"               \n\n\nBy default, only the default omop tables omopTables() will be included (if they exist) into the cdm_reference object.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#including-achilles-tables-in-the-cdm-reference",
    "href": "omop/cdm_reference.html#including-achilles-tables-in-the-cdm-reference",
    "title": "5Â  Creating a CDM reference",
    "section": "5.5 Including achilles tables in the cdm reference",
    "text": "5.5 Including achilles tables in the cdm reference\nIf we have the results tables from the Achilles package in our database, we can also include these in our cdm reference.\nJust to show how this can be done, letâ€™s upload some empty results tables in the Achilles format.\n\ndbWriteTable(conn = con, \n             name = \"achilles_analysis\",\n             value = tibble(\n               analysis_id = NA_integer_,\n               analysis_name = NA_character_,\n               stratum_1_name = NA_character_,\n               stratum_2_name = NA_character_,\n               stratum_3_name = NA_character_,\n               stratum_4_name = NA_character_,\n               stratum_5_name = NA_character_,\n               is_default = NA_character_,\n               category = NA_character_))\ndbWriteTable(conn = con, \n             name = \"achilles_results\",\n             value = tibble(\n               analysis_id = NA_integer_,\n               stratum_1 = NA_character_,\n               stratum_2 = NA_character_,\n               stratum_3 = NA_character_,\n               stratum_4 = NA_character_,\n               stratum_5 = NA_character_,\n               count_value = NA_character_))\ndbWriteTable(conn = con, \n             name = \"achilles_results_dist\",\n             value = tibble(\n               analysis_id = NA_integer_,\n               stratum_1 = NA_character_,\n               stratum_2 = NA_character_,\n               stratum_3 = NA_character_,\n               stratum_4 = NA_character_,\n               stratum_5 = NA_character_,\n               count_value = NA_character_,\n               min_value = NA_character_,\n               max_value = NA_character_,\n               avg_value = NA_character_,\n               stdev_value = NA_character_,\n               median_value = NA_character_,\n               p10_value = NA_character_,\n               p25_value = NA_character_,\n               p75_value = NA_character_,\n               p90_value = NA_character_))\n\nWe can now include these achilles tables in our cdm reference as in the previous case.\n\ncdm &lt;- cdmFromCon(con = con, \n                  cdmSchema = \"main\", \n                  writeSchema = \"main\",\n                  cohortTables = \"my_study_cohort\",\n                  achillesSchema = \"main\",\n                  cdmName = \"example_data\")\ncdm\n\nNote we specified the achillesSchema that in this case is the same as the writeSchema and cdmSchema, but each one of them can be different and point to a separate schema in our database.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#adding-other-tables-to-the-cdm-reference",
    "href": "omop/cdm_reference.html#adding-other-tables-to-the-cdm-reference",
    "title": "5Â  Creating a CDM reference",
    "section": "5.6 Adding other tables to the cdm reference",
    "text": "5.6 Adding other tables to the cdm reference\nLetâ€™s say we have some additional local data that we want to add to our cdm reference. We can add this both to the same source (in this case a database) and to our cdm reference using insertTable() from omopgenerics (insertTable() is also re-exported in CDMConnector). We will show this with the dataset cars built-in to R.\n\ncars |&gt; \n  glimpse()\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13â€¦\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34â€¦\n\n\n\ncdm &lt;- insertTable(cdm = cdm, name = \"cars\", table = cars)\n\nWe can see that now this extra table has been uploaded to the database behind our cdm reference and also added to our reference.\n\ncdm\n\n\n\n\nâ”€â”€ # OMOP CDM reference (duckdb) of example_data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nâ€¢ omop tables: cdm_source, concept, concept_ancestor, concept_relationship,\nconcept_synonym, condition_occurrence, drug_exposure, drug_strength,\nmeasurement, observation, observation_period, person, procedure_occurrence,\nvisit_occurrence, vocabulary\n\n\nâ€¢ cohort tables: my_study_cohort\n\n\nâ€¢ achilles tables: achilles_analysis, achilles_results, achilles_results_dist\n\n\nâ€¢ other tables: cars\n\n\n\ncdm$cars\n\n# Source:   table&lt;cars&gt; [?? x 2]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n   speed  dist\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     4     2\n 2     4    10\n 3     7     4\n 4     7    22\n 5     8    16\n 6     9    10\n 7    10    18\n 8    10    26\n 9    10    34\n10    11    17\n# â„¹ more rows\n\n\nIf we already had the table in the database we could have instead just assigned it to our existing cdm reference. To see this letâ€™s upload the penguins table to our DuckDB database.\n\ndbWriteTable(conn = con, name = \"penguins\", value = penguins)\n\nOnce we have this table in the database, we can just read it using the readSourceTable() function.\n\ncdm &lt;- readSourceTable(cdm = cdm, name = \"penguins\")\n\ncdm\n\n\n\n\nâ”€â”€ # OMOP CDM reference (duckdb) of example_data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nâ€¢ omop tables: cdm_source, concept, concept_ancestor, concept_relationship,\nconcept_synonym, condition_occurrence, drug_exposure, drug_strength,\nmeasurement, observation, observation_period, person, procedure_occurrence,\nvisit_occurrence, vocabulary\n\n\nâ€¢ cohort tables: my_study_cohort\n\n\nâ€¢ achilles tables: achilles_analysis, achilles_results, achilles_results_dist\n\n\nâ€¢ other tables: cars, penguins\n\n\nNote that omopgenerics provides these functions readSourceTable(), listSourceTables(), and dropSourceTable() for the easier management of the tables in the writeSchema.\n\nlistSourceTables(cdm = cdm)\n\n [1] \"achilles_analysis\"         \"achilles_results\"         \n [3] \"achilles_results_dist\"     \"cars\"                     \n [5] \"cdm_source\"                \"concept\"                  \n [7] \"concept_ancestor\"          \"concept_relationship\"     \n [9] \"concept_synonym\"           \"condition_occurrence\"     \n[11] \"drug_exposure\"             \"drug_strength\"            \n[13] \"measurement\"               \"my_study_cohort\"          \n[15] \"my_study_cohort_attrition\" \"my_study_cohort_codelist\" \n[17] \"my_study_cohort_set\"       \"observation\"              \n[19] \"observation_period\"        \"penguins\"                 \n[21] \"person\"                    \"procedure_occurrence\"     \n[23] \"visit_occurrence\"          \"vocabulary\"               \n\ndropSourceTable(cdm = cdm, name = \"penguins\")\nlistSourceTables(cdm = cdm)\n\n [1] \"achilles_analysis\"         \"achilles_results\"         \n [3] \"achilles_results_dist\"     \"cars\"                     \n [5] \"cdm_source\"                \"concept\"                  \n [7] \"concept_ancestor\"          \"concept_relationship\"     \n [9] \"concept_synonym\"           \"condition_occurrence\"     \n[11] \"drug_exposure\"             \"drug_strength\"            \n[13] \"measurement\"               \"my_study_cohort\"          \n[15] \"my_study_cohort_attrition\" \"my_study_cohort_codelist\" \n[17] \"my_study_cohort_set\"       \"observation\"              \n[19] \"observation_period\"        \"person\"                   \n[21] \"procedure_occurrence\"      \"visit_occurrence\"         \n[23] \"vocabulary\"               \n\n\n\n\n\n\n\n\nNoteDifference between insertTable and dbWriteTable\n\n\n\n\n\n\ndbWriteTable() is a function from the DBI package that writes a local R data frame to a database. You need to manually specify the schema and table name and it does not update the cdm reference object.\ninsertTable() is a function from the omopgenerics package designed for use with cdm reference objects. It writes a local table to the database and adds it to the list of tables in the cdm reference. Internally, it uses dbWriteTable() but also handles the schema and table name automatically using the writeSchema and writePrefix from the cdm reference.\n\nIn general, for studies using OMOP CDM data, you should use insertTable() rather than dbWriteTable(). It ensures the table is both written to the correct location in the database and accessible through the cdm reference object. Only use dbWriteTable() if you are confident working directly with the database and understand its structure.\nNote insertTable() would also work for a local cdm reference or any other defined cdm reference source, whereas the dbWriteTable() is a database specific function.\nTODO reference to omopgenerics supported sources.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#mutability-of-the-cdm-reference",
    "href": "omop/cdm_reference.html#mutability-of-the-cdm-reference",
    "title": "5Â  Creating a CDM reference",
    "section": "5.7 Mutability of the cdm reference",
    "text": "5.7 Mutability of the cdm reference\nAn important characteristic of our cdm reference is that we can alter the tables in R, but the OMOP CDM data will not be affected. We will therefore only be transforming the data in our cdm object but the original datasets behind it will remain intact.\nFor example, letâ€™s say we want to perform a study with only people born in 1970. For this we could filter our person table to only people born in this year.\n\ncdm$person &lt;- cdm$person |&gt; \n  filter(year_of_birth == 1970)\n\ncdm$person\n\n# Source:   SQL [?? x 18]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1        33              8532          1970              5           21\n# â„¹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nFrom now on, when we work with our cdm reference this restriction will continue to have been applied.\n\ncdm$person |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n      n\n  &lt;dbl&gt;\n1     1\n\n\nThe original OMOP CDM data itself however will remain unaffected. We can see that, indeed, if we create our reference again the underlying data is unchanged.\n\ncdm &lt;- cdmFromCon(con = con,\n                  cdmSchema = \"main\", \n                  writeSchema = \"main\", \n                  cdmName = \"example_data\")\ncdm$person |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n      n\n  &lt;dbl&gt;\n1   100\n\n\nThe mutability of our cdm reference is a useful feature for studies as it means we can easily tweak our OMOP CDM data if needed. Meanwhile, leaving the underlying data unchanged is essential so that other study code can run against the data, unaffected by any of our changes.\nOne thing we canâ€™t do, though, is alter the structure of OMOP CDM tables. For example, the following code would cause an error as the person table must always have the column person_id.\n\ncdm$person &lt;- cdm$person |&gt; \n  rename(\"new_id\" = \"person_id\")\n\nError in `newOmopTable()`:\n! person_id is not present in table person\n\n\nIn such a case we would have to call the table something else first, and then run the previous code:\n\ncdm$person_new &lt;- cdm$person |&gt; \n  rename(\"new_id\" = \"person_id\") |&gt; \n  compute(name = \"person_new\")\n\nNow we would be allowed to have this new table as an additional table in our cdm reference, knowing it was not in the format of one of the core OMOP CDM tables.\n\ncdm\n\n\n\n\nâ”€â”€ # OMOP CDM reference (duckdb) of example_data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nâ€¢ omop tables: cdm_source, concept, concept_ancestor, concept_relationship,\nconcept_synonym, condition_occurrence, drug_exposure, drug_strength,\nmeasurement, observation, observation_period, person, procedure_occurrence,\nvisit_occurrence, vocabulary\n\n\nâ€¢ cohort tables: -\n\n\nâ€¢ achilles tables: -\n\n\nâ€¢ other tables: person_new\n\n\nThe package omopgenerics provides a comprehensive list of the required features of a valid cdm reference. You can read more about it here.\n\n\n\n\n\n\nNoteName consistency\n\n\n\n\n\nNote also that there must be a name consistency between the name of the table and the assignment in the cdm_reference object.\n\ncdm$new_table &lt;- cdm$person |&gt; \n  compute(name = \"not_new_table\")\n\nError in `[[&lt;-`:\nâœ– You can't assign a table named not_new_table to new_table.\nâ„¹ You can change the name using compute:\ncdm[['new_table']] &lt;- yourObject |&gt;\n  dplyr::compute(name = 'new_table')\nâ„¹ You can also change the name using the `name` argument in your function:\n  `name = 'new_table'`.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#working-with-temporary-and-permanent-tables",
    "href": "omop/cdm_reference.html#working-with-temporary-and-permanent-tables",
    "title": "5Â  Creating a CDM reference",
    "section": "5.8 Working with temporary and permanent tables",
    "text": "5.8 Working with temporary and permanent tables\nWhen we create new tables and our cdm reference is in a database we have a choice between using temporary or permanent tables. In most cases we can work with these interchangeably. Below we create one temporary table and one permanent table. We can see that both of these tables have been added to our cdm reference and that we can use them in the same way. Note that any new computed table will by default be temporary unless otherwise specified.\n\ncdm$person_new_temp &lt;- cdm$person |&gt; \n  head(5) |&gt; \n  compute(temporary = TRUE)\n\n\ncdm$person_new_permanent &lt;- cdm$person |&gt; \n  head(5) |&gt; \n  compute(name = \"person_new_permanent\", temporary = FALSE)\n\n\ncdm\n\ncdm$person_new_temp\n\n# Source:   table&lt;og_001_1761244255&gt; [?? x 18]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1         1              8532          1974              7            1\n2         2              8507          1954              2           25\n3         3              8532          1998              5            2\n4         4              8532          1958              5           23\n5         5              8507          1964              8           14\n# â„¹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\ncdm$person_new_permanent\n\n# Source:   table&lt;person_new_permanent&gt; [?? x 18]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1/:memory:]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1         1              8532          1974              7            1\n2         2              8507          1954              2           25\n3         3              8532          1998              5            2\n4         4              8532          1958              5           23\n5         5              8507          1964              8           14\n# â„¹ 13 more variables: race_concept_id &lt;int&gt;, ethnicity_concept_id &lt;int&gt;,\n#   birth_datetime &lt;dttm&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nOne benefit of working with temporary tables is that they will be automatically dropped at the end of the session, whereas the permanent tables will be left in the database until explicitly dropped. This helps maintain the original database structure tidy and free of irrelevant data.\nHowever, one disadvantage of using temporary tables is that we will generally accumulate more and more of them as we go (in a single R session), whereas we can overwrite permanent tables continuously. For example, if our study code contains a loop that requires a compute, we would either overwrite an intermediate permanent table 100 times or create 100 different temporary tables in the process. In the latter case we should be wary of consuming a lot of drive memory, which could lead to performance issues or even crashes.\n\n\n\n\n\n\nNotename argument in compute()\n\n\n\n\n\nNote that in the previous examples we explicitly specified the name of the new table and whether it must be temporary or permanent (temporary = FALSE), but we do not need to populate the temporary field explicitly as if name is left as NULL (default behavior), then the table will be temporary (temporary = TRUE), and if the name argument is populated with a character (e.g., name = â€œmy_custom_tableâ€), then the created table will be permanent:\n\ncdm$person_new_temp &lt;- cdm$person |&gt; \n  compute()\n\ncdm$person_new_permanent &lt;- cdm$person |&gt; \n  compute(name = \"person_new_permanent\")",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#disconnecting",
    "href": "omop/cdm_reference.html#disconnecting",
    "title": "5Â  Creating a CDM reference",
    "section": "5.9 Disconnecting",
    "text": "5.9 Disconnecting\nOnce we have finished our analysis we can close our connection to the database behind our cdm reference.\n\ncdmDisconnect(cdm)",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/cdm_reference.html#further-reading",
    "href": "omop/cdm_reference.html#further-reading",
    "title": "5Â  Creating a CDM reference",
    "section": "5.10 Further reading",
    "text": "5.10 Further reading\n\nCatalÃ  M, Burn E (2025). omopgenerics: Methods and Classes for thecOMOP Common Data Model. R package version 1.3.1, https://darwin-eu.github.io/omopgenerics/.\nBlack A, Gorbachev A, Burn E, CatalÃ  M, Nika I (2025). CDMConnector: Connect to an OMOP Common Data Model. R package version 2.2.0, https://darwin-eu.github.io/CDMConnector/.\nOmopOnPostgres (in progress)\nOmopOnSpark (in progress)\nOmopOnDuckDB (in progress)",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Creating a CDM reference</span>"
    ]
  },
  {
    "objectID": "omop/exploring_the_cdm.html",
    "href": "omop/exploring_the_cdm.html",
    "title": "6Â  Exploring the OMOP CDM",
    "section": "",
    "text": "6.1 Counting people\nFor this chapter, weâ€™ll use a synthetic COVID-19 dataset (\"synthea-covid19-10k\") that can be downloaded with the omock package. A characterisation of this dataset to better understant its content can be found in the following Shiny App https://dpa-pde-oxford.shinyapps.io/OmopSketchCharacterisation/.\nYou can download the dataset using the function downloadMockDataset():\nOnce the dataset is downloaded you can create the cdm reference:\nThe OMOP CDM is person-centric, with the person table containing records to uniquely identify each person in the database. As each row refers to a unique person, we can quickly get a count of the number of individuals in the database like so\ncdm$person |&gt; \n  count()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpcNPbp9/file35692d8c693d.duckdb]\n      n\n  &lt;dbl&gt;\n1 10754\nThe person table also contains some demographic information, including a gender concept for each person. We can easily get a count grouped by this variable. As the concept id is just a number it is more useful to get the concept name, this can be done with a join to the concept table.\ncdm$person |&gt; \n  group_by(gender_concept_id) |&gt; \n  count() |&gt; \n  left_join(cdm$concept, by = c(\"gender_concept_id\" = \"concept_id\")) |&gt; \n  select(\"gender_concept_id\", \"concept_name\", \"n\") |&gt; \n  collect()\n\n# A tibble: 2 Ã— 3\n# Groups:   gender_concept_id [2]\n  gender_concept_id concept_name     n\n              &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1              8532 FEMALE        5165\n2              8507 MALE          5589",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "omop/exploring_the_cdm.html#counting-people",
    "href": "omop/exploring_the_cdm.html#counting-people",
    "title": "6Â  Exploring the OMOP CDM",
    "section": "",
    "text": "TipVocabulary tables\n\n\n\n\n\nAbove weâ€™ve got counts by specific concept IDs recorded in the condition occurrence table. What these IDs represent is described in the concept table. Here we have the name associated with the concept, along with other information such as its domain and vocabulary id.\n\ncdm$concept |&gt; \n  glimpse()\n\nRows: ??\nColumns: 10\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpcNPbp9/file35692d8c693d.duckdb]\n$ concept_id       &lt;int&gt; 45756805, 45756804, 45756803, 45756802, 45756801, 457â€¦\n$ concept_name     &lt;chr&gt; \"Pediatric Cardiology\", \"Pediatric Anesthesiology\", \"â€¦\n$ domain_id        &lt;chr&gt; \"Provider\", \"Provider\", \"Provider\", \"Provider\", \"Provâ€¦\n$ vocabulary_id    &lt;chr&gt; \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMS\", \"ABMSâ€¦\n$ concept_class_id &lt;chr&gt; \"Physician Specialty\", \"Physician Specialty\", \"Physicâ€¦\n$ standard_concept &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\"â€¦\n$ concept_code     &lt;chr&gt; \"OMOP4821938\", \"OMOP4821939\", \"OMOP4821940\", \"OMOP482â€¦\n$ valid_start_date &lt;date&gt; 1970-01-01, 1970-01-01, 1970-01-01, 1970-01-01, 1970â€¦\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099â€¦\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n\n\nOther vocabulary tables capture other information about concepts, such as the direct relationships between concepts (the concept relationship table) and hierarchical relationships between (the concept ancestor table).\n\ncdm$concept_relationship |&gt; \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpcNPbp9/file35692d8c693d.duckdb]\n$ concept_id_1     &lt;int&gt; 35804314, 35804314, 35804314, 35804327, 35804327, 358â€¦\n$ concept_id_2     &lt;int&gt; 912065, 42542145, 42542145, 35803584, 42542145, 42542â€¦\n$ relationship_id  &lt;chr&gt; \"Has modality\", \"Has accepted use\", \"Is current in\", â€¦\n$ valid_start_date &lt;date&gt; 2021-01-26, 2019-08-29, 2019-08-29, 2019-05-27, 2019â€¦\n$ valid_end_date   &lt;date&gt; 2099-12-31, 2099-12-31, 2099-12-31, 2099-12-31, 2099â€¦\n$ invalid_reason   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n\ncdm$concept_ancestor |&gt; \n  glimpse()\n\nRows: ??\nColumns: 4\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpcNPbp9/file35692d8c693d.duckdb]\n$ ancestor_concept_id      &lt;int&gt; 375415, 727760, 735979, 438112, 529411, 14196â€¦\n$ descendant_concept_id    &lt;int&gt; 4335743, 2056453, 41070383, 36566114, 4326940â€¦\n$ min_levels_of_separation &lt;int&gt; 4, 1, 3, 2, 3, 3, 4, 3, 2, 5, 1, 3, 4, 2, 2, â€¦\n$ max_levels_of_separation &lt;int&gt; 4, 1, 5, 3, 3, 6, 12, 3, 2, 10, 1, 3, 4, 2, 2â€¦\n\n\nMore information on the vocabulary tables (as well as other tables in the OMOP CDM version 5.3) can be found at https://ohdsi.github.io/CommonDataModel/cdm53.html#Vocabulary_Tables.\n\n\n\n\n\n\n\n\n\nTipaddConceptName()\n\n\n\n\n\nThe PatientProfiles package has a utility function that helps you to add concept names to a table. By default, any column that ends in concept_id will be used to join to the concept table and add a concept_name column:\n\ncdm$person |&gt;\n  addConceptName() |&gt;\n  glimpse()\n\nRows: ??\nColumns: 24\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpcNPbp9/file35692d8c693d.duckdb]\n$ person_id                        &lt;int&gt; 10752, 10754, 10746, 10753, 10745, 10â€¦\n$ gender_concept_id                &lt;int&gt; 8532, 8507, 8532, 8507, 8532, 8507, 8â€¦\n$ year_of_birth                    &lt;int&gt; 2017, 1942, 2017, 1953, 2010, 1930, 1â€¦\n$ month_of_birth                   &lt;int&gt; 2, 6, 10, 5, 9, 1, 3, 7, 5, 12, 8, 10â€¦\n$ day_of_birth                     &lt;int&gt; 1, 1, 18, 9, 27, 10, 16, 28, 21, 21, â€¦\n$ birth_datetime                   &lt;dttm&gt; 2017-02-01, 1942-06-01, 2017-10-18, â€¦\n$ race_concept_id                  &lt;int&gt; 8527, 8527, 8515, 8527, 8527, 8515, 8â€¦\n$ ethnicity_concept_id             &lt;int&gt; 38003564, 38003564, 38003564, 3800356â€¦\n$ location_id                      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ provider_id                      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ care_site_id                     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ person_source_value              &lt;chr&gt; \"ffecf9fe-26c1-605c-0ce7-6133f75eb6dcâ€¦\n$ gender_source_value              &lt;chr&gt; \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"Mâ€¦\n$ gender_source_concept_id         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ race_source_value                &lt;chr&gt; \"white\", \"white\", \"asian\", \"white\", \"â€¦\n$ race_source_concept_id           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ ethnicity_source_value           &lt;chr&gt; \"nonhispanic\", \"nonhispanic\", \"nonhisâ€¦\n$ ethnicity_source_concept_id      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ gender_concept_id_name           &lt;chr&gt; \"FEMALE\", \"MALE\", \"FEMALE\", \"MALE\", \"â€¦\n$ race_concept_id_name             &lt;chr&gt; \"White\", \"White\", \"Asian\", \"White\", \"â€¦\n$ ethnicity_concept_id_name        &lt;chr&gt; \"Not Hispanic or Latino\", \"Not Hispanâ€¦\n$ gender_source_concept_id_name    &lt;chr&gt; \"No matching concept\", \"No matching câ€¦\n$ race_source_concept_id_name      &lt;chr&gt; \"No matching concept\", \"No matching câ€¦\n$ ethnicity_source_concept_id_name &lt;chr&gt; \"No matching concept\", \"No matching câ€¦\n\n\nNote you can edit the arguments to only use one desired column or edit the concept name column.\n\ncdm$person |&gt;\n  addConceptName(column = \"gender_concept_id\", nameStyle = \"sex\") |&gt;\n  glimpse()\n\nRows: ??\nColumns: 19\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpcNPbp9/file35692d8c693d.duckdb]\n$ person_id                   &lt;int&gt; 10752, 10754, 10746, 10753, 10745, 10751, â€¦\n$ gender_concept_id           &lt;int&gt; 8532, 8507, 8532, 8507, 8532, 8507, 8532, â€¦\n$ year_of_birth               &lt;int&gt; 2017, 1942, 2017, 1953, 2010, 1930, 1970, â€¦\n$ month_of_birth              &lt;int&gt; 2, 6, 10, 5, 9, 1, 3, 7, 5, 12, 8, 10, 5, â€¦\n$ day_of_birth                &lt;int&gt; 1, 1, 18, 9, 27, 10, 16, 28, 21, 21, 20, 1â€¦\n$ birth_datetime              &lt;dttm&gt; 2017-02-01, 1942-06-01, 2017-10-18, 1953-â€¦\n$ race_concept_id             &lt;int&gt; 8527, 8527, 8515, 8527, 8527, 8515, 8527, â€¦\n$ ethnicity_concept_id        &lt;int&gt; 38003564, 38003564, 38003564, 38003564, 38â€¦\n$ location_id                 &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NAâ€¦\n$ provider_id                 &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NAâ€¦\n$ care_site_id                &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NAâ€¦\n$ person_source_value         &lt;chr&gt; \"ffecf9fe-26c1-605c-0ce7-6133f75eb6dc\", \"fâ€¦\n$ gender_source_value         &lt;chr&gt; \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"Fâ€¦\n$ gender_source_concept_id    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ race_source_value           &lt;chr&gt; \"white\", \"white\", \"asian\", \"white\", \"whiteâ€¦\n$ race_source_concept_id      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ ethnicity_source_value      &lt;chr&gt; \"nonhispanic\", \"nonhispanic\", \"nonhispanicâ€¦\n$ ethnicity_source_concept_id &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ sex                         &lt;chr&gt; \"FEMALE\", \"MALE\", \"FEMALE\", \"MALE\", \"FEMALâ€¦",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "omop/exploring_the_cdm.html#summarising-observation-periods",
    "href": "omop/exploring_the_cdm.html#summarising-observation-periods",
    "title": "6Â  Exploring the OMOP CDM",
    "section": "6.2 Summarising observation periods",
    "text": "6.2 Summarising observation periods\nThe observation period table contains records indicating spans of time over which clinical events can be reliably observed for the people in the person table (see formal definition). Someone can potentially have multiple observation periods. So, say we wanted a count of people grouped by the year during which their first observation period started.\nTo do this first we would need to get the first observation period per person:\n\nfirst_observation_period &lt;- cdm$observation_period |&gt;\n  group_by(person_id) |&gt; \n  arrange(observation_period_start_date) |&gt;\n  filter(row_number() == 1) |&gt; \n  compute()\n\nNow we can add this to the person table to make sure that all individuals defined in the observation period table are also defined the person table. Later we can extract the observation_period_start_year and count the number of records associated in each year:\n\nfirst_records_per_year &lt;- cdm$person |&gt; \n  left_join(first_observation_period, by = \"person_id\") |&gt; \n  mutate(observation_period_start_year = get_year(observation_period_start_date)) |&gt; \n  group_by(observation_period_start_year) |&gt; \n  count() |&gt; \n  collect()\n\nFinally we can plot the counts with ggplot2.\n\nggplot(first_records_per_year) +\n  geom_col(mapping = aes(x = observation_period_start_year, y = n)) +\n  theme_bw()",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "omop/exploring_the_cdm.html#summarising-clinical-records",
    "href": "omop/exploring_the_cdm.html#summarising-clinical-records",
    "title": "6Â  Exploring the OMOP CDM",
    "section": "6.3 Summarising clinical records",
    "text": "6.3 Summarising clinical records\nWhatâ€™s the number of condition occurrence records per person in the database? We can find this out like so\n\nnumber_condition_occurrence_records &lt;- cdm$person |&gt; \n  left_join(\n    cdm$condition_occurrence |&gt; \n      group_by(person_id) |&gt; \n      count(name = \"condition_occurrence_records\"),\n    by=\"person_id\"\n  ) |&gt; \n  mutate(condition_occurrence_records = coalesce(condition_occurrence_records, 0)) |&gt;\n  group_by(condition_occurrence_records) |&gt;\n  count() |&gt; \n  collect()\n\nggplot(number_condition_occurrence_records) +\n  geom_col(mapping = aes(x = condition_occurrence_records, y = n)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nHow about we were interested in getting record counts for some specific concepts related to Covid-19 symptoms?\n\ncodes &lt;- c(437663, 437390, 31967, 4289517, 4223659, 312437, 434490, 254761, 77074)\nsymptoms_records &lt;- cdm$condition_occurrence |&gt; \n  filter(condition_concept_id %in% codes) |&gt; \n  group_by(condition_concept_id) |&gt; \n  count() |&gt; \n  addConceptName(column = \"condition_concept_id\", nameStyle = \"concept_name\") |&gt;\n  collect()\n\nggplot(symptoms_records) +\n  geom_col(mapping = aes(x = concept_name, y = n)) +\n  theme_bw()+\n  xlab(\"\")\n\n\n\n\n\n\n\n\nWe can also use summarise for various other calculations\n\ncdm$person |&gt; \n  summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n            q05_year_of_birth = quantile(year_of_birth, 0.05, na.rm=TRUE),\n            mean_year_of_birth = round(mean(year_of_birth, na.rm=TRUE),0),\n            median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n            q95_year_of_birth = quantile(year_of_birth, 0.95, na.rm=TRUE),\n            max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt;  \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpcNPbp9/file35692d8c693d.duckdb]\n$ min_year_of_birth    &lt;int&gt; 1923\n$ q05_year_of_birth    &lt;dbl&gt; 1927\n$ mean_year_of_birth   &lt;dbl&gt; 1971\n$ median_year_of_birth &lt;dbl&gt; 1970\n$ q95_year_of_birth    &lt;dbl&gt; 2018\n$ max_year_of_birth    &lt;int&gt; 2023\n\n\nAs weâ€™ve seen before, we can also quickly get results for various groupings or restrictions\n\ngrouped_summary &lt;- cdm$person |&gt; \n   group_by(gender_concept_id) |&gt; \n   summarise(min_year_of_birth = min(year_of_birth, na.rm=TRUE),\n             q25_year_of_birth = quantile(year_of_birth, 0.25, na.rm=TRUE),\n             median_year_of_birth = median(year_of_birth, na.rm=TRUE),\n             q75_year_of_birth = quantile(year_of_birth, 0.75, na.rm=TRUE),\n             max_year_of_birth = max(year_of_birth, na.rm=TRUE)) |&gt; \n  left_join(cdm$concept, by = c(\"gender_concept_id\" = \"concept_id\")) |&gt; \n  collect() \n\ngrouped_summary |&gt; \n  ggplot(mapping = aes(x = concept_name, group = concept_name, fill = concept_name)) +\n  geom_boxplot(mapping = aes(\n    lower = q25_year_of_birth, \n    upper = q75_year_of_birth, \n    middle = median_year_of_birth, \n    ymin = min_year_of_birth, \n    ymax = max_year_of_birth),\n    stat = \"identity\", width = 0.5) + \n  theme_bw()+ \n  theme(legend.position = \"none\") +\n  xlab(\"\")",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "omop/exploring_the_cdm.html#the-omopsketch-package",
    "href": "omop/exploring_the_cdm.html#the-omopsketch-package",
    "title": "6Â  Exploring the OMOP CDM",
    "section": "6.4 The OmopSketch package",
    "text": "6.4 The OmopSketch package\nThe OmopSketch R package aims to characterise and visualise an OMOP CDM instance to asses if it meets the necessary criteria to answer a specific clinical question and conduct a certain study. It can be very useful to conduct some of the analyses that we were conducting in the previous sections with bespoke code. For example, you can quickly summarise and visualise the observation period table as:\n\nresult &lt;- summariseObservationPeriod(cdm$observation_period)\ntableObservationPeriod(result = result)\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation period ordinal\nVariable name\nEstimate name\n\nCDM name\n\n\n\nsynthea-covid19-10k\n\n\n\n\nall\nNumber records\nN\n10,754\n\n\n\nNumber subjects\nN\n10,754\n\n\n\nRecords per person\nmean (sd)\n1.00 (0.00)\n\n\n\n\nmedian [Q25 - Q75]\n1 [1 - 1]\n\n\n\nDuration in days\nmean (sd)\n3,958.53 (3,430.29)\n\n\n\n\nmedian [Q25 - Q75]\n3,326 [2,927 - 3,340]\n\n\n1st\nNumber subjects\nN\n10,754\n\n\n\nDuration in days\nmean (sd)\n3,958.53 (3,430.29)\n\n\n\n\nmedian [Q25 - Q75]\n3,326 [2,927 - 3,340]\n\n\n\n\n\n\n\nWith this table we can see that only one pbservation period is defined by person and that individuals have a median follow up of ~ 9 years.\nWe can also visualise how many individuals are in observation per year:\n\nresult &lt;- summariseInObservation(\n  cdm$observation_period,\n  interval = \"years\",\n  output = \"person\", \n  dateRange = c(\"2000-01-01\", \"2023-12-31\")\n)\n\nâ†’ The observation period in the cdm ends in 2023-06-14\n\nplotInObservation(result = result)\n\n`result_id` is not present in result.\n\n\n`result_id` is not present in result.\n\n\n\n\n\n\n\n\n\nOr even the median age of the individuals in observation stratified by sex:\n\nresult &lt;- summariseInObservation(\n  cdm$observation_period,\n  interval = \"years\",\n  output = \"age\", \n  sex = TRUE, \n  dateRange = c(\"2000-01-01\", \"2023-12-31\")\n)\n\nâ†’ The observation period in the cdm ends in 2023-06-14\n\n\nâ„¹ The following estimates will be computed:\nâ€¢ age: median\nâ†’ Start summary of data, at 2025-10-23 18:32:36.322842\n\nâœ” Summary finished, at 2025-10-23 18:32:36.484312\nâ„¹ The following estimates will be computed:\nâ€¢ age: median\nâ†’ Start summary of data, at 2025-10-23 18:32:38.479882\n\nâœ” Summary finished, at 2025-10-23 18:32:38.690504\n\nplotInObservation(result = result, colour = \"sex\")\n\n`result_id` is not present in result.\n`result_id` is not present in result.\n\n\n\n\n\n\n\n\n\nThe package also provides functions to characterise the clinical tables to show percentage of records in observation, domains recorded or the source vocabularies:\n\nresult &lt;- summariseClinicalRecords(cdm = cdm, omopTableName = \"drug_exposure\")\n\nâ„¹ Adding variables of interest to drug_exposure.\nâ„¹ Summarising records per person in drug_exposure.\nâ„¹ Summarising drug_exposure: `in_observation`, `standard_concept`,\n  `source_vocabulary`, `domain_id`, and `type_concept`.\n\ntableClinicalRecords(result = result)\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable name\nVariable level\nEstimate name\n\nDatabase name\n\n\n\nsynthea-covid19-10k\n\n\n\n\ndrug_exposure\n\n\nNumber records\n-\nN\n337,509\n\n\nNumber subjects\n-\nN (%)\n10,754 (100.00%)\n\n\nRecords per person\n-\nMean (SD)\n31.38 (45.63)\n\n\n\n\nMedian [Q25 - Q75]\n22 [13 - 33]\n\n\n\n\nRange [min to max]\n[1 to 1,152]\n\n\nIn observation\nYes\nN (%)\n337,509 (100.00%)\n\n\nDomain\nDrug\nN (%)\n337,509 (100.00%)\n\n\nSource vocabulary\nCvx\nN (%)\n310,584 (92.02%)\n\n\n\nRxnorm\nN (%)\n26,925 (7.98%)\n\n\nStandard concept\nS\nN (%)\n337,509 (100.00%)\n\n\nType concept id\nPharmacy claim\nN (%)\n337,509 (100.00%)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipDatabase characterisation\n\n\n\n\n\nWhen you start using a database characterising it is probably a good idea to know what information is recorded in the database and explore the studies that are feasible. You can perform a database characterisation using OmopSketch as:\n\nresult &lt;- databaseCharacteristics(cdm = cdm)\n\nNote this process can take several hours or even days depending on the size of the database. Later you can visualise all the results in a Shiny App such as the one shown at the beginning of this chapter.\n\nshinyCharacteristics(result = result, directory = getwd())",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "omop/exploring_the_cdm.html#disconnecting",
    "href": "omop/exploring_the_cdm.html#disconnecting",
    "title": "6Â  Exploring the OMOP CDM",
    "section": "6.5 Disconnecting",
    "text": "6.5 Disconnecting\nOnce we have finished our analysis we can close our connection to the database behind our cdm reference.\n\ncdmDisconnect(cdm)",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "omop/exploring_the_cdm.html#further-reading",
    "href": "omop/exploring_the_cdm.html#further-reading",
    "title": "6Â  Exploring the OMOP CDM",
    "section": "6.6 Further reading",
    "text": "6.6 Further reading\n\nAlcalde-Herraiz M, Lopez-Guell K, Rowlands E, Campanile C, Burn E, CatalÃ  M (2025). OmopSketch: Characterise Tables of an OMOP Common Data Model Instance. R package version 0.5.1, https://OHDSI.github.io/OmopSketch/.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Exploring the OMOP CDM</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html",
    "href": "omop/adding_features.html",
    "title": "7Â  Identifying patient characteristics",
    "section": "",
    "text": "7.1 Adding specific demographics\nFor this chapter, weâ€™ll again use our example COVID-19 dataset.\nAs part of an analysis, we almost always have a need to identify certain characteristics related to the individuals in our data. These characteristics might be time-invariant (i.e.Â a characteristic that does not change as time passes and a person ages) or time-varying.1\nThe PatientProfiles package makes it easy for us to add demographic information to tables in the OMOP CDM. Like the CDMConnector package weâ€™ve seen previously, the fact that the structure of the OMOP CDM is known allows the PatientProfiles package to abstract away some common data manipulations required to do research with patient-level data.2\nLetâ€™s say weâ€™re interested in individualsâ€™ age and sex at the time of COVID-19 diagnosis. We can add these variables to the table as follows (noting that, since age is time-varying, we need to specify the date relative to which it should be calculated).\ncdm$condition_occurrence &lt;- cdm$condition_occurrence |&gt; \n  addSex() |&gt; \n  addAge(indexDate = \"condition_start_date\")\n\ncdm$condition_occurrence |&gt; \n  glimpse()\n\nRows: ??\nColumns: 18\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1â€¦\n$ person_id                     &lt;int&gt; 2, 6, 7, 8, 8, 8, 8, 16, 16, 18, 18, 25,â€¦\n$ condition_concept_id          &lt;int&gt; 381316, 321042, 381316, 37311061, 437663â€¦\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, â€¦\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ visit_occurrence_id           &lt;int&gt; 19, 55, 67, 79, 79, 79, 79, 168, 171, 19â€¦\n$ visit_detail_id               &lt;int&gt; 1000019, 1000055, 1000067, 1000079, 1000â€¦\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"410429000\", \"230690007\", \"â€¦\n$ condition_source_concept_id   &lt;int&gt; 381316, 321042, 381316, 37311061, 437663â€¦\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\"â€¦\n$ age                           &lt;int&gt; 57, 25, 97, 2, 2, 2, 2, 75, 77, 57, 76, â€¦\nWe have now added two variables containing values for age and sex.\ncdm$condition_occurrence |&gt; \n  glimpse()\n\nRows: ??\nColumns: 18\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1â€¦\n$ person_id                     &lt;int&gt; 2, 6, 7, 8, 8, 8, 8, 16, 16, 18, 18, 25,â€¦\n$ condition_concept_id          &lt;int&gt; 381316, 321042, 381316, 37311061, 437663â€¦\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, â€¦\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ visit_occurrence_id           &lt;int&gt; 19, 55, 67, 79, 79, 79, 79, 168, 171, 19â€¦\n$ visit_detail_id               &lt;int&gt; 1000019, 1000055, 1000067, 1000079, 1000â€¦\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"410429000\", \"230690007\", \"â€¦\n$ condition_source_concept_id   &lt;int&gt; 381316, 321042, 381316, 37311061, 437663â€¦\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\"â€¦\n$ age                           &lt;int&gt; 57, 25, 97, 2, 2, 2, 2, 75, 77, 57, 76, â€¦\nWith these now added, it is straightforward to calculate the mean age at condition start date by sex or even plot the distribution of age at diagnosis by sex.\ncdm$condition_occurrence |&gt;\n  group_by(sex) |&gt;\n  summarise(mean_age = mean(age, na.rm=TRUE)) |&gt; \n  collect()\n\n# A tibble: 2 Ã— 2\n  sex    mean_age\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Female     50.8\n2 Male       56.5\ncdm$condition_occurrence |&gt;\n  select(\"person_id\", \"age\", \"sex\") |&gt; \n  collect()  |&gt;\n  ggplot(aes(fill = sex)) +\n  facet_grid(sex ~ .) +\n  geom_histogram(aes(age), colour = \"black\", binwidth = 5) +\n  theme_bw() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html#adding-specific-demographics",
    "href": "omop/adding_features.html#adding-specific-demographics",
    "title": "7Â  Identifying patient characteristics",
    "section": "",
    "text": "NoteShow query\n\n\n\n\n\n\ncdm$condition_occurrence |&gt; \n  addSexQuery() |&gt; \n  show_query()\n\nWarning: ! The following columns will be overwritten: sex\n\n\n&lt;SQL&gt;\nSELECT\n  condition_occurrence_id,\n  og_002_1761244459.person_id AS person_id,\n  condition_concept_id,\n  condition_start_date,\n  condition_start_datetime,\n  condition_end_date,\n  condition_end_datetime,\n  condition_type_concept_id,\n  condition_status_concept_id,\n  stop_reason,\n  provider_id,\n  visit_occurrence_id,\n  visit_detail_id,\n  condition_source_value,\n  condition_source_concept_id,\n  condition_status_source_value,\n  age,\n  RHS.sex AS sex\nFROM og_002_1761244459\nLEFT JOIN (\n  SELECT\n    person_id,\n    CASE\nWHEN (gender_concept_id = 8507.0) THEN 'Male'\nWHEN (gender_concept_id = 8532.0) THEN 'Female'\nELSE 'None'\nEND AS sex\n  FROM person\n) RHS\n  ON (og_002_1761244459.person_id = RHS.person_id)\n\n\nThe difference between addSexQuery() and addSex() will be explained in the next tip chunk.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html#adding-multiple-demographics-simultaneously",
    "href": "omop/adding_features.html#adding-multiple-demographics-simultaneously",
    "title": "7Â  Identifying patient characteristics",
    "section": "7.2 Adding multiple demographics simultaneously",
    "text": "7.2 Adding multiple demographics simultaneously\nWeâ€™ve now seen individual functions from PatientProfiles that add specific patient characteristics, such as age and sex. The package also includes functions to add other characteristics, such as the number of days of prior observation in the database (rather unimaginatively named addPriorObservation()). In addition to these individual functions, the package also provides a more general function that retrieves all of these characteristics at the same time.3\n\ncdm$drug_exposure &lt;- cdm$drug_exposure |&gt; \n  addDemographics(\n    indexDate = \"drug_exposure_start_date\",\n    age = TRUE,\n    sex = TRUE,\n    priorObservation = TRUE,\n    futureObservation = TRUE,\n    dateOfBirth = TRUE\n  )\n\ncdm$drug_exposure |&gt; \n  glimpse()\n\nRows: ??\nColumns: 28\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n$ drug_exposure_id             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13â€¦\n$ person_id                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,â€¦\n$ drug_concept_id              &lt;int&gt; 40213260, 40213260, 40213260, 40213260, 4â€¦\n$ drug_exposure_start_date     &lt;date&gt; 2021-04-30, 2020-04-24, 2021-04-30, 2020â€¦\n$ drug_exposure_start_datetime &lt;dttm&gt; 2021-04-30 16:49:39, 2020-04-24 16:49:39â€¦\n$ drug_exposure_end_date       &lt;date&gt; 2021-04-30, 2020-04-24, 2021-04-30, 2020â€¦\n$ drug_exposure_end_datetime   &lt;dttm&gt; 2021-04-30 16:49:39, 2020-04-24 16:49:39â€¦\n$ verbatim_end_date            &lt;date&gt; 2021-04-30, 2020-04-24, 2021-04-30, 2020â€¦\n$ drug_type_concept_id         &lt;int&gt; 32869, 32869, 32869, 32869, 32869, 32869,â€¦\n$ stop_reason                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ refills                      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ quantity                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ days_supply                  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ sig                          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ route_concept_id             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ lot_number                   &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"â€¦\n$ provider_id                  &lt;int&gt; 12357, 12357, 12356, 12356, 12357, 12356,â€¦\n$ visit_occurrence_id          &lt;int&gt; 6, 8, 6, 8, 6, 6, 7, 2, 6, 8, 9, 1, 7, 2,â€¦\n$ visit_detail_id              &lt;int&gt; 1000006, 1000008, 1000006, 1000008, 10000â€¦\n$ drug_source_value            &lt;chr&gt; \"121\", \"121\", \"121\", \"121\", \"113\", \"113\",â€¦\n$ drug_source_concept_id       &lt;int&gt; 40213260, 40213260, 40213260, 40213260, 4â€¦\n$ route_source_value           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ dose_unit_source_value       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, Nâ€¦\n$ age                          &lt;int&gt; 51, 50, 51, 50, 51, 51, 53, 52, 51, 50, 4â€¦\n$ sex                          &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", \"â€¦\n$ prior_observation            &lt;int&gt; 2548, 2177, 2548, 2177, 2548, 2548, 3290,â€¦\n$ future_observation           &lt;int&gt; 742, 1113, 742, 1113, 742, 742, 0, 371, 7â€¦\n$ date_of_birth                &lt;date&gt; 1970-04-24, 1970-04-24, 1970-04-24, 1970â€¦\n\n\nWith these characteristics added, we can now calculate mean age, prior observation (the number of days have passed since each individualâ€™s most recent observation start date), and future observation (the number of days until the individualâ€™s nearest observation end date) at drug exposure start date, stratified by sex.\n\ncdm$drug_exposure |&gt;\n  group_by(sex) |&gt;\n  summarise(mean_age = mean(age, na.rm=TRUE),\n            mean_prior_observation = mean(prior_observation, na.rm=TRUE),\n            mean_future_observation = mean(future_observation, na.rm=TRUE)) |&gt; \n  collect()\n\n# A tibble: 2 Ã— 4\n  sex    mean_age mean_prior_observation mean_future_observation\n  &lt;chr&gt;     &lt;dbl&gt;                  &lt;dbl&gt;                   &lt;dbl&gt;\n1 Male       43.0                  2455.                   1768.\n2 Female     39.4                  2096.                   1661.\n\n\n\n\n\n\n\n\nTipReturning a query from PatientProfiles rather than the result\n\n\n\n\n\nIn the above examples, the functions from PatientProfiles execute queries and write the results to a table in the database (either a temporary table if no name is provided when calling the function, or a permanent table). We might instead want to just get the underlying query back so that we have more control over how and when the query is executed.\n\ncdm$visit_occurrence |&gt; \n  addSex() |&gt; \n  filter(sex == \"Male\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT og_004_1761244462.*\nFROM og_004_1761244462\nWHERE (sex = 'Male')\n\n\n\ncdm$visit_occurrence |&gt; \n  addSex(name = \"my_new_table\") |&gt; \n  filter(sex == \"Male\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT test_my_new_table.*\nFROM results.test_my_new_table\nWHERE (sex = 'Male')\n\n\n\ncdm$visit_occurrence |&gt; \n  addSexQuery() |&gt; \n  filter(sex == \"Male\") |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT q01.*\nFROM (\n  SELECT visit_occurrence.*, sex\n  FROM visit_occurrence\n  LEFT JOIN (\n    SELECT\n      person_id,\n      CASE\nWHEN (gender_concept_id = 8507.0) THEN 'Male'\nWHEN (gender_concept_id = 8532.0) THEN 'Female'\nELSE 'None'\nEND AS sex\n    FROM person\n  ) RHS\n    ON (visit_occurrence.person_id = RHS.person_id)\n) q01\nWHERE (sex = 'Male')\n\n\nQuery functions can be useful in some contexts where you donâ€™t want to generate multiple temporary tables or do not want to lose indexes of a certain table, but they can also generate large queries that could result in low performance.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html#creating-categories",
    "href": "omop/adding_features.html#creating-categories",
    "title": "7Â  Identifying patient characteristics",
    "section": "7.3 Creating categories",
    "text": "7.3 Creating categories\nWhen adding age, either via addAge or addDemographics, we can also include an additional variable that groups individuals into age categories. These age groups must be specified in a list of vectors, each of which containing the lower and upper bounds.\n\ncdm$visit_occurrence &lt;- cdm$visit_occurrence |&gt;\n  addAge(\n    indexDate = \"visit_start_date\",\n    ageGroup = list(c(0,17), c(18, 64), c(65, Inf))\n  )\n\ncdm$visit_occurrence |&gt; \n  # data quality issues with our synthetic data means we have \n  # some negative ages so will drop these\n  filter(age &gt;= 0) |&gt; \n  group_by(age_group) |&gt; \n  tally() |&gt; \n  collect() |&gt; \n  ggplot() + \n  geom_col(aes(x = age_group, y = n, fill = age_group)) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipNaming age groups\n\n\n\n\n\nAs we have seen, by default the age groups are named according to their lower and upper bounds (â€˜0 to 17â€™, â€˜18 to 64â€™, and â€˜65 or aboveâ€™). However, we can customise these labels by assigning names to the list of age groups:\n\ncdm$condition_occurrence |&gt;\n  addAgeQuery(\n    indexDate = \"condition_start_date\",\n    ageGroup = list(\"pediatric\" = c(0,17), \"adult\" = c(18, Inf))\n  ) |&gt;\n  filter(age &gt;= 0) |&gt; \n  group_by(age_group) |&gt; \n  tally() |&gt; \n  collect() |&gt; \n  ggplot() + \n  geom_col(aes(x = age_group, y = n, fill = age_group)) + \n  theme_bw()\n\nWarning: ! The following columns will be overwritten: age\n\n\n\n\n\n\n\n\n\nIf you take a close look at the documentation of the function, youâ€™ll see that it also allows you to add multiple age groups and to control the name of the new column, which by default is â€˜age_groupâ€™.\n\n\n\nPatientProfiles also provides a more general function for adding categories. Can you guess its name? Thatâ€™s right, we have addCategories() for this.\n\ncdm$condition_occurrence |&gt;\n  addPriorObservation(indexDate = \"condition_start_date\") |&gt;\n  addCategories(\n    variable = \"prior_observation\",\n    categories = list(\"prior_observation_group\" = list(\n      c(0, 364), c(365, Inf)  \n    ))\n  ) |&gt; \n  glimpse()\n\nRows: ??\nColumns: 20\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n$ condition_occurrence_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1â€¦\n$ person_id                     &lt;int&gt; 2, 6, 7, 8, 8, 8, 8, 16, 16, 18, 18, 25,â€¦\n$ condition_concept_id          &lt;int&gt; 381316, 321042, 381316, 37311061, 437663â€¦\n$ condition_start_date          &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_start_datetime      &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_end_date            &lt;date&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_end_datetime        &lt;dttm&gt; 1986-09-08, 2021-06-23, 2021-04-07, 202â€¦\n$ condition_type_concept_id     &lt;int&gt; 38000175, 38000175, 38000175, 38000175, â€¦\n$ condition_status_concept_id   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ stop_reason                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ provider_id                   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ visit_occurrence_id           &lt;int&gt; 19, 55, 67, 79, 79, 79, 79, 168, 171, 19â€¦\n$ visit_detail_id               &lt;int&gt; 1000019, 1000055, 1000067, 1000079, 1000â€¦\n$ condition_source_value        &lt;chr&gt; \"230690007\", \"410429000\", \"230690007\", \"â€¦\n$ condition_source_concept_id   &lt;int&gt; 381316, 321042, 381316, 37311061, 437663â€¦\n$ condition_status_source_value &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, â€¦\n$ sex                           &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\"â€¦\n$ age                           &lt;int&gt; 57, 25, 97, 2, 2, 2, 2, 75, 77, 57, 76, â€¦\n$ prior_observation             &lt;int&gt; 3437, 2898, 2842, 872, 872, 872, 872, 23â€¦\n$ prior_observation_group       &lt;chr&gt; \"365 or above\", \"365 or above\", \"365 or â€¦",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html#adding-custom-variables",
    "href": "omop/adding_features.html#adding-custom-variables",
    "title": "7Â  Identifying patient characteristics",
    "section": "7.4 Adding custom variables",
    "text": "7.4 Adding custom variables\nWhile PatientProfiles provides a range of functions that can help you add characteristics of interest, you may also want to add other features. Obviously, the package canâ€™t cover all the possible custom characteristics you may wish to add. However, we will see two common groups of custom variables you may want to add:\n\nvariables derived from existing columns within the same table,\nvariables obtained from other tables and joined to our table of interest.\n\nIn the first case, where we want to add a new variable derived from existing variables within our table, weâ€™ll typically use mutate() (from the dplyr package). For example, perhaps we just want to add a new variable to our observation period table that contains the year of each individualâ€™s observation period start date. This is rather straightforward.\n\ncdm$observation_period &lt;- cdm$observation_period |&gt; \n  mutate(observation_period_start_year = get_year(observation_period_start_date))\n\ncdm$observation_period |&gt; \n  glimpse()\n\nRows: ??\nColumns: 6\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n$ observation_period_id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1â€¦\n$ person_id                     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1â€¦\n$ observation_period_start_date &lt;date&gt; 2014-05-09, 1977-04-11, 2014-04-19, 201â€¦\n$ observation_period_end_date   &lt;date&gt; 2023-05-12, 1986-09-15, 2023-04-22, 202â€¦\n$ period_type_concept_id        &lt;int&gt; 44814724, 44814724, 44814724, 44814724, â€¦\n$ observation_period_start_year &lt;dbl&gt; 2014, 1977, 2014, 2014, 2013, 2013, 2013â€¦\n\n\nThe second case is usually a more complex task, as adding a new variable involves joining to some other table following a certain logic. This table may have been created by some intermediate query that we wrote to derive the variable of interest. For example, letâ€™s say we want to add the number of condition occurrence records for each individual to the person table (remember that we saw how to calculate this in the previous chapter). To do this, we will need to perform a join between the person and condition occurrence tables (as some people might not have any records in the condition occurrence table). Here weâ€™ll create a table containing just the information weâ€™re interested in and compute it to a temporary table.\n\ncondition_summary &lt;- cdm$person |&gt; \n  select(\"person_id\") |&gt; \n  left_join(\n    cdm$condition_occurrence |&gt; \n      group_by(person_id) |&gt; \n      count(name = \"condition_occurrence_records\"),\n    by=\"person_id\"\n  ) |&gt; \n  select(\"person_id\", \"condition_occurrence_records\") |&gt; \n  mutate(condition_occurrence_records = coalesce(condition_occurrence_records, 0)) |&gt;\n  compute()\n\ncondition_summary |&gt; \n  glimpse()\n\nRows: ??\nColumns: 2\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n$ person_id                    &lt;int&gt; 2, 6, 7, 8, 16, 18, 25, 36, 40, 42, 44, 4â€¦\n$ condition_occurrence_records &lt;dbl&gt; 1, 1, 1, 4, 2, 2, 1, 4, 1, 3, 2, 5, 1, 3,â€¦\n\n\nWe can see what goes on behind the scenes by viewing the associated SQL.\n\ncdm$person |&gt; \n  select(\"person_id\") |&gt; \n  left_join(\n    cdm$condition_occurrence |&gt; \n      group_by(person_id) |&gt; \n      count(name = \"condition_occurrence_records\"),\n    by=\"person_id\"\n  ) |&gt; \n  select(\"person_id\", \"condition_occurrence_records\") |&gt; \n  mutate(condition_occurrence_records = coalesce(condition_occurrence_records, 0)) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT\n  person_id,\n  COALESCE(condition_occurrence_records, 0.0) AS condition_occurrence_records\nFROM (\n  SELECT person.person_id AS person_id, condition_occurrence_records\n  FROM person\n  LEFT JOIN (\n    SELECT person_id, COUNT(*) AS condition_occurrence_records\n    FROM og_002_1761244459\n    GROUP BY person_id\n  ) RHS\n    ON (person.person_id = RHS.person_id)\n) q01\n\n\n\n\n\n\n\n\nTipTaking care with joins\n\n\n\n\n\nWhen adding variables through joins we need to pay particular attention to the dimensions of the resulting table. While sometimes we may want to have additional rows added as well as new columns, this is often not desired. For example, if we have a table with one row per person, performing a left join to another table containing multiple rows per person will result in multiple rows per person in the output.\nExamples where to be careful include when joining to the observation period table, as individuals can have multiple observation periods, and when working with cohorts (which are the focus of the next chapter) as individuals can also enter the same study cohort multiple times.\nJust to underline how problematic joins can become if we donâ€™t take care, here we join the condition occurrence table and the drug exposure table, both of which have multiple records per person. Even with our small synthetic dataset, this produces an extremely large table. When working with real patient data, which is oftentimes much, much larger, this would be extremely problematic (and would unlikely be needed to answer any research question). In other words, donâ€™t try this at home!\n\ncdm$condition_occurrence |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n      n\n  &lt;dbl&gt;\n1  9967\n\ncdm$drug_exposure |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n       n\n   &lt;dbl&gt;\n1 337509\n\ncdm$condition_occurrence |&gt; \n  select(person_id, condition_start_date) |&gt; \n  left_join(\n    cdm$drug_exposure |&gt; \n      select(person_id, drug_exposure_start_date), \n    by = \"person_id\"\n  ) |&gt; \n  tally()\n\n# Source:   SQL [?? x 1]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpPnE8vX/file35ee2e938e24.duckdb]\n       n\n   &lt;dbl&gt;\n1 410683",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html#disconnecting",
    "href": "omop/adding_features.html#disconnecting",
    "title": "7Â  Identifying patient characteristics",
    "section": "7.5 Disconnecting",
    "text": "7.5 Disconnecting\nOnce we have finished our analysis we can close our connection to the database behind our cdm reference.\n\ncdmDisconnect(cdm)",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html#further-reading",
    "href": "omop/adding_features.html#further-reading",
    "title": "7Â  Identifying patient characteristics",
    "section": "7.6 Further reading",
    "text": "7.6 Further reading\n\nCatalÃ  M, Guo Y, Du M, Lopez-Guell K, Burn E, Mercade-Besora N (2025). PatientProfiles: Identify Characteristics of Patients in the OMOP Common Data Model. R package version 1.4.3, https://darwin-eu.github.io/PatientProfiles/.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/adding_features.html#footnotes",
    "href": "omop/adding_features.html#footnotes",
    "title": "7Â  Identifying patient characteristics",
    "section": "",
    "text": "In some datasets, characteristics that could conceptually be considered as time-varying are encoded as time-invariant. One example of the latter is that in some cases an individual may be associated with a particular socioeconomic status or nationality that for the purposes of the data is treated as time-invariant.â†©ï¸Ž\nAlthough these manipulations can seem quite simple on the face of it, their implementation across different database platforms with different data granularity (for example, whether day of birth has been filled in for all patients or not) presents challenges that the PatientProfiles package solves for us.â†©ï¸Ž\nThis function also provides a more time-efficient method than getting the characteristics one by one. This is because these characteristics are all derived from the OMOP CDM person and observation period tables, and so can be identified simultaneously.â†©ï¸Ž",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Identifying patient characteristics</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html",
    "href": "omop/creating_cohorts.html",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "",
    "text": "8.1 What is a cohort?\nWhen conducting research using the OMOP Common Data Model (CDM), we often aim to identify groups of individuals who share specific characteristics. The inclusion criteria for these groups can range from relatively simple (e.g.Â people diagnosed with asthma) to highly complex (e.g.Â adults diagnosed with asthma who had at least one year of prior observation in the database before their diagnosis, no prior history of chronic obstructive pulmonary disease (COPD), and no history of using short-acting beta-antagonists).\nThe groups of individuals we identify are called cohorts. In the OMOP CDM, cohorts are represented using a specific structure: a cohort table with four required fields:\nIndividuals must be defined in the person table and must be under observation(i.e.Â have an ongoing record in the observation period table) to be part of a cohort. Individuals can enter a cohort multiple times, but the time periods in which they are in the cohort cannot overlap.\nIt is beyond the scope of this book to describe all the different ways cohorts could be created, however in this chapter we provide a summary of some of the key building blocks for cohort creation. Cohort-building pipelines can be created following these principles to create a wide range of study cohorts.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html#what-is-a-cohort",
    "href": "omop/creating_cohorts.html#what-is-a-cohort",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "",
    "text": "Cohort definition id a unique identifier for each cohort (multiple cohorts can be defined in the same cohort table).\nSubject id a foreign key linking the subject in the cohort to the person table.\nCohort start date date indicating the beginning of the cohort record.\nCohort end date date indicating the end of the cohort record.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html#set-up",
    "href": "omop/creating_cohorts.html#set-up",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "8.2 Set up",
    "text": "8.2 Set up\nWeâ€™ll use the same Covid-19 synthetic dataset that we used before for demonstrating how cohorts can be constructed.\n\nlibrary(omock)\nlibrary(CohortConstructor)\nlibrary(CohortCharacteristics)\nlibrary(dplyr)\n        \ncdm &lt;- mockCdmFromDataset(datasetName = \"synthea-covid19-10k\", source = \"duckdb\")",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html#general-concept-based-cohort",
    "href": "omop/creating_cohorts.html#general-concept-based-cohort",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "8.3 General concept based cohort",
    "text": "8.3 General concept based cohort\nOften study cohorts will be based around a specific clinical event identified by some set of clinical codes. Here, for example, we use the CohortConstructor package to create a cohort of people with Covid-19. For this we are identifying any clinical records with the code 37311061.\n\ncdm$covid &lt;- conceptCohort(cdm = cdm, \n                           conceptSet = list(\"covid\" = 37311061), \n                           name = \"covid\")\ncdm$covid\n\n# Source:   table&lt;results.test_covid&gt; [?? x 4]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpP7hxuj/file366913efe62c.duckdb]\n   cohort_definition_id subject_id cohort_start_date cohort_end_date\n                  &lt;int&gt;      &lt;int&gt; &lt;date&gt;            &lt;date&gt;         \n 1                    1       1606 2021-01-18        2021-02-13     \n 2                    1       2527 2021-08-22        2021-09-14     \n 3                    1       2695 2020-07-18        2020-07-31     \n 4                    1       2002 2021-04-23        2021-04-23     \n 5                    1       2954 2020-06-13        2020-06-29     \n 6                    1       5361 2020-10-05        2020-11-01     \n 7                    1      10503 2021-08-14        2021-09-19     \n 8                    1       4029 2020-08-27        2020-09-20     \n 9                    1       4343 2021-02-08        2021-02-28     \n10                    1       4677 2020-07-31        2020-08-14     \n# â„¹ more rows\n\n\n\n\n\n\n\n\nNoteName consistency\n\n\n\n\n\nNote that the name argument determines the name of the permanent table written in the database and as we have seen before, we have to be consistent assigning the tables to the cdm object. Thatâ€™s why we used name = \"covid\" and then we were able to assign it to cdm$covid. Otherwise, see this failing example:\n\ncdm$not_covid &lt;- conceptCohort(cdm = cdm, \n                               conceptSet = list(\"covid\" = 37311061), \n                               name = \"covid\")\n\nWarning: ! `codelist` casted to integers.\n\n\nâ„¹ Subsetting table condition_occurrence using 1 concept with domain: condition.\nâ„¹ Combining tables.\nâ„¹ Creating cohort attributes.\nâ„¹ Applying cohort requirements.\nâ„¹ Merging overlapping records.\nâœ” Cohort covid created.\n\n\nError in `[[&lt;-`:\nâœ– You can't assign a table named covid to not_covid.\nâ„¹ You can change the name using compute:\ncdm[['not_covid']] &lt;- yourObject |&gt;\n  dplyr::compute(name = 'not_covid')\nâ„¹ You can also change the name using the `name` argument in your function:\n  `name = 'not_covid'`.\n\n\n\n\n\n\n\n\n\n\n\nTipFinding appropriate codes\n\n\n\n\n\nIn defining the cohorts above, we have needed to provide concept IDs for our outcomes of interest. But where do these come from?\nWe can search for codes of interest using the CodelistGenerator package. This can be done using a text search with the function getCandidateCodes(). For example, we can have found the code we used above (and many others) like so:\n\nlibrary(CodelistGenerator)\ngetCandidateCodes(cdm = cdm, \n                  keywords = c(\"coronavirus\", \"covid\"),\n                  domains = \"condition\",\n                  includeDescendants = TRUE)\n\nLimiting to domains of interest\nGetting concepts to include\nAdding descendants\nSearch completed. Finishing up.\nâœ” 37 candidate concepts identified\n\nTime taken: 0 minutes and 1 seconds\n\n\n# A tibble: 37 Ã— 6\n   concept_id found_from   concept_name domain_id vocabulary_id standard_concept\n        &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;           \n 1   37310254 From initiaâ€¦ Otitis mediâ€¦ Condition SNOMED        S               \n 2   37310287 From initiaâ€¦ Myocarditisâ€¦ Condition SNOMED        S               \n 3     703445 From initiaâ€¦ Low risk caâ€¦ Condition SNOMED        S               \n 4     756039 From initiaâ€¦ Respiratoryâ€¦ Condition OMOP Extensiâ€¦ S               \n 5     705076 From initiaâ€¦ Post-acute â€¦ Condition OMOP Extensiâ€¦ S               \n 6   37311061 From initiaâ€¦ COVID-19     Condition SNOMED        S               \n 7    3655975 From initiaâ€¦ Sepsis due â€¦ Condition SNOMED        S               \n 8    3655977 From initiaâ€¦ Rhabdomyolyâ€¦ Condition SNOMED        S               \n 9     703446 From initiaâ€¦ Moderate riâ€¦ Condition SNOMED        S               \n10     756031 From initiaâ€¦ Bronchitis â€¦ Condition OMOP Extensiâ€¦ S               \n# â„¹ 27 more rows\n\n\nWe can also do automated searches that make use of the hierarchies in the vocabularies. Here, for example, we find the code for the drug ingredient Acetaminophen and all of its descendants.\n\ncodes &lt;- getDrugIngredientCodes(cdm = cdm, name = \"acetaminophen\")\n\ncodes\n\n\n\n\nâ”€â”€ 1 codelist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n- 161_acetaminophen (25747 codes)\n\n\nNote that in practice clinical expertise is vital in the identification of appropriate codes so as to decide which the codes are in line with the clinical idea at hand. Additionally, not all codes may be used in the database. To check for this, we can use the function subsetToCodesInUse().\n\nusedCodes &lt;- subsetToCodesInUse(list(\"acet\" = codes[[1]]),\n                                cdm = cdm)\n\n\n\n\nWe can see that as well as having the cohort entries above, our cohort table is associated with several attributes.\nFirst, we can see the settings associated with cohort.\n\nsettings(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 4\n$ cohort_definition_id &lt;int&gt; 1\n$ cohort_name          &lt;chr&gt; \"covid\"\n$ cdm_version          &lt;chr&gt; \"5.3\"\n$ vocabulary_version   &lt;chr&gt; \"v5.0 22-JUN-22\"\n\n\nIn settings, we can see the cohort name that by default is the name of the codelist used, in this case â€˜covidâ€™ as we used conceptSet = list(covid = 37311061). Also, the cdm and vocabulary versions are recorded in the settings by the CohortConstructor package.\nSecond, we can get counts of each cohort.\n\ncohortCount(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 3\n$ cohort_definition_id &lt;int&gt; 1\n$ number_records       &lt;int&gt; 964\n$ number_subjects      &lt;int&gt; 964\n\n\nWhere you can see the number of records and number of subjects for each cohort. In this case, there are no multiple records per subject.\nAttrition can also be retrieved from any cohort.\n\nattrition(cdm$covid) |&gt; \n  glimpse()\n\nRows: 6\nColumns: 7\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1\n$ number_records       &lt;int&gt; 964, 964, 964, 964, 964, 964\n$ number_subjects      &lt;int&gt; 964, 964, 964, 964, 964, 964\n$ reason_id            &lt;int&gt; 1, 2, 3, 4, 5, 6\n$ reason               &lt;chr&gt; \"Initial qualifying events\", \"Record in observatiâ€¦\n$ excluded_records     &lt;int&gt; 0, 0, 0, 0, 0, 0\n$ excluded_subjects    &lt;int&gt; 0, 0, 0, 0, 0, 0\n\n\nAnd finally, you can extract the codelists used to create a cohort table:\n\ncodelist &lt;- cohortCodelist(cdm$covid, cohortId = 1)\ncodelist\n\n\n\n\nâ”€â”€ 1 codelist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n- covid (1 codes)\n\ncodelist$covid\n\n[1] 37311061\n\n\nNote that in this case, we had to provide the cohortId of the cohort of interest.\nAll these attributes can be retrieved because it is a cohort_table object, a class on top of the usual cdm_table class that we have seen before:\n\nclass(cdm$covid)\n\n[1] \"cohort_table\"          \"cdm_table\"             \"GeneratedCohortSet\"   \n[4] \"tbl_duckdb_connection\" \"tbl_dbi\"               \"tbl_sql\"              \n[7] \"tbl_lazy\"              \"tbl\"                  \n\n\nAs we will see below, these attributes of the cohorts become particularly useful as we apply further restrictions on our cohort.\n\n\n\n\n\n\nTipBehind the scenes\n\n\n\n\n\nAll these attributes that we have seen are part of the attributes of the cohort_table object and are used by these utility functions:\n\nnames(attributes(cdm$covid))\n\n[1] \"names\"            \"class\"            \"tbl_source\"       \"tbl_name\"        \n[5] \"cohort_set\"       \"cohort_attrition\" \"cohort_codelist\"  \"cdm_reference\"   \n\n\nIn particular, the cohort_set (contains the settings() source), cohort_attrition (contains the source for cohortCount() and attrition()) and cohort_codelist (contains the source for cohortCodelist()) attributes are the ones of interest. For database backends, these attributes are stored directly in the database so that they persist for when we read them again. Note that although it may appear that there is only one tableâ€”cdm$covidâ€”in fact, four tables are written to the database:\n\nlibrary(omopgenerics) # TODO https://github.com/OHDSI/omock/issues/189 \n\n\nAttaching package: 'omopgenerics'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlistSourceTables(cdm = cdm)\n\n[1] \"covid\"           \"covid_attrition\" \"covid_codelist\"  \"covid_set\"      \n\n\nWe do not have to worry about the attributes and the naming of the tables as CohortConstructor, CDMConnector and omopgenerics take care of that and if we create the cohorts with functions such as conceptCohort() then we will be able to read them back with the cohortTables argument of cdmFromCon() or the readSourceTable() function and all the attributes will be in place.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html#applying-inclusion-criteria",
    "href": "omop/creating_cohorts.html#applying-inclusion-criteria",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "8.4 Applying inclusion criteria",
    "text": "8.4 Applying inclusion criteria\n\n8.4.1 Only include first cohort entry per person\nLetâ€™s say we first want to restrict our cohort to only include the first record for each person. This can be done by using the funtion requireIsFirstEntry():\n\ncdm$covid &lt;- cdm$covid |&gt; \n     requireIsFirstEntry() \n\n\n\n8.4.2 Restrict to study period\nThen we are only interested in records from January 1st, 2020 onwards.\n\ncdm$covid &lt;- cdm$covid |&gt;\n   requireInDateRange(dateRange = c(as.Date(\"2020-01-01\"), NA))\n\n\n\n8.4.3 Applying demographic inclusion criteria\nFinally, we want to restrict our population of interest to only adult males under 65 years old. We can do that with the requireDemographics() function.\n\ncdm$covid &lt;- cdm$covid |&gt;\n   requireDemographics(ageRange = c(18, 64), sex = \"Male\")\n\n\n\n\n\n\n\nNoteSimilarity of naming with PatientProfiles\n\n\n\n\n\nNote that all these require*() functions that come from the CohortConstructor package use functionalities from PatientProfiles and the naming is consistent. For example, requireDemographics() uses addDemographics(), requirePriorObservation() uses addPriorObservation(), and so onâ€¦\n\n\n\n\n\n8.4.4 Applying cohort-based inclusion criteria\nIn addition to demographic requirements, we may also want to use another cohort as part of the inclusion or exclusion criteria. For example, we might exclude anyone with a history of cardiac conditions prior to their COVID-19 cohort entry.\nWe can first generate a cohort table with records of myocardial infarction.\n\ncdm$cardiac &lt;- conceptCohort(\n  cdm = cdm,\n  conceptSet = list(\"myocaridal_infarction\" = c(317576L, 313217L, 321042L, 4329847L)), \n  name = \"cardiac\"\n)\ncdm$cardiac\n\n# Source:   table&lt;results.test_cardiac&gt; [?? x 4]\n# Database: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpP7hxuj/file366913efe62c.duckdb]\n   cohort_definition_id subject_id cohort_start_date cohort_end_date\n                  &lt;int&gt;      &lt;int&gt; &lt;date&gt;            &lt;date&gt;         \n 1                    1       4333 2004-05-11        2004-05-11     \n 2                    1       4841 2000-04-10        2000-04-10     \n 3                    1       4947 2018-03-02        2018-03-02     \n 4                    1       6321 2014-09-19        2014-09-19     \n 5                    1       6385 2022-12-15        2022-12-15     \n 6                    1       6745 1990-11-01        1990-11-01     \n 7                    1       7713 2011-05-20        2011-05-20     \n 8                    1       9648 1993-11-09        1993-11-09     \n 9                    1       9658 2020-03-22        2020-03-22     \n10                    1      10101 2007-12-24        2007-12-24     \n# â„¹ more rows\n\n\nNow we can apply the inclusion criteria that individuals have no records of myocardical infarction prior to their Covid-19 cohort entry.\n\ncdm$covid &lt;- cdm$covid |&gt; \n  requireCohortIntersect(targetCohortTable = \"cardiac\", \n                         indexDate = \"cohort_start_date\", \n                         window = c(-Inf, -1), \n                         intersections = 0) \n\nNote that if we had wanted to require that individuals did have a history of a cardiac condition, we would instead have set intersections = c(1, Inf) above.\n\n\n\n\n\n\nNoteUse requireConceptIntersect\n\n\n\n\n\nWe could have applied the exact same inclusion criteria using the requireConceptIntersect() function, this code would be equivalent:\n\ncdm$covid &lt;- cdm$covid |&gt; \n  requireConceptIntersect(\n    conceptSet = list(\"myocaridal_infarction\" = c(317576L, 313217L, 321042L, 4329847L)),\n    indexDate = \"cohort_start_date\", \n    window = c(-Inf, -1), \n    intersections = 0\n  )\n\nIn fact, this approach is generally more efficient unless we plan to reuse the myocardial_infarction cohort for another inclusion criteria or analysis. Note, however, that intersecting with the cohort table is more flexible, as it allows for more complex inclusion and exclusion logic. However, you need to be careful with the order of criteria, for example if we restricted the myocardial_infarction cohort to a specific time period before performing the intersection, we would need to avoid applying a separate inclusion criterion based on that same time span.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html#cohort-attributes",
    "href": "omop/creating_cohorts.html#cohort-attributes",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "8.5 Cohort attributes",
    "text": "8.5 Cohort attributes\nUsing the require*() functions, the cohort attributes have been updated to reflect the applied inclusion criteria.\n\nsettings(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 8\n$ cohort_definition_id   &lt;int&gt; 1\n$ cohort_name            &lt;chr&gt; \"covid\"\n$ cdm_version            &lt;chr&gt; \"5.3\"\n$ vocabulary_version     &lt;chr&gt; \"v5.0 22-JUN-22\"\n$ age_range              &lt;chr&gt; \"18_64\"\n$ sex                    &lt;chr&gt; \"Male\"\n$ min_prior_observation  &lt;dbl&gt; 0\n$ min_future_observation &lt;dbl&gt; 0\n\n\n\ncohortCount(cdm$covid) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 3\n$ cohort_definition_id &lt;int&gt; 1\n$ number_records       &lt;int&gt; 193\n$ number_subjects      &lt;int&gt; 193\n\n\n\nattrition(cdm$covid) |&gt; \n  glimpse()\n\nRows: 13\nColumns: 7\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ number_records       &lt;int&gt; 964, 964, 964, 964, 964, 964, 964, 964, 443, 210,â€¦\n$ number_subjects      &lt;int&gt; 964, 964, 964, 964, 964, 964, 964, 964, 443, 210,â€¦\n$ reason_id            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n$ reason               &lt;chr&gt; \"Initial qualifying events\", \"Record in observatiâ€¦\n$ excluded_records     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 521, 233, 0, 0, 17\n$ excluded_subjects    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 521, 233, 0, 0, 17\n\n\nWe can visualize the attrition with the CohortCharacteristics package. We can first extract it with summariseCohortAttrition() and then plotCohortAttrition to better view the impact of applying each inclusion criteria:\n\nattrition_summary &lt;- summariseCohortAttrition(cohort = cdm$covid)\nplotCohortAttrition(result = attrition_summary, type = 'png')\n\n\n\n\n\n\n\n\nNote that the conceptCohort() first step leads to several rows in the attrition table, whereas any other require*() function always adds just one record of attrition.\n\n\n\n\n\n\nTipCohort naming utilities\n\n\n\n\n\nAs we have seen, by default the naming of the cohorts is the name of the codelist:\n\ncdm$my_cohort &lt;- conceptCohort(cdm = cdm, \n                               conceptSet = list(\"concept_1\" = 37311061L, \"concept_2\" = 317576L), \n                               name = \"my_cohort\")\n\nâ„¹ Subsetting table condition_occurrence using 2 concepts with domain:\n  condition.\nâ„¹ Combining tables.\nâ„¹ Creating cohort attributes.\nâ„¹ Applying cohort requirements.\nâ„¹ Merging overlapping records.\nâœ” Cohort my_cohort created.\n\nsettings(cdm$my_cohort)\n\n# A tibble: 2 Ã— 4\n  cohort_definition_id cohort_name cdm_version vocabulary_version\n                 &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;             \n1                    1 concept_1   5.3         v5.0 22-JUN-22    \n2                    2 concept_2   5.3         v5.0 22-JUN-22    \n\n\nBut maybe we are interested in renaming a cohort (e.g., after applying the inclusion criteria). We can do that with the renameCohort() utility function:\n\ncdm$my_cohort &lt;- cdm$my_cohort |&gt;\n  requirePriorObservation(minPriorObservation = 365, cohortId = 1) |&gt;\n  renameCohort(cohortId = 1, newCohortName = \"concept_1_365obs\")\nsettings(cdm$my_cohort)\n\n# A tibble: 2 Ã— 5\n  cohort_definition_id cohort_name      cdm_version vocabulary_version\n                 &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;             \n1                    1 concept_1_365obs 5.3         v5.0 22-JUN-22    \n2                    2 concept_2        5.3         v5.0 22-JUN-22    \n# â„¹ 1 more variable: min_prior_observation &lt;dbl&gt;\n\n\nNote that for arguments such as cohortId, targetCohortId, etc., we are able to use the name of the cohort of interest. See for example:\n\ncdm$my_cohort &lt;- cdm$my_cohort |&gt;\n  requireSex(sex = \"Female\", cohortId = \"concept_2\") |&gt;\n  renameCohort(cohortId = \"concept_2\", newCohortName = \"concept_2_female\")\nsettings(cdm$my_cohort)\n\n# A tibble: 2 Ã— 6\n  cohort_definition_id cohort_name      cdm_version vocabulary_version\n                 &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;             \n1                    1 concept_1_365obs 5.3         v5.0 22-JUN-22    \n2                    2 concept_2_female 5.3         v5.0 22-JUN-22    \n# â„¹ 2 more variables: min_prior_observation &lt;dbl&gt;, sex &lt;chr&gt;\n\n\nThis functionality also applies to other packages, such as CohortCharacteristics, PatientProfiles and DrugUtilisation. In some cases, it is useful to add the cohort_name as a column to not have to check manually the equivalence between cohort definition id and cohort name. This can be done using the PatientProfiles utility function addCohortName():\n\nlibrary(PatientProfiles)\ncdm$my_cohort |&gt;\n  addCohortName() |&gt;\n  glimpse()\n\nRows: ??\nColumns: 5\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpP7hxuj/file366913efe62c.duckdb]\n$ cohort_definition_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2â€¦\n$ subject_id           &lt;int&gt; 71, 82, 134, 151, 153, 183, 194, 196, 200, 215, 2â€¦\n$ cohort_start_date    &lt;date&gt; 1936-01-24, 1997-08-04, 2001-01-09, 2022-01-13, â€¦\n$ cohort_end_date      &lt;date&gt; 1936-01-24, 1997-08-04, 2001-01-09, 2022-01-13, â€¦\n$ cohort_name          &lt;chr&gt; \"concept_2_female\", \"concept_2_female\", \"concept_â€¦\n\n\nAlso other utility functions that can be useful are those provided by omopgenerics:\n\nlibrary(omopgenerics)\ngetCohortId(cohort = cdm$my_cohort, cohortName = \"concept_2_female\")\n\nconcept_2_female \n               2 \n\ngetCohortId(cohort = cdm$my_cohort)\n\nconcept_1_365obs concept_2_female \n               1                2 \n\ngetCohortName(cohort = cdm$my_cohort, cohortId = 1)\n\n                 1 \n\"concept_1_365obs\" \n\ngetCohortName(cohort = cdm$my_cohort, cohortId = c(2, 1))\n\n                 2                  1 \n\"concept_2_female\" \"concept_1_365obs\" \n\ngetCohortName(cohort = cdm$my_cohort)\n\n                 1                  2 \n\"concept_1_365obs\" \"concept_2_female\"",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html#disconnecting",
    "href": "omop/creating_cohorts.html#disconnecting",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "8.6 Disconnecting",
    "text": "8.6 Disconnecting\nOnce we have finished our analysis we can close our connection to the database behind our cdm reference.\n\ncdmDisconnect(cdm)",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/creating_cohorts.html#further-reading",
    "href": "omop/creating_cohorts.html#further-reading",
    "title": "8Â  Adding cohorts to the CDM",
    "section": "8.7 Further reading",
    "text": "8.7 Further reading\n\nCohort tables\nBurn E, CatalÃ  M, Mercade-Besora N, Alcalde-Herraiz M, Du M, Guo Y, Chen X, Lopez-Guell K, Rowlands E (2025). CohortConstructor: Build and Manipulate Study Cohorts Using a Common Data Model. R package version 0.5.0, https://ohdsi.github.io/CohortConstructor/.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Adding cohorts to the CDM</span>"
    ]
  },
  {
    "objectID": "omop/working_with_cohorts.html",
    "href": "omop/working_with_cohorts.html",
    "title": "9Â  Working with cohorts",
    "section": "",
    "text": "9.1 Cohort intersections\nWhen conducting research, it is often necessary to study patients who meet multiple clinical criteria simultaneously. For example, we may be interested in analysing outcomes among patients who have both diabetes and hypertension. Using the OMOP CDM, this typically involves first creating two separate cohorts: one for patients with diabetes and another for those with hypertension. To identify patients who meet both conditions, the next step is to compute the intersection of these cohorts. This ensures that the final study population includes only individuals who satisfy all specified criteria. Hence, finding cohort intersections is a common and essential task when working with the OMOP CDM, enabling researchers to define precise target populations that align with their research objectives.\nDepending on the research question, the definition of a cohort intersection may vary. For instance, you might require patients to have a diagnosis of hypertension before developing diabetes, or that both diagnoses occur within a specific time window. These additional temporal or clinical criteria can make cohort intersection more complex. The PatientProfiles R package addresses these challenges by providing a suite of flexible functions to support the calculation of cohort intersections under various scenarios.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "omop/working_with_cohorts.html#intersection-between-two-cohorts",
    "href": "omop/working_with_cohorts.html#intersection-between-two-cohorts",
    "title": "9Â  Working with cohorts",
    "section": "9.2 Intersection between two cohorts",
    "text": "9.2 Intersection between two cohorts\nSuppose we are interested in studying patients with gastrointestinal (GI) bleeding who have also been exposed to acetaminophen. First, we would create two separate cohorts: one for patients with GI bleeding and another for patients with exposure to acetaminophen. Below is an example of the code used to create these cohorts within the GiBleed synthetic database. A characterisation of this dataset can be found here.\n\nlibrary(omock)\nlibrary(dplyr)\nlibrary(PatientProfiles)\nlibrary(CohortConstructor)\nlibrary(omopgenerics) \n\ncdm &lt;- mockCdmFromDataset(datasetName = \"GiBleed\", source = \"duckdb\")\n\n# gi_bleed contains all records of GiBbleed, end date is 30 days after index \n# date\ncdm$gi_bleed &lt;- conceptCohort(\n  cdm = cdm,\n  conceptSet = list(\"gi_bleed\" = 192671L), \n  name = \"gi_bleed\", \n  exit = \"event_start_date\"\n) |&gt;\n  padCohortEnd(days = 30)\n\n# drugs cohort contains records of acetaminophen using start and end dates of \n# the drug records and collapsing records separated by less than 30 days\ncdm$drugs &lt;- conceptCohort(\n  cdm = cdm,\n  conceptSet = list(\"acetaminophen\" = c(\n    1125315L, 1127078L, 1127433L, 40229134L, 40231925L, 40162522L, 19133768L\n  )), \n  name = \"drugs\", \n  exit = \"event_end_date\"\n) |&gt;\n  collapseCohorts(gap = 30)\n\nThe PatientProfiles package contains functions to obtain the intersection flag, count, date, or number of days between cohorts.\n\n9.2.1 Flag\nTo get a binary indicator showing the presence of an intersection between the cohorts within a given time window, we can use addCohortIntersectFlag().\n\nx &lt;- cdm$gi_bleed |&gt;\n  addCohortIntersectFlag(\n    targetCohortTable = \"drugs\",\n    window = list(\"prior\" = c(-Inf, -1), \"index\" = c(0, 0), \"post\" = c(1, Inf))\n  )\n\nx |&gt;\n  summarise(\n    acetaminophen_prior = sum(acetaminophen_prior, na.rm = TRUE),\n    acetaminophen_index = sum(acetaminophen_index, na.rm = TRUE),\n    acetaminophen_post = sum(acetaminophen_post, na.rm = TRUE)\n  ) |&gt;\n  collect()\n\n# A tibble: 1 Ã— 3\n  acetaminophen_prior acetaminophen_index acetaminophen_post\n                &lt;dbl&gt;               &lt;dbl&gt;              &lt;dbl&gt;\n1                 467                   1                315\n\n\n\n\n\n\n\n\nNoteWindow naming\n\n\n\n\n\nWindows work very similarly to age groups that we have seen before. If a name is not provided, an automatic name will be obtained from the values of the window limits:\n\ncdm$gi_bleed |&gt;\n  addCohortIntersectFlag(\n    targetCohortTable = \"drugs\",\n    window = list(c(-Inf, -1), c(0, 0), c(1, Inf))\n  ) |&gt;\n  glimpse()\n\nRows: ??\nColumns: 7\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpZ5jH6u/file371960663dcd.duckdb]\n$ cohort_definition_id     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ subject_id               &lt;int&gt; 5014, 1265, 1848, 2483, 2896, 4102, 4860, 492â€¦\n$ cohort_start_date        &lt;date&gt; 1949-01-08, 2018-02-20, 1982-10-10, 1984-11-â€¦\n$ cohort_end_date          &lt;date&gt; 1949-02-07, 2018-02-21, 1982-11-09, 1984-12-â€¦\n$ acetaminophen_minf_to_m1 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ acetaminophen_0_to_0     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ acetaminophen_1_to_inf   &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, â€¦\n\n\nNote that to avoid conflicts with column naming, all names will be lower case, spaces are not allowed, and the - symbol for negative values is replaced by m. Thatâ€™s why it is usually nice to provide your own custom names:\n\ncdm$gi_bleed |&gt;\n  addCohortIntersectFlag(\n    targetCohortTable = \"drugs\",\n    window = list(\"prior\" = c(-Inf, -1), \"index\" = c(0, 0), \"post\" = c(1, Inf))\n  ) |&gt;\n  glimpse()\n\nRows: ??\nColumns: 7\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpZ5jH6u/file371960663dcd.duckdb]\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ subject_id           &lt;int&gt; 5014, 1265, 1848, 2483, 2896, 4102, 4860, 4923, 5â€¦\n$ cohort_start_date    &lt;date&gt; 1949-01-08, 2018-02-20, 1982-10-10, 1984-11-07, â€¦\n$ cohort_end_date      &lt;date&gt; 1949-02-07, 2018-02-21, 1982-11-09, 1984-12-07, â€¦\n$ acetaminophen_prior  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ acetaminophen_index  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ acetaminophen_post   &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1â€¦\n\n\n\n\n\n\n\n\n\n\n\nTipNew column naming\n\n\n\n\n\nBy default, the name of new columns is â€˜{cohort_name}_{window_name}â€™ as we have seen in the prior examples. In some cases, you only have one cohort or one window and you might want to rename the column as you please. In that case, you can use the nameStyle argument to change the new naming of the columns:\n\ncdm$gi_bleed |&gt;\n  addCohortIntersectFlag(\n    targetCohortTable = \"drugs\",\n    window = list(\"prior\" = c(-Inf, -1), \"index\" = c(0, 0), \"post\" = c(1, Inf)),\n    nameStyle = \"my_column_{window_name}\"\n  ) |&gt;\n  glimpse()\n\nRows: ??\nColumns: 7\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpZ5jH6u/file371960663dcd.duckdb]\n$ cohort_definition_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ subject_id           &lt;int&gt; 5014, 1265, 1848, 2483, 2896, 4102, 4860, 4923, 5â€¦\n$ cohort_start_date    &lt;date&gt; 1949-01-08, 2018-02-20, 1982-10-10, 1984-11-07, â€¦\n$ cohort_end_date      &lt;date&gt; 1949-02-07, 2018-02-21, 1982-11-09, 1984-12-07, â€¦\n$ my_column_post       &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1â€¦\n$ my_column_prior      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ my_column_index      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n\n\nIf multiple windows are provided but â€˜{window_name}â€™ is not included in nameStyle, then an error will prompt:\n\ncdm$gi_bleed |&gt;\n  addCohortIntersectFlag(\n    targetCohortTable = \"drugs\",\n    window = list(\"prior\" = c(-Inf, -1), \"index\" = c(0, 0), \"post\" = c(1, Inf)),\n    nameStyle = \"my_new_column\"\n  ) |&gt;\n  glimpse()\n\nError in `.addIntersect()`:\n! The following elements are not present in nameStyle:\nâ€¢ {window_name}\n\n\nMany functions that create new columns (usually functions that start with add*()) have this nameStyle functionality that allows you to control the naming of the new columns created.\n\n\n\n\n\n9.2.2 Count\nTo get the count of occurrences of intersection between two cohorts, we can use addCohortIntersectCount():\n\nx &lt;- cdm$gi_bleed |&gt;\n  addCohortIntersectCount(\n    targetCohortTable = \"drugs\",\n    window = list(\"prior\" = c(-Inf, -1), \"index\" = c(0, 0), \"post\" = c(1, Inf)),\n  )\n\nx |&gt;\n  summarise(\n    sum_prior = sum(acetaminophen_prior, na.rm = TRUE),\n    mean_prior = mean(acetaminophen_prior, na.rm = TRUE),\n    sum_index = sum(acetaminophen_index, na.rm = TRUE),\n    mean_index = mean(acetaminophen_index, na.rm = TRUE),\n    sum_post = sum(acetaminophen_post, na.rm = TRUE),\n    mean_post = mean(acetaminophen_post, na.rm = TRUE)\n  ) |&gt;\n  collect()\n\n# A tibble: 1 Ã— 6\n  sum_prior mean_prior sum_index mean_index sum_post mean_post\n      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      1669       3.48         1    0.00209      758      1.58\n\n\n\n\n\n\n\n\nNoteHandling the observation period\n\n\n\n\n\nNote that only intersections in the current observation period are considered.\nThe count and flag new columns can also have NA values meaning that the individual was not in observation in that window of interest. If we see individual 2070, it has 3748 days of future observation:\n\ncdm$gi_bleed |&gt;\n  filter(subject_id == 2070) |&gt;\n  addFutureObservation() |&gt;\n  glimpse()\n\nRows: ??\nColumns: 5\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpZ5jH6u/file371960663dcd.duckdb]\n$ cohort_definition_id &lt;int&gt; 1\n$ subject_id           &lt;int&gt; 2070\n$ cohort_start_date    &lt;date&gt; 2008-08-15\n$ cohort_end_date      &lt;date&gt; 2008-09-14\n$ future_observation   &lt;int&gt; 3748\n\n\nNow we will perform the intersect with the following window of interest: c(2000, 3000), c(3000, 4000), c(4000, 5000).\n\ncdm$gi_bleed |&gt;\n  filter(subject_id == 2070) |&gt;\n  addCohortIntersectCount(\n    targetCohortTable = \"drugs\",\n    window = list(c(2000, 3000), c(3000, 4000), c(4000, 5000)),\n  ) |&gt;\nglimpse()\n\nRows: ??\nColumns: 7\nDatabase: DuckDB 1.4.1 [unknown@Linux 6.11.0-1018-azure:R 4.4.1//tmp/RtmpZ5jH6u/file371960663dcd.duckdb]\n$ cohort_definition_id       &lt;int&gt; 1\n$ subject_id                 &lt;int&gt; 2070\n$ cohort_start_date          &lt;date&gt; 2008-08-15\n$ cohort_end_date            &lt;date&gt; 2008-09-14\n$ acetaminophen_2000_to_3000 &lt;dbl&gt; 0\n$ acetaminophen_3000_to_4000 &lt;dbl&gt; 0\n$ acetaminophen_4000_to_5000 &lt;dbl&gt; NA\n\n\nSee that for the window 2000 to 3000, where the individual is still in observation, a 0 is reported. The same happens for the window 3000 to 4000 even if the individual does not have complete observation in the window. But for the last window, as the individual is not in observation at any point of the window, NA is reported.\n\n\n\n\n\n9.2.3 Date and times\nTo get the date of the intersection with a cohort within a given time window, we can use addCohortIntersectDate(). To get the number of days between the index date and intersection, we can use addCohortIntersectDays().\nBoth functions allow the order argument to specify which value to return:\n\nfirst returns the first date/days that satisfy the window\nlast returns the last date/days that satisfy the window\n\n\nx &lt;- cdm$gi_bleed |&gt;\n  addCohortIntersectDate(\n    targetCohortTable = \"drugs\",\n    window = list(\"post\" = c(1, Inf)),\n    order = \"first\"\n  )\n\nx |&gt;\n  summarise(acetaminophen_post = median(acetaminophen_post, na.rm = TRUE)) |&gt;\n  collect()\n\n# A tibble: 1 Ã— 1\n  acetaminophen_post \n  &lt;dttm&gt;             \n1 2004-02-01 00:00:00\n\n\n\nx &lt;- cdm$gi_bleed |&gt;\n  addCohortIntersectDays(\n    targetCohortTable = \"drugs\",\n    window = list(\"prior\" = c(-Inf, -1)),\n    order = \"last\"\n  )\n\nx |&gt;\n  summarise(acetaminophen_prior = median(acetaminophen_prior, na.rm = TRUE)) |&gt;\n  collect()\n\n# A tibble: 1 Ã— 1\n  acetaminophen_prior\n                &lt;dbl&gt;\n1               -3159\n\n\nNote that for the window in the future, we used order = \"first\" and for the window in the past, we used order = \"last\" as in both cases we wanted to get the intersection that was closer to the index date. Individuals with no intersection will have NA on the newly created columns.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "omop/working_with_cohorts.html#intersection-between-a-cohort-and-tables-with-patient-data",
    "href": "omop/working_with_cohorts.html#intersection-between-a-cohort-and-tables-with-patient-data",
    "title": "9Â  Working with cohorts",
    "section": "9.3 Intersection between a cohort and tables with patient data",
    "text": "9.3 Intersection between a cohort and tables with patient data\nSometimes we might want to get the intersection between a cohort and another OMOP table. PatientProfiles also includes several addTableIntersect* functions to obtain intersection flags, counts, days, or dates between a cohort and clinical tables.\nFor example, if we want to get the number of general practitioner (GP) visits for individuals in the cohort, we can use the visit_occurrence table:\n\nx &lt;- cdm$gi_bleed |&gt;\n  addTableIntersectCount(\n    tableName = \"visit_occurrence\",\n    window = list(c(-Inf, -1)),\n    nameStyle = \"number_visits\"\n  )\n\nx |&gt;\n  summarise(visit_occurrence_prior = median(number_visits, na.rm = TRUE)) |&gt;\n  collect()\n\n# A tibble: 1 Ã— 1\n  visit_occurrence_prior\n                   &lt;dbl&gt;\n1                      0",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "omop/working_with_cohorts.html#disconnecting",
    "href": "omop/working_with_cohorts.html#disconnecting",
    "title": "9Â  Working with cohorts",
    "section": "9.4 Disconnecting",
    "text": "9.4 Disconnecting\nOnce we have finished our analysis we can close our connection to the database behind our cdm reference.\n\ncdmDisconnect(cdm)",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "omop/working_with_cohorts.html#further-reading",
    "href": "omop/working_with_cohorts.html#further-reading",
    "title": "9Â  Working with cohorts",
    "section": "9.5 Further reading",
    "text": "9.5 Further reading\nFull details on the intersection functions in PatientProfiles can be found on the package website: https://darwin-eu.github.io/PatientProfiles/.",
    "crumbs": [
      "Working with the OMOP CDM from R",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Working with cohorts</span>"
    ]
  },
  {
    "objectID": "final_remarks.html",
    "href": "final_remarks.html",
    "title": "Final remarks",
    "section": "",
    "text": "Learning more\nTidy R programming with the OMOP Common Data Model aims to (1) explain the main principles for working with databases from R and (2) how to apply these principles and use them with the OMOP CDM. Hopefully, after reading this book, you can understand how the dplyr and dbplyr packages interact with the databases, in particular with data formatted to the OMOP CDM; how the cdm_reference object can be used to extract and identify your population of interest; and add the desired features to your dataset. Note that in this book we always worked with relatively small synthetic data with unrealistic performance. Any analysis conducted with real-world data and big databases will take more time, thatâ€™s why we would always recommend you test your code against synthetic data or subsets of your data to ensure good performance. Be careful, especially while writing custom code using join functions that can create some ugly SQL.\nIf you find this book useful then joining the Tidy R in OMOP OHDSI working group will likely be of interest. Building on many of the concepts and tools seen in this book, the Tidy R in OMOP OHDSI working group aims to (1) create and promote a unified set of resources to guide Tidy R development in OMOP and support the OHDSI community, and (2) establish an overview of available packages relevant to Tidy R programming in OMOP (tidyverse style packages). If you are interested in joining the working group then please email MartÃ­ CatalÃ  or Raivo Kolde.",
    "crumbs": [
      "Final remarks"
    ]
  },
  {
    "objectID": "final_remarks.html#support-us",
    "href": "final_remarks.html#support-us",
    "title": "Final remarks",
    "section": "Support us",
    "text": "Support us\nWe encourage you to support this work by either citing the book in your papers or documentation, recommending it to your colleagues, or starring the GitHub repository, or simply letting us know how it helped you. Most importantly, please use it in research that results in patient benefit.",
    "crumbs": [
      "Final remarks"
    ]
  }
]